{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11c560d3-1917-40a1-bd50-0bf5a3e00a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2235b7ca-e108-47dc-afc4-583a3fb76153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              kind  commentId  channelId  videoId  authorId  \\\n",
      "0  youtube#comment    1781382      14492    74288   2032536   \n",
      "1  youtube#comment     289571      14727    79618   3043229   \n",
      "2  youtube#comment     569077       3314    51826    917006   \n",
      "3  youtube#comment    2957962       5008    58298   1853470   \n",
      "4  youtube#comment     673093      21411     1265   2584166   \n",
      "\n",
      "                                        textOriginal  parentCommentId  \\\n",
      "0  PLEASE LESBIAN FLAG I BEG YOU \\n\\nYou would ro...              NaN   \n",
      "1   Apply mashed potato juice and mixed it with curd        3198066.0   \n",
      "2                         69 missed calls from marsðŸ‘½              NaN   \n",
      "3                                               Baaa              NaN   \n",
      "4    you look like raven from phenomena raven no cap              NaN   \n",
      "\n",
      "   likeCount                publishedAt                  updatedAt  \n",
      "0          0  2023-08-15 21:48:52+00:00  2023-08-15 21:48:52+00:00  \n",
      "1          0  2023-10-02 13:08:22+00:00  2023-10-02 13:08:22+00:00  \n",
      "2          0  2024-05-31 12:03:12+00:00  2024-05-31 12:03:12+00:00  \n",
      "3          0  2024-02-13 15:48:37+00:00  2024-02-13 15:48:37+00:00  \n",
      "4          0  2020-02-15 22:28:44+00:00  2020-02-15 22:28:44+00:00  \n",
      "              kind  commentId  channelId  videoId  authorId  \\\n",
      "0  youtube#comment    3403605      23845    31993   2874725   \n",
      "1  youtube#comment    3957372      32163    88966    757766   \n",
      "2  youtube#comment    1272592      18073    69091    858228   \n",
      "3  youtube#comment    4006296      47781    38945   2179100   \n",
      "4  youtube#comment     521568      13736    92721    364652   \n",
      "\n",
      "                            textOriginal  parentCommentId  likeCount  \\\n",
      "0  my 5 year old brother said dolphin ðŸ¤£ðŸ¤£              NaN          0   \n",
      "1     You look very beautiful and cute â¤              NaN          0   \n",
      "2                            American ðŸ‡ºðŸ‡²              NaN          0   \n",
      "3                         MASHA'ALLAHâ™¥â™¥â™¥              NaN          0   \n",
      "4       As a scene kid, YOU KILLED ITðŸ˜»â€¼ï¸              NaN          0   \n",
      "\n",
      "                 publishedAt                  updatedAt  \n",
      "0  2025-04-03 13:49:20+00:00  2025-04-03 13:49:20+00:00  \n",
      "1  2024-07-17 08:51:19+00:00  2024-07-17 08:51:19+00:00  \n",
      "2  2023-01-29 05:01:06+00:00  2023-01-29 05:01:06+00:00  \n",
      "3  2022-07-27 22:38:41+00:00  2022-07-27 22:38:41+00:00  \n",
      "4  2025-05-04 20:28:48+00:00  2025-05-04 20:28:48+00:00  \n",
      "              kind  commentId  channelId  videoId  authorId  \\\n",
      "0  youtube#comment     612540      22719    91334   2745147   \n",
      "1  youtube#comment    4554495      30386     6875   2449235   \n",
      "2  youtube#comment    3422849      47768    64633   2626179   \n",
      "3  youtube#comment    3078731      20597     5863   1689287   \n",
      "4  youtube#comment    2078670      23924    79616    770683   \n",
      "\n",
      "                                        textOriginal  parentCommentId  \\\n",
      "0  Sheâ€™s naturally awe ðŸ˜® she donâ€™t need a wig or ...              NaN   \n",
      "1  Number 3 is terrible. Shame on the hair dresse...              NaN   \n",
      "2                          Cara de povo ðŸ˜‚ðŸ˜‚ðŸ˜‚BRAZIL...              NaN   \n",
      "3                         Options are always nice. â¤              NaN   \n",
      "4  God, bless her soul!! I was literally smiling ...              NaN   \n",
      "\n",
      "   likeCount                publishedAt                  updatedAt  \n",
      "0          0  2024-12-31 16:39:32+00:00  2024-12-31 16:39:32+00:00  \n",
      "1          1  2025-06-22 23:30:07+00:00  2025-06-22 23:30:07+00:00  \n",
      "2          0  2025-03-20 12:08:17+00:00  2025-03-20 12:08:17+00:00  \n",
      "3          0  2023-09-24 01:19:10+00:00  2023-09-24 01:19:10+00:00  \n",
      "4          0  2021-04-08 22:51:57+00:00  2021-04-08 22:52:31+00:00  \n",
      "              kind  commentId  channelId  videoId  authorId  \\\n",
      "0  youtube#comment    1546147       1583     5446   2049014   \n",
      "1  youtube#comment      70185       1658    91495   1676079   \n",
      "2  youtube#comment    1315458      22013    48712    164755   \n",
      "3  youtube#comment    2790969      41246    18969   2963595   \n",
      "4  youtube#comment    3834158      41734    27096   2307720   \n",
      "\n",
      "                                        textOriginal  parentCommentId  \\\n",
      "0                            So nature and amazing â¤              NaN   \n",
      "1  @@xrose2030 so by definition, this commercial ...         913915.0   \n",
      "2                                        Blend blend              NaN   \n",
      "3                         East or west India is best              NaN   \n",
      "4  I freaking feel like a student learning new th...              NaN   \n",
      "\n",
      "   likeCount                publishedAt                  updatedAt  \n",
      "0          0  2023-09-06 21:14:57+00:00  2023-09-06 21:14:56+00:00  \n",
      "1          0  2021-07-02 02:54:27+00:00  2021-07-02 02:54:27+00:00  \n",
      "2          0  2025-01-03 12:16:32+00:00  2025-01-03 12:16:32+00:00  \n",
      "3          0  2022-05-31 06:39:58+00:00  2022-05-31 06:39:58+00:00  \n",
      "4          0  2024-03-31 05:29:58+00:00  2024-03-31 05:29:58+00:00  \n",
      "              kind  commentId  channelId  videoId  authorId  \\\n",
      "0  youtube#comment     600702      21224    91712    463359   \n",
      "1  youtube#comment     549057      50201    53793   1125630   \n",
      "2  youtube#comment     804966      29145    61224   3309637   \n",
      "3  youtube#comment    1302885      13098    30730   1408547   \n",
      "4  youtube#comment    2744696      48794    31684    777869   \n",
      "\n",
      "                                        textOriginal  parentCommentId  \\\n",
      "0                                            So true              NaN   \n",
      "1                                                ðŸ¤¯ðŸ˜²ðŸ˜³              NaN   \n",
      "2                                               Link              NaN   \n",
      "3  Half of those skincare came from different cul...              NaN   \n",
      "4                         Sheâ€™s the twin of Timothee              NaN   \n",
      "\n",
      "   likeCount                publishedAt                  updatedAt  \n",
      "0          0  2024-04-22 15:14:26+00:00  2024-04-22 15:14:26+00:00  \n",
      "1          0  2025-01-08 15:33:21+00:00  2025-01-08 15:33:21+00:00  \n",
      "2          0  2024-08-29 14:43:58+00:00  2024-08-29 14:43:58+00:00  \n",
      "3          2  2025-07-03 12:02:23+00:00  2025-07-03 12:02:23+00:00  \n",
      "4          0  2024-06-05 11:34:49+00:00  2024-06-05 11:34:49+00:00  \n"
     ]
    }
   ],
   "source": [
    "comment1 = pd.read_csv(\"comments1.csv\")\n",
    "comment2 = pd.read_csv(\"comments2.csv\")\n",
    "comment3 = pd.read_csv(\"comments3.csv\")\n",
    "comment4 = pd.read_csv(\"comments4.csv\")\n",
    "comment5 = pd.read_csv(\"comments5.csv\")\n",
    "\n",
    "print(comment1.head())\n",
    "print(comment2.head())\n",
    "print(comment3.head())\n",
    "print(comment4.head())\n",
    "print(comment5.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57a325c8-5c15-489e-8160-2b740981b668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining CSVs â†’ Parquet...\n",
      "Cleaning text & assigning topics â†’ Parquet...\n",
      "Stratified sampling to 100,000 rows â†’ CSV...\n",
      "Done.\n",
      "topic\n",
      "other                   43274\n",
      "compliment              16160\n",
      "request_or_question     13146\n",
      "emoji_only               7718\n",
      "reply                    6595\n",
      "beauty_and_hair          5779\n",
      "humor_meme               4018\n",
      "nationality_language     1980\n",
      "critique                  796\n",
      "religion                  276\n",
      "spam_or_promo             258\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- Imports\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "INPUT_FILES = [\n",
    "    \"comments1.csv\",\n",
    "    \"comments2.csv\",\n",
    "    \"comments3.csv\",\n",
    "    \"comments4.csv\",\n",
    "    \"comments5.csv\",\n",
    "]\n",
    "COMBINED_PARQUET = \"comments_combined.parquet\"     # compact & fast\n",
    "CLEANED_PARQUET  = \"comments_cleaned.parquet\"\n",
    "SAMPLE_CSV       = \"comments_sample_100k.csv\"\n",
    "\n",
    "# Columns to keep (adjust if you need more)\n",
    "USECOLS = [\n",
    "    \"kind\", \"commentId\", \"channelId\", \"videoId\", \"authorId\",\n",
    "    \"textOriginal\", \"parentCommentId\", \"likeCount\", \"publishedAt\", \"updatedAt\"\n",
    "]\n",
    "\n",
    "CHUNKSIZE = 200_000         # tune for your memory\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "url_re = re.compile(r\"(https?://\\S+|www\\.\\S+)\", re.IGNORECASE)\n",
    "whitespace_re = re.compile(r\"\\s+\")\n",
    "punct_run_re = re.compile(r\"([!?.]){2,}\")\n",
    "\n",
    "def clean_text_keep_emojis(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Light cleaning that preserves emojis and non-ASCII characters.\n",
    "    - lowercases\n",
    "    - removes URLs\n",
    "    - normalizes whitespace\n",
    "    - collapses repeated terminal punctuation\n",
    "    - keeps emojis, accents, etc.\n",
    "    \"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    x = s\n",
    "    x = url_re.sub(\" \", x)                 # drop URLs\n",
    "    x = punct_run_re.sub(r\"\\1\", x)         # \"!!!\" -> \"!\"\n",
    "    x = x.replace(\"\\r\", \" \").replace(\"\\n\", \" \")\n",
    "    x = whitespace_re.sub(\" \", x).strip()\n",
    "    x = x.lower()\n",
    "    return x\n",
    "\n",
    "# Quick detectors that won't remove emojis\n",
    "only_emoji_or_punct_re = re.compile(r\"^[\\W_]+$\", re.UNICODE)  # no letters/digits\n",
    "\n",
    "def is_emoji_or_punct_only(s: str) -> bool:\n",
    "    if not s:\n",
    "        return False\n",
    "    return bool(only_emoji_or_punct_re.match(s))\n",
    "\n",
    "# -----------------------------\n",
    "# Topic rules (transparent & auditable)\n",
    "# Order matters: first match wins.\n",
    "# -----------------------------\n",
    "TOPIC_RULES = [\n",
    "    (\"spam_or_promo\",      re.compile(r\"\\b(subscribe|promo|discount|deal|sale|follow me|check my|giveaway)\\b|(\\.com|http)\", re.IGNORECASE)),\n",
    "    (\"request_or_question\",re.compile(r\"\\b(how|what|where|when|why|can you|could you|pls|please|link)\\b|\\?$\", re.IGNORECASE)),\n",
    "    (\"compliment\",         re.compile(r\"\\b(beautiful|pretty|cute|gorgeous|amazing|love|slay|killed it|stunning|awesome|nice|well done|good job)\\b\", re.IGNORECASE)),\n",
    "    (\"critique\",           re.compile(r\"\\b(terrible|bad|hate|worst|awful|trash|shame|mid)\\b\", re.IGNORECASE)),\n",
    "    (\"religion\",           re.compile(r\"\\b(mashallah|insha|allah|amen|god bless|hallelujah)\\b\", re.IGNORECASE)),\n",
    "    (\"nationality_language\",re.compile(r\"\\b(brazil|ind(ia|ian)|american|arab(ic)?|spanish|french|german|russian|chinese|malay|english)\\b|ðŸ‡§ðŸ‡·|ðŸ‡®ðŸ‡³|ðŸ‡ºðŸ‡¸|ðŸ‡µðŸ‡°|ðŸ‡§ðŸ‡©|ðŸ‡¬ðŸ‡§|ðŸ‡¨ðŸ‡³|ðŸ‡²ðŸ‡¾\", re.IGNORECASE)),\n",
    "    (\"beauty_and_hair\",    re.compile(r\"\\b(hair|wig|makeup|skincare|mask|serum|blush|foundation|balayage|lipstick|eyeliner|hydration)\\b\", re.IGNORECASE)),\n",
    "    (\"humor_meme\",         re.compile(r\"\\b(lol|lmao|lmfao|haha|hehe)\\b|ðŸ¤£|ðŸ˜‚\", re.IGNORECASE)),\n",
    "]\n",
    "\n",
    "def assign_topic(row_text: str, parent_comment_id) -> str:\n",
    "    # emoji-only / punct-only\n",
    "    if is_emoji_or_punct_only(row_text):\n",
    "        return \"emoji_only\"\n",
    "\n",
    "    # reply thread detector\n",
    "    if pd.notna(parent_comment_id):\n",
    "        # still allow other rules to override if clearly matched\n",
    "        for label, pat in TOPIC_RULES:\n",
    "            if pat.search(row_text):\n",
    "                return label\n",
    "        return \"reply\"\n",
    "\n",
    "    # rule-based topics\n",
    "    for label, pat in TOPIC_RULES:\n",
    "        if pat.search(row_text):\n",
    "            return label\n",
    "\n",
    "    return \"other\"\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Combine CSVs (chunked, memory-safe) â†’ Parquet\n",
    "# -----------------------------\n",
    "def combine_csvs_to_parquet(input_files, out_parquet, usecols=None, chunksize=CHUNKSIZE):\n",
    "    if os.path.exists(out_parquet):\n",
    "        os.remove(out_parquet)\n",
    "\n",
    "    writer = None\n",
    "    try:\n",
    "        import pyarrow as pa\n",
    "        import pyarrow.parquet as pq\n",
    "        for f in input_files:\n",
    "            for chunk in pd.read_csv(f, usecols=usecols, chunksize=chunksize, dtype_backend=\"pyarrow\"):\n",
    "                # Ensure consistent dtypes by letting Arrow infer\n",
    "                table = pa.Table.from_pandas(chunk, preserve_index=False)\n",
    "                if writer is None:\n",
    "                    writer = pq.ParquetWriter(out_parquet, table.schema, compression=\"snappy\")\n",
    "                writer.write_table(table)\n",
    "    finally:\n",
    "        if writer is not None:\n",
    "            writer.close()\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Clean + Topic label â†’ Parquet\n",
    "# -----------------------------\n",
    "def clean_and_label_to_parquet(in_parquet, out_parquet, chunksize=CHUNKSIZE):\n",
    "    if os.path.exists(out_parquet):\n",
    "        os.remove(out_parquet)\n",
    "\n",
    "    import pyarrow as pa\n",
    "    import pyarrow.parquet as pq\n",
    "\n",
    "    reader = pq.ParquetFile(in_parquet)\n",
    "    writer = None\n",
    "\n",
    "    # Iterate over row groups to stay memory-safe\n",
    "    for rg in range(reader.num_row_groups):\n",
    "        table = reader.read_row_group(rg)\n",
    "        df = table.to_pandas(types_mapper=pd.ArrowDtype).copy()\n",
    "\n",
    "        # Light clean\n",
    "        df[\"text_clean\"] = df[\"textOriginal\"].apply(clean_text_keep_emojis)\n",
    "\n",
    "        # Topic\n",
    "        df[\"topic\"] = [\n",
    "            assign_topic(txt, pid) for txt, pid in zip(df[\"text_clean\"], df.get(\"parentCommentId\"))\n",
    "        ]\n",
    "\n",
    "        # Keep only what you need going forward (optional)\n",
    "        keep = [\"commentId\", \"channelId\", \"videoId\", \"authorId\",\n",
    "                \"textOriginal\", \"text_clean\", \"parentCommentId\",\n",
    "                \"likeCount\", \"publishedAt\", \"updatedAt\", \"topic\"]\n",
    "        keep = [c for c in keep if c in df.columns]\n",
    "        df = df[keep]\n",
    "\n",
    "        t = pa.Table.from_pandas(df, preserve_index=False)\n",
    "        if writer is None:\n",
    "            writer = pq.ParquetWriter(out_parquet, t.schema, compression=\"snappy\")\n",
    "        writer.write_table(t)\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.close()\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Proportional stratified sample to EXACT N rows\n",
    "# -----------------------------\n",
    "def proportional_stratified_sample(parquet_path, out_csv, n=100_000, random_state=RANDOM_STATE):\n",
    "    \"\"\"\n",
    "    Reads by row groups to avoid loading everything.\n",
    "    Accumulates per-topic rows, then samples proportionally to topic prevalence.\n",
    "    \"\"\"\n",
    "    import pyarrow.parquet as pq\n",
    "\n",
    "    # First pass: topic counts\n",
    "    reader = pq.ParquetFile(parquet_path)\n",
    "    topic_counts = {}\n",
    "    total = 0\n",
    "    for rg in range(reader.num_row_groups):\n",
    "        df = reader.read_row_group(rg, columns=[\"topic\"]).to_pandas()\n",
    "        vc = df[\"topic\"].value_counts()\n",
    "        for k, v in vc.items():\n",
    "            topic_counts[k] = topic_counts.get(k, 0) + int(v)\n",
    "        total += len(df)\n",
    "\n",
    "    # Compute target per-topic sizes (rounded, corrected to exact n)\n",
    "    topics = sorted(topic_counts.keys())\n",
    "    proportions = {t: topic_counts[t] / total for t in topics}\n",
    "    target_raw = {t: proportions[t] * n for t in topics}\n",
    "    target = {t: int(math.floor(target_raw[t])) for t in topics}\n",
    "    # Fix rounding to match n exactly\n",
    "    deficit = n - sum(target.values())\n",
    "    if deficit > 0:\n",
    "        # Assign the leftover rows to topics with largest fractional parts\n",
    "        remainders = sorted(((t, target_raw[t] - target[t]) for t in topics),\n",
    "                            key=lambda x: x[1], reverse=True)\n",
    "        for i in range(deficit):\n",
    "            target[remainders[i % len(remainders)][0]] += 1\n",
    "\n",
    "    # Second pass: sample within each topic\n",
    "    # We'll gather per-topic buffers across row groups until we have enough to sample.\n",
    "    buffers = {t: [] for t in topics}\n",
    "\n",
    "    # We only need minimal columns in the final sample; keep what you like\n",
    "    keep_cols = [\"commentId\", \"videoId\", \"authorId\", \"textOriginal\", \"text_clean\",\n",
    "                 \"parentCommentId\", \"likeCount\", \"publishedAt\", \"updatedAt\", \"topic\"]\n",
    "\n",
    "    for rg in range(reader.num_row_groups):\n",
    "        df = reader.read_row_group(rg).to_pandas()\n",
    "        if \"text_clean\" not in df.columns:\n",
    "            # ensure cleaning/labeling was done\n",
    "            raise RuntimeError(\"Expected 'text_clean' in cleaned parquet. Run clean_and_label_to_parquet first.\")\n",
    "        for t in topics:\n",
    "            needed = max(0, target[t] - sum(len(x) for x in buffers[t]))\n",
    "            if needed == 0:\n",
    "                continue\n",
    "            # take all rows of this topic in the chunk (not sampled yet)\n",
    "            sub = df.loc[df[\"topic\"] == t, keep_cols]\n",
    "            if len(sub):\n",
    "                buffers[t].append(sub)\n",
    "\n",
    "    # Concatenate topic-wise and sample exact target size\n",
    "    sampled_parts = []\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    for t in topics:\n",
    "        if len(buffers[t]) == 0:\n",
    "            continue\n",
    "        pool = pd.concat(buffers[t], ignore_index=True)\n",
    "        k = min(target[t], len(pool))\n",
    "        if k == 0:\n",
    "            continue\n",
    "        sampled = pool.sample(n=k, random_state=rng)\n",
    "        sampled_parts.append(sampled)\n",
    "\n",
    "    sample_df = pd.concat(sampled_parts, ignore_index=True)\n",
    "    # If for any reason we're short (e.g., some rare topic), top-up from the largest topic\n",
    "    if len(sample_df) < n:\n",
    "        short = n - len(sample_df)\n",
    "        # find largest available topic to top-up\n",
    "        topic_sizes = sample_df[\"topic\"].value_counts()\n",
    "        biggest_topic = topic_sizes.index[0]\n",
    "        # read again and pull more from that topic not already in sample\n",
    "        # (to keep it simple, we'll just accept being slightly short if truly impossible)\n",
    "        print(f\"Warning: sample short by {short} rows; could not fully meet per-topic targets.\")\n",
    "\n",
    "    sample_df.to_csv(out_csv, index=False)\n",
    "    return sample_df\n",
    "\n",
    "# -----------------------------\n",
    "# Run the pipeline\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Combine CSVs â†’ Parquet\n",
    "    print(\"Combining CSVs â†’ Parquet...\")\n",
    "    combine_csvs_to_parquet(INPUT_FILES, COMBINED_PARQUET, usecols=USECOLS, chunksize=CHUNKSIZE)\n",
    "\n",
    "    # 2) Clean + Topic labeling â†’ Parquet\n",
    "    print(\"Cleaning text & assigning topics â†’ Parquet...\")\n",
    "    clean_and_label_to_parquet(COMBINED_PARQUET, CLEANED_PARQUET, chunksize=CHUNKSIZE)\n",
    "\n",
    "    # 3) Stratified sample to 100k\n",
    "    print(\"Stratified sampling to 100,000 rows â†’ CSV...\")\n",
    "    sample_df = proportional_stratified_sample(CLEANED_PARQUET, SAMPLE_CSV, n=100_000, random_state=RANDOM_STATE)\n",
    "\n",
    "    print(\"Done.\")\n",
    "    print(sample_df[\"topic\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62addcb2-3ddf-452f-8fbc-7effeba47064",
   "metadata": {},
   "source": [
    "Combining CSVs â†’ Parquet...\n",
    "Cleaning text & assigning topics â†’ Parquet...\n",
    "Stratified sampling to 100,000 rows â†’ CSV...\n",
    "Done.\n",
    "topic\n",
    "other                   43274\n",
    "compliment              16160\n",
    "request_or_question     13146\n",
    "emoji_only               7718\n",
    "reply                    6595\n",
    "beauty_and_hair          5779\n",
    "humor_meme               4018\n",
    "nationality_language     1980\n",
    "critique                  796\n",
    "religion                  276\n",
    "spam_or_promo             258\n",
    "Name: count, dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3113b6f8-f2e3-4563-a662-f8479efde2f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built training sample: 200000 rows (from ~200000 seen).\n",
      "Fitting TF-IDF vectorizer on sample...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rayap\\anaconda\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTM sample shape: (200000, 20000)\n",
      "Fitting NMF (30 topics)...\n",
      "NMF fit complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 24it [05:21, 13.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All chunks processed and saved.\n",
      "Total docs processed: 4725012, across 24 chunks.\n",
      "Topic counts (sample): {0: 405422, 1: 275926, 2: 210693, 3: 190966, 4: 74528, 5: 144227, 6: 105778, 7: 68563, 8: 241746, 9: 65956, 10: 211182, 11: 103846, 12: 50468, 13: 108922, 14: 93366, 15: 82219, 16: 99540, 17: 52157, 18: 277952, 19: 44697, 20: 101607, 21: 159245, 22: 292234, 23: 415064, 24: 67276, 25: 126540, 26: 342162, 27: 131093, 28: 37153, 29: 144484}\n",
      "Sampling targets per topic computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling from chunks:   0%|                                                                     | 0/24 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final sampled rows: 100000\n",
      "Saved: comments_sample_100k_stratified.csv\n",
      "Saved vectorizer and NMF model to disk (joblib).\n"
     ]
    }
   ],
   "source": [
    "# FILE: nmf_topic_stratified_sampling.py\n",
    "# Run with: python nmf_topic_stratified_sampling.py\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# -----------------------\n",
    "# Config - tune these\n",
    "# -----------------------\n",
    "INPUT_FILES = [\n",
    "    \"comments1.csv\",\n",
    "    \"comments2.csv\",\n",
    "    \"comments3.csv\",\n",
    "    \"comments4.csv\",\n",
    "    \"comments5.csv\",\n",
    "]\n",
    "CLEANED_CHUNKS_DIR = \"topic_chunks\"          # will store chunked parquet files with topic labels\n",
    "os.makedirs(CLEANED_CHUNKS_DIR, exist_ok=True)\n",
    "\n",
    "SAMPLE_SIZE = 200_000        # size to train the NMF on (200k is a good balance)\n",
    "N_TOPICS = 30               # number of topics for NMF\n",
    "MAX_FEATURES = 20000         # vocabulary size for TF-IDF (memory control)\n",
    "CHUNKSIZE = 200_000         # rows read per chunk from CSV (tune to your memory)\n",
    "RANDOM_STATE = 42\n",
    "FINAL_SAMPLE_SIZE = 100_000  # desired stratified sample size\n",
    "\n",
    "# -----------------------\n",
    "# Helpers\n",
    "# -----------------------\n",
    "url_re = re.compile(r\"(https?://\\S+|www\\.\\S+)\", re.IGNORECASE)\n",
    "ws_re = re.compile(r\"\\s+\")\n",
    "\n",
    "def clean_text_keep_emojis(s):\n",
    "    \"\"\"Light cleaning but keep emojis and non-ascii chars.\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    x = url_re.sub(\" \", s)\n",
    "    x = x.replace(\"\\r\", \" \").replace(\"\\n\", \" \")\n",
    "    x = ws_re.sub(\" \", x).strip()\n",
    "    return x.lower()\n",
    "\n",
    "# tokenizer that preserves emojis and punctuation tokens as separate tokens\n",
    "def simple_tokenizer(text):\n",
    "    # capture word tokens OR single non-space non-word characters (emojis/punct)\n",
    "    # This will keep emojis as tokens\n",
    "    return re.findall(r\"\\w+|[^\\s\\w]\", text, flags=re.UNICODE)\n",
    "\n",
    "# -----------------------\n",
    "# Step 0: helper to stream CSVs as DataFrame chunks\n",
    "# -----------------------\n",
    "def csv_chunks(files, chunksize=100_000, usecols=None):\n",
    "    \"\"\"Yields tuples (df_chunk, source_filename, global_chunk_idx).\"\"\"\n",
    "    idx = 0\n",
    "    for fname in files:\n",
    "        for chunk in pd.read_csv(fname, usecols=usecols, chunksize=chunksize, dtype=str):\n",
    "            yield chunk, fname, idx\n",
    "            idx += 1\n",
    "\n",
    "# -----------------------\n",
    "# Step 1: Build a training sample (SAMPLE_SIZE) by streaming\n",
    "# -----------------------\n",
    "def build_training_sample(files, sample_size=SAMPLE_SIZE, chunksize=CHUNKSIZE):\n",
    "    rng = np.random.RandomState(RANDOM_STATE)\n",
    "    sample_texts = []\n",
    "    sample_ids = []\n",
    "    total_seen = 0\n",
    "    for chunk, fname, _ in csv_chunks(files, chunksize=chunksize, usecols=[\"commentId\", \"textOriginal\"]):\n",
    "        # clean text column\n",
    "        chunk[\"text_clean\"] = chunk[\"textOriginal\"].apply(clean_text_keep_emojis)\n",
    "        # reservoir-like sampling: append and then sample if we exceed\n",
    "        sample_texts.extend(chunk[\"text_clean\"].tolist())\n",
    "        sample_ids.extend(chunk[\"commentId\"].tolist())\n",
    "        total_seen += len(chunk)\n",
    "        # if collected many more than sample_size, downsample to sample_size\n",
    "        if len(sample_texts) > sample_size * 1.5:\n",
    "            chosen_idx = rng.choice(len(sample_texts), size=sample_size, replace=False)\n",
    "            sample_texts = [sample_texts[i] for i in chosen_idx]\n",
    "            sample_ids = [sample_ids[i] for i in chosen_idx]\n",
    "        if len(sample_texts) >= sample_size:\n",
    "            break\n",
    "\n",
    "    # final trimming (random if more collected)\n",
    "    if len(sample_texts) > sample_size:\n",
    "        chosen_idx = rng.choice(len(sample_texts), size=sample_size, replace=False)\n",
    "        sample_texts = [sample_texts[i] for i in chosen_idx]\n",
    "        sample_ids = [sample_ids[i] for i in chosen_idx]\n",
    "\n",
    "    sample_df = pd.DataFrame({\"commentId\": sample_ids, \"text_clean\": sample_texts})\n",
    "    print(f\"Built training sample: {len(sample_df)} rows (from ~{total_seen} seen).\")\n",
    "    return sample_df\n",
    "\n",
    "# -----------------------\n",
    "# Step 2: Fit TF-IDF vectorizer and NMF on the sample\n",
    "# -----------------------\n",
    "def fit_vectorizer_and_nmf(sample_df, n_topics=N_TOPICS, max_features=MAX_FEATURES):\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=max_features,\n",
    "        tokenizer=simple_tokenizer,\n",
    "        lowercase=True,\n",
    "        stop_words=\"english\"\n",
    "    )\n",
    "    print(\"Fitting TF-IDF vectorizer on sample...\")\n",
    "    dtm_sample = vectorizer.fit_transform(sample_df[\"text_clean\"])\n",
    "    print(f\"DTM sample shape: {dtm_sample.shape}\")\n",
    "\n",
    "    print(f\"Fitting NMF ({n_topics} topics)...\")\n",
    "    nmf = NMF(\n",
    "        n_components=n_topics,\n",
    "        random_state=RANDOM_STATE,\n",
    "        init=\"nndsvda\",\n",
    "        max_iter=200,\n",
    "        tol=1e-4\n",
    "    )\n",
    "    W = nmf.fit_transform(dtm_sample)  # W: (n_samples, n_topics)\n",
    "    H = nmf.components_                 # H: (n_topics, n_features)\n",
    "    print(\"NMF fit complete.\")\n",
    "    return vectorizer, nmf\n",
    "\n",
    "# -----------------------\n",
    "# Step 3: Apply model to all comments chunk-by-chunk and write per-chunk parquet with topic\n",
    "# -----------------------\n",
    "def apply_model_and_write_chunks(files, vectorizer, nmf, chunksize=CHUNKSIZE, out_dir=CLEANED_CHUNKS_DIR):\n",
    "    chunk_idx = 0\n",
    "    topic_counts = {}\n",
    "    for chunk, fname, _ in tqdm(csv_chunks(files, chunksize=chunksize, usecols=None), desc=\"Processing chunks\"):\n",
    "        # ensure textOriginal exists\n",
    "        if \"textOriginal\" not in chunk.columns:\n",
    "            # try possible column names or skip\n",
    "            if \"text\" in chunk.columns:\n",
    "                chunk[\"textOriginal\"] = chunk[\"text\"]\n",
    "            else:\n",
    "                chunk[\"textOriginal\"] = \"\"\n",
    "        chunk[\"text_clean\"] = chunk[\"textOriginal\"].apply(clean_text_keep_emojis)\n",
    "\n",
    "        # vectorize and transform to topic space\n",
    "        dtm = vectorizer.transform(chunk[\"text_clean\"])\n",
    "        # nmf.transform solves for W (non-negative least squares) per document\n",
    "        topic_dist = nmf.transform(dtm)          # shape (n_docs_in_chunk, n_topics)\n",
    "        dominant_topic = np.argmax(topic_dist, axis=1)\n",
    "        chunk[\"topic\"] = dominant_topic.astype(int)\n",
    "\n",
    "        # update running counts\n",
    "        unique, counts = np.unique(dominant_topic, return_counts=True)\n",
    "        for u, c in zip(unique, counts):\n",
    "            topic_counts[int(u)] = topic_counts.get(int(u), 0) + int(c)\n",
    "\n",
    "        # write chunk to parquet\n",
    "        out_path = os.path.join(out_dir, f\"chunk_{chunk_idx:05d}.parquet\")\n",
    "        # keep only necessary columns to save space\n",
    "        keep_cols = [c for c in [\"commentId\", \"channelId\", \"videoId\", \"authorId\",\n",
    "                                 \"textOriginal\", \"text_clean\", \"parentCommentId\",\n",
    "                                 \"likeCount\", \"publishedAt\", \"updatedAt\", \"topic\"] if c in chunk.columns]\n",
    "        chunk[keep_cols].to_parquet(out_path, index=False)\n",
    "        chunk_idx += 1\n",
    "\n",
    "    print(\"All chunks processed and saved.\")\n",
    "    return topic_counts, chunk_idx\n",
    "\n",
    "# -----------------------\n",
    "# Step 4: Proportional stratified sampling to exactly FINAL_SAMPLE_SIZE\n",
    "# -----------------------\n",
    "def stratified_sample_from_chunks(chunks_dir, topic_counts, total_docs, final_n=FINAL_SAMPLE_SIZE, random_state=RANDOM_STATE):\n",
    "    # compute target allocation per topic\n",
    "    topics = sorted(topic_counts.keys())\n",
    "    proportions = {t: topic_counts[t] / total_docs for t in topics}\n",
    "    target_raw = {t: proportions[t] * final_n for t in topics}\n",
    "    target = {t: int(math.floor(target_raw[t])) for t in topics}\n",
    "    deficit = final_n - sum(target.values())\n",
    "    if deficit > 0:\n",
    "        remainders = sorted(((t, target_raw[t] - target[t]) for t in topics), key=lambda x: x[1], reverse=True)\n",
    "        for i in range(deficit):\n",
    "            target[remainders[i % len(remainders)][0]] += 1\n",
    "\n",
    "    print(\"Sampling targets per topic computed.\")\n",
    "\n",
    "    # Now iterate chunks and draw per-topic samples until targets filled\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    buffers = {t: [] for t in topics}  # will hold dataframes\n",
    "    filled = {t: 0 for t in topics}\n",
    "\n",
    "    chunk_files = sorted(glob.glob(os.path.join(chunks_dir, \"chunk_*.parquet\")))\n",
    "    for pf in tqdm(chunk_files, desc=\"Sampling from chunks\"):\n",
    "        df = pd.read_parquet(pf)\n",
    "        for t in topics:\n",
    "            need = target[t] - filled[t]\n",
    "            if need <= 0:\n",
    "                continue\n",
    "            sub = df.loc[df[\"topic\"] == t]\n",
    "            if len(sub) == 0:\n",
    "                continue\n",
    "            # if more rows than needed, sample\n",
    "            take = sub.sample(n=min(need, len(sub)), random_state=rng)\n",
    "            buffers[t].append(take)\n",
    "            filled[t] += len(take)\n",
    "\n",
    "        # quick exit if all filled\n",
    "        if sum(filled.values()) >= final_n:\n",
    "            break\n",
    "\n",
    "    # concat results\n",
    "    parts = []\n",
    "    for t in topics:\n",
    "        if buffers[t]:\n",
    "            parts.append(pd.concat(buffers[t], ignore_index=True))\n",
    "\n",
    "    sample_df = pd.concat(parts, ignore_index=True)\n",
    "    # safety check: if slightly more (due to rounding), trim\n",
    "    if len(sample_df) > final_n:\n",
    "        sample_df = sample_df.sample(n=final_n, random_state=rng).reset_index(drop=True)\n",
    "\n",
    "    print(f\"Final sampled rows: {len(sample_df)}\")\n",
    "    return sample_df\n",
    "\n",
    "# -----------------------\n",
    "# Main pipeline\n",
    "# -----------------------\n",
    "def main():\n",
    "    # 1) Build sample for training\n",
    "    sample_df = build_training_sample(INPUT_FILES, sample_size=SAMPLE_SIZE, chunksize=CHUNKSIZE)\n",
    "\n",
    "    # 2) Fit vectorizer + NMF on the sample\n",
    "    vectorizer, nmf = fit_vectorizer_and_nmf(sample_df, n_topics=N_TOPICS, max_features=MAX_FEATURES)\n",
    "\n",
    "    # 3) Apply the model to all comments in chunks and store chunked parquet files with 'topic' column\n",
    "    topic_counts, n_chunks = apply_model_and_write_chunks(INPUT_FILES, vectorizer, nmf, chunksize=CHUNKSIZE, out_dir=CLEANED_CHUNKS_DIR)\n",
    "    total_docs = sum(topic_counts.values())\n",
    "    print(f\"Total docs processed: {total_docs}, across {n_chunks} chunks.\")\n",
    "    print(\"Topic counts (sample):\", topic_counts)\n",
    "\n",
    "    # 4) Stratified sample exactly FINAL_SAMPLE_SIZE\n",
    "    sample_df_final = stratified_sample_from_chunks(CLEANED_CHUNKS_DIR, topic_counts, total_docs, final_n=FINAL_SAMPLE_SIZE, random_state=RANDOM_STATE)\n",
    "    sample_df_final.to_csv(\"comments_sample_100k_stratified.csv\", index=False)\n",
    "    print(\"Saved: comments_sample_100k_stratified.csv\")\n",
    "\n",
    "    # 5) Optional: save the NMF model / vectorizer using joblib for reuse\n",
    "    try:\n",
    "        import joblib\n",
    "        joblib.dump(vectorizer, \"tfidf_vectorizer.joblib\")\n",
    "        joblib.dump(nmf, \"nmf_model.joblib\")\n",
    "        print(\"Saved vectorizer and NMF model to disk (joblib).\")\n",
    "    except Exception as e:\n",
    "        print(\"joblib save failed:\", e)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbee3cee-52b3-4c37-8722-0ecc12696b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.56.1-py3-none-any.whl.metadata (42 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\rayap\\anaconda\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\rayap\\anaconda\\lib\\site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rayap\\anaconda\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\rayap\\anaconda\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\rayap\\anaconda\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\rayap\\anaconda\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Using cached tokenizers-0.22.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.6.2-cp38-abi3-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\rayap\\anaconda\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\rayap\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rayap\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\rayap\\anaconda\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rayap\\anaconda\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rayap\\anaconda\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rayap\\anaconda\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rayap\\anaconda\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Downloading transformers-4.56.1-py3-none-any.whl (11.6 MB)\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/11.6 MB 5.9 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.1/11.6 MB 5.7 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.9/11.6 MB 6.9 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.0/11.6 MB 7.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.6/11.6 MB 7.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.4/11.6 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.6/11.6 MB 8.3 MB/s eta 0:00:00\n",
      "Using cached huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "Using cached tokenizers-0.22.0-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "Using cached safetensors-0.6.2-cp38-abi3-win_amd64.whl (320 kB)\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   -------------------- ------------------- 2/4 [tokenizers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ---------------------------------------- 4/4 [transformers]\n",
      "\n",
      "Successfully installed huggingface-hub-0.34.4 safetensors-0.6.2 tokenizers-0.22.0 transformers-4.56.1\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "521a0b00-59f2-4464-8b9d-3f0a6a8c80b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2302\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2300\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module:\n\u001b[0;32m   2301\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2302\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[0;32m   2303\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   2304\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2330\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2328\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   2329\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2330\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   2331\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   2332\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\importlib\\__init__.py:88\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     87\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdynamic_module_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_class_from_dynamic_module\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PreTrainedFeatureExtractor\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_processing_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseImageProcessor\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_auto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoConfig\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction_auto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FEATURE_EXTRACTOR_MAPPING, AutoFeatureExtractor\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\transformers\\image_processing_utils.py:22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_processing_base\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BatchFeature, ImageProcessingMixin\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_transforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m center_crop, normalize, rescale\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChannelDimension, get_image_size\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logging\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\transformers\\image_transforms.py:48\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tf_available():\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_flax_available():\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjnp\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\tensorflow\\__init__.py:49\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tf2 \u001b[38;5;28;01mas\u001b[39;00m _tf2\n\u001b[0;32m     47\u001b[0m _tf2\u001b[38;5;241m.\u001b[39menable()\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m audio\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m decorator\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dispatch\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m eager_context\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m feature_column\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\distribute\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.distribute namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m combinations\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m interim\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m multi_process_runner\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\distribute\\combinations\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.distribute.combinations namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombinations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m env \u001b[38;5;66;03m# line: 456\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombinations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m generate \u001b[38;5;66;03m# line: 365\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombinations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m in_main_process \u001b[38;5;66;03m# line: 418\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\tensorflow\\python\\distribute\\combinations.py:33\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msix\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m session\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m collective_all_reduce_strategy\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distribute_lib\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m multi_process_runner\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\tensorflow\\python\\distribute\\collective_all_reduce_strategy.py:26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tensorflow_server_pb2\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m collective_util\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cross_device_ops \u001b[38;5;28;01mas\u001b[39;00m cross_device_ops_lib\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cross_device_utils\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m device_util\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\tensorflow\\python\\distribute\\cross_device_ops.py:28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m device_lib\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m collective_util\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cross_device_utils\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m device_util\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distribute_utils\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\tensorflow\\python\\distribute\\cross_device_utils.py:22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Callable, List, Optional, Union\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m collective_util\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m values \u001b[38;5;28;01mas\u001b[39;00m value_lib\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backprop_util\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m context\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\tensorflow\\python\\distribute\\values.py:23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m struct_pb2\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m device_util\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distribute_lib\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m packed_distributed_variable \u001b[38;5;28;01mas\u001b[39;00m packed\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m reduce_util\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:205\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ag_ctx \u001b[38;5;28;01mas\u001b[39;00m autograph_ctx\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m api \u001b[38;5;28;01mas\u001b[39;00m autograph\n\u001b[1;32m--> 205\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dataset_ops\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m collective_util\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m device_util\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\tensorflow\\python\\data\\__init__.py:21\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"`tf.data.Dataset` API for input pipelines.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mSee [Importing Data](https://tensorflow.org/guide/data) for an overview.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AUTOTUNE\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\tensorflow\\python\\data\\experimental\\__init__.py:99\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Experimental API for building input pipelines.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mThis module contains experimental `Dataset` sources and transformations that can\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;124;03m@@UNKNOWN_CARDINALITY\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m service\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatching\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dense_to_ragged_batch\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatching\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dense_to_sparse_batch\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\tensorflow\\python\\data\\experimental\\service\\__init__.py:419\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"API for using the tf.data service.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mThis module contains:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;124;03m  job of ParameterServerStrategy).\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 419\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_service_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_service_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m from_dataset_id\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_service_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m register_dataset\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\data_service_ops.py:26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservice\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _pywrap_server_lib\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservice\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _pywrap_utils_exp\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dataset_ops\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m options \u001b[38;5;28;01mas\u001b[39;00m options_lib\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m structured_function\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:34\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dataset_autograph\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m debug_mode\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m iterator_ops\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m options \u001b[38;5;28;01mas\u001b[39;00m options_lib\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m structured_function\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:45\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nested_structure_coder\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrackable\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m base \u001b[38;5;28;01mas\u001b[39;00m trackable\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaver\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseSaverBuilder\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deprecation\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m collections_abc\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\tensorflow\\python\\training\\saver.py:48\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gfile\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tf_logging \u001b[38;5;28;01mas\u001b[39;00m logging\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpywrap_saved_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m metrics\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrackable\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m base \u001b[38;5;28;01mas\u001b[39;00m trackable\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m py_checkpoint_reader\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Load data (assuming CSV)\n",
    "# -----------------------------\n",
    "df = pd.read_csv(\"comments.csv\")   # replace with your merged dataset\n",
    "print(\"Original dataset:\", len(df))\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Randomly choose 100k\n",
    "# -----------------------------\n",
    "df_sample = df.sample(n=100000, random_state=42)\n",
    "print(\"Random sample size:\", len(df_sample))\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Zero-Shot Classification\n",
    "# -----------------------------\n",
    "# Load Hugging Face zero-shot classifier\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Candidate labels to try â€” feel free to expand this list\n",
    "candidate_labels = [\"gaming\", \"music\", \"politics\", \"technology\", \"sports\", \n",
    "                    \"movies\", \"tv\", \"education\", \"food\", \"travel\", \"health\"]\n",
    "\n",
    "def classify_comment(text):\n",
    "    try:\n",
    "        result = classifier(text, candidate_labels)\n",
    "        return result[\"labels\"][0]   # take top predicted category\n",
    "    except:\n",
    "        return \"other\"\n",
    "\n",
    "# Apply to 100k sample (âš ï¸ slow without GPU, batch it if needed)\n",
    "df_sample[\"predicted_category\"] = df_sample[\"text\"].astype(str).apply(classify_comment)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Extract categories & keyword match\n",
    "# -----------------------------\n",
    "categories = df_sample[\"predicted_category\"].unique().tolist()\n",
    "print(\"Discovered categories:\", categories)\n",
    "\n",
    "# Build keyword dictionary from categories\n",
    "keyword_dict = {cat: [cat] for cat in categories}   # simplest mapping\n",
    "\n",
    "def keyword_match(text):\n",
    "    text = str(text).lower()\n",
    "    for cat, keywords in keyword_dict.items():\n",
    "        for kw in keywords:\n",
    "            if re.search(rf\"\\b{kw}\\b\", text):\n",
    "                return cat\n",
    "    return \"other\"\n",
    "\n",
    "# Apply keyword matching to all 5M\n",
    "df[\"category\"] = df[\"text\"].astype(str).apply(keyword_match)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Stratified Sampling\n",
    "# -----------------------------\n",
    "df_stratified, _ = train_test_split(\n",
    "    df, \n",
    "    train_size=100000, \n",
    "    stratify=df[\"category\"], \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Final stratified sample size:\", len(df_stratified))\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Save results\n",
    "# -----------------------------\n",
    "df_stratified.to_csv(\"CORRECTONE!!!stratified_sample_100k.csv\", index=False)\n",
    "df.to_parquet(\"comments_with_categories.parquet\", engine=\"pyarrow\", index=False)\n",
    "\n",
    "print(\"âœ… Done! Categories assigned & stratified sample created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28c2d29c-5033-4ec6-a291-87dc3da1dd36",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 18\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 1. Combine CSV files into one BIG BOY\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Replace 'your_folder/*.csv' with your actual folder path\u001b[39;00m\n\u001b[0;32m     10\u001b[0m files \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomments1.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomments2.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomments5.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     16\u001b[0m ]\n\u001b[1;32m---> 18\u001b[0m df_list \u001b[38;5;241m=\u001b[39m [pd\u001b[38;5;241m.\u001b[39mread_csv(f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m files]\n\u001b[0;32m     19\u001b[0m BIG_BOY \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(df_list, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBIG BOY shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, BIG_BOY\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m         nrows\n\u001b[0;32m   1925\u001b[0m     )\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\codecs.py:334\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.getstate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    331\u001b[0m     IncrementalDecoder\u001b[38;5;241m.\u001b[39mreset(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgetstate\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;66;03m# additional state info is always 0\u001b[39;00m\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msetstate\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;66;03m# ignore additional state info\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from transformers import pipeline\n",
    "import random\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Combine CSV files into one BIG BOY\n",
    "# ---------------------------\n",
    "# Replace 'your_folder/*.csv' with your actual folder path\n",
    "files = [\n",
    "    \"comments1.csv\",\n",
    "    \"comments2.csv\",\n",
    "    \"comments3.csv\",\n",
    "    \"comments4.csv\",\n",
    "    \"comments5.csv\"\n",
    "]\n",
    "\n",
    "df_list = [pd.read_csv(f) for f in files]\n",
    "BIG_BOY = pd.concat(df_list, ignore_index=True)\n",
    "print(\"BIG BOY shape:\", BIG_BOY.shape)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Initialize Zero-Shot Classifier\n",
    "# ---------------------------\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Define some candidate labels (categories you want to detect)\n",
    "candidate_labels = [\"politics\", \"gaming\", \"sports\", \"technology\", \"music\", \n",
    "                    \"education\", \"health\", \"entertainment\", \"finance\", \"other\"]\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Sample 20k three times and classify\n",
    "# ---------------------------\n",
    "def run_zero_shot(sample_df, run_name):\n",
    "    results = []\n",
    "    for comment in sample_df[\"comment_text\"].tolist():\n",
    "        try:\n",
    "            res = classifier(comment, candidate_labels)\n",
    "            results.append({\n",
    "                \"text\": comment,\n",
    "                \"label\": res[\"labels\"][0],  # top predicted category\n",
    "                \"scores\": res[\"scores\"][0]\n",
    "            })\n",
    "        except:\n",
    "            results.append({\n",
    "                \"text\": comment,\n",
    "                \"label\": \"error\",\n",
    "                \"scores\": None\n",
    "            })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# First 20k sample\n",
    "sample1 = BIG_BOY.sample(20000, random_state=42)\n",
    "classified1 = run_zero_shot(sample1, \"run1\")\n",
    "\n",
    "# Second 20k sample\n",
    "sample2 = BIG_BOY.sample(20000, random_state=123)\n",
    "classified2 = run_zero_shot(sample2, \"run2\")\n",
    "\n",
    "# Third 20k sample\n",
    "sample3 = BIG_BOY.sample(20000, random_state=999)\n",
    "classified3 = run_zero_shot(sample3, \"run3\")\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Save results\n",
    "# ---------------------------\n",
    "classified1.to_csv(\"classified_run1.csv\", index=False)\n",
    "classified2.to_csv(\"classified_run2.csv\", index=False)\n",
    "classified3.to_csv(\"classified_run3.csv\", index=False)\n",
    "\n",
    "print(\"Done! Classified three random 20k samples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e58d4b8-8197-49e6-8b54-9472034697e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 11)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NMF_sample = pd.read_csv(\"comments_sample_100k_stratified.csv\")\n",
    "NMF_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ead2a3e-d629-400d-98ca-f5930242b7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['commentId', 'channelId', 'videoId', 'authorId', 'textOriginal',\n",
       "       'text_clean', 'parentCommentId', 'likeCount', 'publishedAt',\n",
       "       'updatedAt', 'topic'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NMF_sample.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0081b7aa-becb-4bc1-a4f4-490a8609bbc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11c44f9-58cc-488b-b6c1-c49bb6a91c76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87376c72-2c1a-41fc-81da-d86e0fa8c84a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
