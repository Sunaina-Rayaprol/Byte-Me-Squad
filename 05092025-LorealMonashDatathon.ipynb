{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06bee039-5f10-4da9-94d5-ddf4c88ff0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Combined dataset saved as BIG_BOY.csv\n",
      "Shape of BIG BOY: (4725012, 10)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of CSV files\n",
    "csv_files = [\n",
    "    \"comments1.csv\",\n",
    "    \"comments2.csv\",\n",
    "    \"comments3.csv\",\n",
    "    \"comments4.csv\",\n",
    "    \"comments5.csv\"\n",
    "]\n",
    "\n",
    "# Read and combine into one dataframe\n",
    "df_list = [pd.read_csv(file) for file in csv_files]\n",
    "big_boy = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Save the combined dataset\n",
    "big_boy.to_csv(\"BIG_BOY.csv\", index=False)\n",
    "\n",
    "print(\"✅ Combined dataset saved as BIG_BOY.csv\")\n",
    "print(\"Shape of BIG BOY:\", big_boy.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b56be954-1855-4cf3-b1aa-e0825aa4554e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Random sample of 100,000 created\n",
      "Shape of sample: (100000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Randomly choose 20k rows\n",
    "sample_100k = big_boy.sample(n=100000, random_state=42)  # random_state makes it reproducible\n",
    "\n",
    "print(\"✅ Random sample of 100,000 created\")\n",
    "print(\"Shape of sample:\", sample_100k.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd7a928a-1f56-49e4-be3c-35ab311d62e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                     kind  commentId  channelId  videoId  authorId  \\\n",
       "3956772  youtube#comment    1999899      33069    36229   2377260   \n",
       "3284632  youtube#comment    1235288      37371     5214   2495217   \n",
       "1185839  youtube#comment    1120879      18073    76480   2689034   \n",
       "238094   youtube#comment    3806774      46757    78060   3531698   \n",
       "1107951  youtube#comment    1874941      33445    91263    968927   \n",
       "...                  ...        ...        ...      ...       ...   \n",
       "811650   youtube#comment    2788176      41246    87323   2446840   \n",
       "32900    youtube#comment     668979        805    18615   3561486   \n",
       "2355010  youtube#comment    4642725      33445    91263   3547007   \n",
       "4197186  youtube#comment     365684      11679      743   1946253   \n",
       "1421695  youtube#comment    2350664      16876    18101   3254686   \n",
       "\n",
       "                                              textOriginal  parentCommentId  \\\n",
       "3956772  💙💙💙💙💙💙💙💙💙💙💙💙💙💙💙💙💙💙💙💙💙💙💙💙💙💙💙💙💙💙💙💙💙💙💙💙💙💙💙💙💙💙💙💙💙💙...              NaN   \n",
       "3284632  Reasons why heavily attractive people (you💗) d...              NaN   \n",
       "1185839                                                 🇵🇰              NaN   \n",
       "238094              \"hmm what's next\" \\n* hair falls out *              NaN   \n",
       "1107951                          You are so pretty gurl ☺️              NaN   \n",
       "...                                                    ...              ...   \n",
       "811650                                      I kile you 😢😮😮              NaN   \n",
       "32900                                                Güzel              NaN   \n",
       "2355010                                         AN AHOGE!!              NaN   \n",
       "4197186  DIEGO❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤...        2102498.0   \n",
       "1421695                                   Boy lag Rahi ha😂              NaN   \n",
       "\n",
       "         likeCount                publishedAt                  updatedAt  \n",
       "3956772          0  2025-03-15 12:09:40+00:00  2025-03-15 12:09:40+00:00  \n",
       "3284632          2  2023-12-26 04:22:37+00:00  2023-12-26 04:23:09+00:00  \n",
       "1185839          0  2022-10-02 12:58:13+00:00  2022-10-02 12:58:13+00:00  \n",
       "238094           1  2022-07-23 06:39:15+00:00  2022-07-23 06:39:15+00:00  \n",
       "1107951          0  2024-07-01 10:41:25+00:00  2024-07-01 10:41:25+00:00  \n",
       "...            ...                        ...                        ...  \n",
       "811650           0  2023-09-18 12:08:11+00:00  2023-09-18 12:08:11+00:00  \n",
       "32900            0  2023-02-28 17:36:58+00:00  2023-02-28 17:36:58+00:00  \n",
       "2355010          0  2024-07-26 07:29:03+00:00  2024-07-26 07:29:03+00:00  \n",
       "4197186          0  2024-06-04 22:05:34+00:00  2024-06-04 22:05:34+00:00  \n",
       "1421695          0  2024-04-08 08:20:06+00:00  2024-04-08 08:20:06+00:00  \n",
       "\n",
       "[100000 rows x 10 columns]>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_100k.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091746ff-ab23-45d5-8941-e08f4d6d87d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ca2cda5-76aa-4a8e-b00c-71e1497499a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6bd2d59-ed73-4c17-9f8b-273b04ccb504",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rayap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab8dea8e-544e-453e-8bcf-e42a38589aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessing complete. Cleaned text saved in 'clean_comment' column.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load your 100k sample\n",
    "df = sample_100k  # adjust filename\n",
    "\n",
    "# English stopwords (download if needed: nltk.download(\"stopwords\"))\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    # Lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove emojis and non-ASCII characters\n",
    "    text = text.encode(\"ascii\", \"ignore\").decode()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    \n",
    "    # Remove digits\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in text.split() if word not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Apply cleaning to the \"comment\" column (adjust column name if needed)\n",
    "df[\"clean_comment\"] = df[\"textOriginal\"].apply(clean_text)\n",
    "\n",
    "# Save cleaned version\n",
    "df.to_csv(\"sample_100k_clean.csv\", index=False)\n",
    "\n",
    "print(\"✅ Preprocessing complete. Cleaned text saved in 'clean_comment' column.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40973a73-8eaf-48d7-9498-ca154e81d4a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                  NaN\n",
       "1    reasons heavily attractive people dont get ask...\n",
       "2                                                  NaN\n",
       "3                            hmm whats next hair falls\n",
       "4                                          pretty gurl\n",
       "5                                         best ad ever\n",
       "6                                            beautiful\n",
       "7    doesnt seem like micheal whats admit fault wom...\n",
       "8                          larga de ser ridcula mulher\n",
       "9                              rupey kat overacting ke\n",
       "Name: clean_comment, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLEANsample100k = pd.read_csv(\"sample_100k_clean.csv\")\n",
    "CLEANsample100k['clean_comment'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0623bb54-db9c-41b0-9aa0-e9a1cf7f4eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built training sample: 200000 rows (from ~200000 seen).\n",
      "Fitting TF-IDF vectorizer on sample...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rayap\\anaconda\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTM sample shape: (200000, 20000)\n",
      "Fitting NMF (30 topics)...\n",
      "NMF fit complete.\n",
      "\n",
      "--- Top words per topic ---\n",
      "Topic 0: ❤ | muslim | gorgeous | hindu | ginger | amazing | bald | best | belle | stunning | 😢 | mashallah | ️ | perfect | africa\n",
      "Topic 1: . | just | make | skin | people | women | video | face | \" | really | … | time | beauty | want | great\n",
      "Topic 2: ! | thank | gorgeous | amazing | stunning | omg | great | looks | absolutely | thanks | yes | good | did | oh | :\n",
      "Topic 3: 😂 | 😅 | 🤣 | 😢 | lol | hai | said | bro | ho | funny | face | brother | bhi | ki | h\n",
      "Topic 4: beautiful | way | looking | just | u | looks | make | 🤩 | ur | bald | skin | absolutely | woman | eyes | wig\n",
      "Topic 5: ' | s | t | don | m | \" | just | doesn | know | need | didn | people | think | way | ve\n",
      "Topic 6: 😊 | thank | thanks | 😅 | happy | 😢 | bald | muslim | hindu | hi | good | ginger | ☺ | welcome | video\n",
      "Topic 7: 😍 | 😘 | gorgeous | 🤩 | 💖 | looking | amazing | 👌 | ✨ | stunning | 🔥 | share | video | 💕 | 🤗\n",
      "Topic 8: ? | did | u | song | use | does | shade | face | lipstick | link | \" | product | foundation | gay | know\n",
      "Topic 9: 💜 | 💙 | bts | army | purple | 💚 | 🖤 | 💛 | 💗 | 🧡 | korean | 💖 | 💕 | 🤍 | 🤎\n",
      "Topic 10: ️ | ♥ | ☺ | ‍ | thank | ❣ | ✨ | 🔥 | ♀ | 💕 | 😘 | share | 💖 | ☠ | 💗\n",
      "Topic 11: love | u | videos | pakistan | omg | ur | ginger | 😘 | bald | 💕 | video | voice | make | natural | song\n",
      "Topic 12: nice | video | 👌 | looking | di | make | mam | didi | 🙂 | looks | sharing | bro | work | 👍 | ☺\n",
      "Topic 13: @ | ​ | - | yes | thank | thanks | ok | lol | 🤣 | 😭 | oh | u | welcome | just | yeah\n",
      "Topic 14: look | night | good | u | day | amazing | gorgeous | better | make | great | best | stunning | 🌃 | bald | wig\n",
      "Topic 15: india | best | 🇮 | 🇳 | pakistan | east | west | 👇 | 😘 | make | vs | 🇵 | ka | 🇰 | n\n",
      "Topic 16: pretty | ur | omg | u | way | 🤩 | looks | soo | really | wig | bald | sooo | look | looking | r\n",
      "Topic 17: wow | amazing | 🤩 | 👌 | gorgeous | 😲 | super | just | looking | 😳 | looks | cool | 👏 | di | great\n",
      "Topic 18: ’ | s | t | don | m | just | need | … | “ | know | think | people | doesn | ” | make\n",
      "Topic 19: cute | omg | boy | looking | voice | soo | super | looks | s | 🥰 | really | sooo | ho | didi | ur\n",
      "Topic 20: 😮 | 😢 | 😅 | omg | 😱 | 💨 | ‍ | real | hai | 👏 | que | oh | did | really | make\n",
      "Topic 21: hair | curly | long | 😭 | straight | cut | black | natural | style | short | blonde | gorgeous | wavy | ur | did\n",
      "Topic 22: makeup | need | good | 💄 | korean | better | best | wear | t | hai | don | u | amazing | face | artist\n",
      "Topic 23: , | just | \" | - | make | ) | skin | : | people | yes | thank | ( | face | women | think\n",
      "Topic 24: 🎉 | 😢 | 😅 | happy | birthday | hindu | congratulations | best | 🥳 | 👌 | 🎊 | slay | great | 10 | 👑\n",
      "Topic 25: 🥰 | thank | 😘 | 🙏 | share | dear | 💕 | 💗 | 💞 | 🤩 | 💖 | 👌 | aww | 🤗 | video\n",
      "Topic 26: like | looks | just | u | looking | make | look | bro | better | really | people | : | feel | 😅 | 😭\n",
      "Topic 27: 👍 | good | 👌 | 🏻 | looks | 👏 | job | 🙏 | 😘 | looking | 🏼 | 💯 | really | great | 🌹\n",
      "Topic 28: indian | best | 🇳 | 🇮 | make | pakistani | 👌 | m | pakistan | 😘 | super | american | mekup | looking | bride\n",
      "Topic 29: girl | happy | u | angry | boy | gorgeous | 😭 | slay | ur | sad | r | 😅 | yes | just | did\n",
      "---------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 24it [05:01, 12.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All chunks processed and saved.\n",
      "Total docs processed: 4725012, across 24 chunks.\n",
      "Topic counts (sample): {0: 405422, 1: 275926, 2: 210693, 3: 190966, 4: 74528, 5: 144227, 6: 105778, 7: 68563, 8: 241746, 9: 65956, 10: 211182, 11: 103846, 12: 50468, 13: 108922, 14: 93366, 15: 82219, 16: 99540, 17: 52157, 18: 277952, 19: 44697, 20: 101607, 21: 159245, 22: 292234, 23: 415064, 24: 67276, 25: 126540, 26: 342162, 27: 131093, 28: 37153, 29: 144484}\n",
      "Sampling targets per topic computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling from chunks:   0%|                                                                     | 0/24 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final sampled rows: 100000\n",
      "Saved: comments_sample_100k_stratified.csv\n",
      "Saved vectorizer and NMF model to disk (joblib).\n"
     ]
    }
   ],
   "source": [
    "# FILE: nmf_topic_stratified_sampling.py\n",
    "# Run with: python nmf_topic_stratified_sampling.py\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# -----------------------\n",
    "# Config - tune these\n",
    "# -----------------------\n",
    "INPUT_FILES = [\n",
    "    \"comments1.csv\",\n",
    "    \"comments2.csv\",\n",
    "    \"comments3.csv\",\n",
    "    \"comments4.csv\",\n",
    "    \"comments5.csv\",\n",
    "]\n",
    "CLEANED_CHUNKS_DIR = \"topic_chunks\"          \n",
    "os.makedirs(CLEANED_CHUNKS_DIR, exist_ok=True)\n",
    "\n",
    "SAMPLE_SIZE = 200_000        \n",
    "N_TOPICS = 30               \n",
    "MAX_FEATURES = 20000         \n",
    "CHUNKSIZE = 200_000         \n",
    "RANDOM_STATE = 42\n",
    "FINAL_SAMPLE_SIZE = 100_000  \n",
    "\n",
    "# -----------------------\n",
    "# Helpers\n",
    "# -----------------------\n",
    "url_re = re.compile(r\"(https?://\\S+|www\\.\\S+)\", re.IGNORECASE)\n",
    "ws_re = re.compile(r\"\\s+\")\n",
    "\n",
    "def clean_text_keep_emojis(s):\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    x = url_re.sub(\" \", s)\n",
    "    x = x.replace(\"\\r\", \" \").replace(\"\\n\", \" \")\n",
    "    x = ws_re.sub(\" \", x).strip()\n",
    "    return x.lower()\n",
    "\n",
    "def simple_tokenizer(text):\n",
    "    return re.findall(r\"\\w+|[^\\s\\w]\", text, flags=re.UNICODE)\n",
    "\n",
    "def csv_chunks(files, chunksize=100_000, usecols=None):\n",
    "    idx = 0\n",
    "    for fname in files:\n",
    "        for chunk in pd.read_csv(fname, usecols=usecols, chunksize=chunksize, dtype=str):\n",
    "            yield chunk, fname, idx\n",
    "            idx += 1\n",
    "\n",
    "def build_training_sample(files, sample_size=SAMPLE_SIZE, chunksize=CHUNKSIZE):\n",
    "    rng = np.random.RandomState(RANDOM_STATE)\n",
    "    sample_texts = []\n",
    "    sample_ids = []\n",
    "    total_seen = 0\n",
    "    for chunk, fname, _ in csv_chunks(files, chunksize=chunksize, usecols=[\"commentId\", \"textOriginal\"]):\n",
    "        chunk[\"text_clean\"] = chunk[\"textOriginal\"].apply(clean_text_keep_emojis)\n",
    "        sample_texts.extend(chunk[\"text_clean\"].tolist())\n",
    "        sample_ids.extend(chunk[\"commentId\"].tolist())\n",
    "        total_seen += len(chunk)\n",
    "        if len(sample_texts) > sample_size * 1.5:\n",
    "            chosen_idx = rng.choice(len(sample_texts), size=sample_size, replace=False)\n",
    "            sample_texts = [sample_texts[i] for i in chosen_idx]\n",
    "            sample_ids = [sample_ids[i] for i in chosen_idx]\n",
    "        if len(sample_texts) >= sample_size:\n",
    "            break\n",
    "    if len(sample_texts) > sample_size:\n",
    "        chosen_idx = rng.choice(len(sample_texts), size=sample_size, replace=False)\n",
    "        sample_texts = [sample_texts[i] for i in chosen_idx]\n",
    "        sample_ids = [sample_ids[i] for i in chosen_idx]\n",
    "    sample_df = pd.DataFrame({\"commentId\": sample_ids, \"text_clean\": sample_texts})\n",
    "    print(f\"Built training sample: {len(sample_df)} rows (from ~{total_seen} seen).\")\n",
    "    return sample_df\n",
    "\n",
    "def fit_vectorizer_and_nmf(sample_df, n_topics=N_TOPICS, max_features=MAX_FEATURES, n_top_words=15):\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=max_features,\n",
    "        tokenizer=simple_tokenizer,\n",
    "        lowercase=True,\n",
    "        stop_words=\"english\"\n",
    "    )\n",
    "    print(\"Fitting TF-IDF vectorizer on sample...\")\n",
    "    dtm_sample = vectorizer.fit_transform(sample_df[\"text_clean\"])\n",
    "    print(f\"DTM sample shape: {dtm_sample.shape}\")\n",
    "\n",
    "    print(f\"Fitting NMF ({n_topics} topics)...\")\n",
    "    nmf = NMF(\n",
    "        n_components=n_topics,\n",
    "        random_state=RANDOM_STATE,\n",
    "        init=\"nndsvda\",\n",
    "        max_iter=200,\n",
    "        tol=1e-4\n",
    "    )\n",
    "    W = nmf.fit_transform(dtm_sample)  \n",
    "    H = nmf.components_                \n",
    "    print(\"NMF fit complete.\")\n",
    "\n",
    "    # Print top words for each topic\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    topic_keywords = {}\n",
    "    print(\"\\n--- Top words per topic ---\")\n",
    "    for topic_idx, topic in enumerate(H):\n",
    "        top_indices = topic.argsort()[::-1][:n_top_words]\n",
    "        top_words = [feature_names[i] for i in top_indices]\n",
    "        topic_keywords[topic_idx] = top_words\n",
    "        print(f\"Topic {topic_idx}: {' | '.join(top_words)}\")\n",
    "    print(\"---------------------------\\n\")\n",
    "\n",
    "    return vectorizer, nmf, topic_keywords\n",
    "\n",
    "def apply_model_and_write_chunks(files, vectorizer, nmf, chunksize=CHUNKSIZE, out_dir=CLEANED_CHUNKS_DIR):\n",
    "    chunk_idx = 0\n",
    "    topic_counts = {}\n",
    "    for chunk, fname, _ in tqdm(csv_chunks(files, chunksize=chunksize, usecols=None), desc=\"Processing chunks\"):\n",
    "        if \"textOriginal\" not in chunk.columns:\n",
    "            if \"text\" in chunk.columns:\n",
    "                chunk[\"textOriginal\"] = chunk[\"text\"]\n",
    "            else:\n",
    "                chunk[\"textOriginal\"] = \"\"\n",
    "        chunk[\"text_clean\"] = chunk[\"textOriginal\"].apply(clean_text_keep_emojis)\n",
    "        dtm = vectorizer.transform(chunk[\"text_clean\"])\n",
    "        topic_dist = nmf.transform(dtm)          \n",
    "        dominant_topic = np.argmax(topic_dist, axis=1)\n",
    "        chunk[\"topic\"] = dominant_topic.astype(int)\n",
    "\n",
    "        unique, counts = np.unique(dominant_topic, return_counts=True)\n",
    "        for u, c in zip(unique, counts):\n",
    "            topic_counts[int(u)] = topic_counts.get(int(u), 0) + int(c)\n",
    "\n",
    "        out_path = os.path.join(out_dir, f\"chunk_{chunk_idx:05d}.parquet\")\n",
    "        keep_cols = [c for c in [\"commentId\", \"channelId\", \"videoId\", \"authorId\",\n",
    "                                 \"textOriginal\", \"text_clean\", \"parentCommentId\",\n",
    "                                 \"likeCount\", \"publishedAt\", \"updatedAt\", \"topic\"] if c in chunk.columns]\n",
    "        chunk[keep_cols].to_parquet(out_path, index=False)\n",
    "        chunk_idx += 1\n",
    "\n",
    "    print(\"All chunks processed and saved.\")\n",
    "    return topic_counts, chunk_idx\n",
    "\n",
    "def stratified_sample_from_chunks(chunks_dir, topic_counts, total_docs, topic_keywords,\n",
    "                                  final_n=FINAL_SAMPLE_SIZE, random_state=RANDOM_STATE):\n",
    "    topics = sorted(topic_counts.keys())\n",
    "    proportions = {t: topic_counts[t] / total_docs for t in topics}\n",
    "    target_raw = {t: proportions[t] * final_n for t in topics}\n",
    "    target = {t: int(math.floor(target_raw[t])) for t in topics}\n",
    "    deficit = final_n - sum(target.values())\n",
    "    if deficit > 0:\n",
    "        remainders = sorted(((t, target_raw[t] - target[t]) for t in topics), key=lambda x: x[1], reverse=True)\n",
    "        for i in range(deficit):\n",
    "            target[remainders[i % len(remainders)][0]] += 1\n",
    "\n",
    "    print(\"Sampling targets per topic computed.\")\n",
    "\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    buffers = {t: [] for t in topics}\n",
    "    filled = {t: 0 for t in topics}\n",
    "\n",
    "    chunk_files = sorted(glob.glob(os.path.join(chunks_dir, \"chunk_*.parquet\")))\n",
    "    for pf in tqdm(chunk_files, desc=\"Sampling from chunks\"):\n",
    "        df = pd.read_parquet(pf)\n",
    "        for t in topics:\n",
    "            need = target[t] - filled[t]\n",
    "            if need <= 0:\n",
    "                continue\n",
    "            sub = df.loc[df[\"topic\"] == t]\n",
    "            if len(sub) == 0:\n",
    "                continue\n",
    "            take = sub.sample(n=min(need, len(sub)), random_state=rng)\n",
    "            buffers[t].append(take)\n",
    "            filled[t] += len(take)\n",
    "        if sum(filled.values()) >= final_n:\n",
    "            break\n",
    "\n",
    "    parts = []\n",
    "    for t in topics:\n",
    "        if buffers[t]:\n",
    "            df_topic = pd.concat(buffers[t], ignore_index=True)\n",
    "            df_topic[\"category\"] = \", \".join(topic_keywords[t][:5])  # category = top 5 keywords\n",
    "            parts.append(df_topic)\n",
    "\n",
    "    sample_df = pd.concat(parts, ignore_index=True)\n",
    "    if len(sample_df) > final_n:\n",
    "        sample_df = sample_df.sample(n=final_n, random_state=rng).reset_index(drop=True)\n",
    "\n",
    "    print(f\"Final sampled rows: {len(sample_df)}\")\n",
    "    return sample_df\n",
    "\n",
    "def main():\n",
    "    sample_df = build_training_sample(INPUT_FILES, sample_size=SAMPLE_SIZE, chunksize=CHUNKSIZE)\n",
    "    vectorizer, nmf, topic_keywords = fit_vectorizer_and_nmf(sample_df, n_topics=N_TOPICS, max_features=MAX_FEATURES)\n",
    "\n",
    "    topic_counts, n_chunks = apply_model_and_write_chunks(INPUT_FILES, vectorizer, nmf, chunksize=CHUNKSIZE, out_dir=CLEANED_CHUNKS_DIR)\n",
    "    total_docs = sum(topic_counts.values())\n",
    "    print(f\"Total docs processed: {total_docs}, across {n_chunks} chunks.\")\n",
    "    print(\"Topic counts (sample):\", topic_counts)\n",
    "\n",
    "    sample_df_final = stratified_sample_from_chunks(CLEANED_CHUNKS_DIR, topic_counts, total_docs, topic_keywords,\n",
    "                                                    final_n=FINAL_SAMPLE_SIZE, random_state=RANDOM_STATE)\n",
    "    sample_df_final.to_csv(\"comments_sample_100k_stratified.csv\", index=False)\n",
    "    print(\"Saved: comments_sample_100k_stratified.csv\")\n",
    "\n",
    "    try:\n",
    "        import joblib\n",
    "        joblib.dump(vectorizer, \"tfidf_vectorizer.joblib\")\n",
    "        joblib.dump(nmf, \"nmf_model.joblib\")\n",
    "        print(\"Saved vectorizer and NMF model to disk (joblib).\")\n",
    "    except Exception as e:\n",
    "        print(\"joblib save failed:\", e)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddb1449d-5954-4d07-b6ae-d9e121f223fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 12)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NMF_sample = pd.read_csv(\"comments_sample_100k_stratified.csv\")\n",
    "NMF_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20a6fa46-a033-4d4e-9019-5f6ae0356e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['commentId', 'channelId', 'videoId', 'authorId', 'textOriginal',\n",
       "       'text_clean', 'parentCommentId', 'likeCount', 'publishedAt',\n",
       "       'updatedAt', 'topic', 'category'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NMF_sample.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bac21607-2c50-4618-a65b-ae8b9da51728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['❤, muslim, gorgeous, hindu, ginger',\n",
       "       '., just, make, skin, people',\n",
       "       '!, thank, gorgeous, amazing, stunning', '😂, 😅, 🤣, 😢, lol',\n",
       "       'beautiful, way, looking, just, u', \"', s, t, don, m\",\n",
       "       '😊, thank, thanks, 😅, happy', '😍, 😘, gorgeous, 🤩, 💖',\n",
       "       '?, did, u, song, use', '💜, 💙, bts, army, purple',\n",
       "       '️, ♥, ☺, \\u200d, thank', 'love, u, videos, pakistan, omg',\n",
       "       'nice, video, 👌, looking, di', '@, \\u200b, -, yes, thank',\n",
       "       'look, night, good, u, day', 'india, best, 🇮, 🇳, pakistan',\n",
       "       'pretty, ur, omg, u, way', 'wow, amazing, 🤩, 👌, gorgeous',\n",
       "       '’, s, t, don, m', 'cute, omg, boy, looking, voice',\n",
       "       '😮, 😢, 😅, omg, 😱', 'hair, curly, long, 😭, straight',\n",
       "       'makeup, need, good, 💄, korean', ',, just, \", -, make',\n",
       "       '🎉, 😢, 😅, happy, birthday', '🥰, thank, 😘, 🙏, share',\n",
       "       'like, looks, just, u, looking', '👍, good, 👌, 🏻, looks',\n",
       "       'indian, best, 🇳, 🇮, make', 'girl, happy, u, angry, boy'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NMF_sample['category'].unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
