{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "491e8112-55df-484e-925d-407f77181709",
      "metadata": {
        "id": "491e8112-55df-484e-925d-407f77181709"
      },
      "source": [
        "## COMBINING DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "56e9cc1d-4014-4e40-ad77-8b7a8080142a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56e9cc1d-4014-4e40-ad77-8b7a8080142a",
        "outputId": "abf241c2-bb81-4fa0-9283-e5718819b1c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: polars in /usr/local/lib/python3.12/dist-packages (1.25.2)\n"
          ]
        }
      ],
      "source": [
        "! pip install polars\n",
        "import polars as pl"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OoG-twd-PbKx"
      },
      "id": "OoG-twd-PbKx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "swLQurqyP77q"
      },
      "id": "swLQurqyP77q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "id": "a53d16fd",
        "outputId": "73122ee8-52d6-4c1f-d8c0-3ed485fe131e"
      },
      "source": [
        "df2 = pl.read_csv('/content/comments2.csv')\n",
        "display(df2.head())"
      ],
      "id": "a53d16fd",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "shape: (5, 10)\n",
              "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
              "â”‚ kind       â”† commentId â”† channelId â”† videoId â”† â€¦ â”† parentCom â”† likeCount â”† published â”† updatedAt â”‚\n",
              "â”‚ ---        â”† ---       â”† ---       â”† ---     â”†   â”† mentId    â”† ---       â”† At        â”† ---       â”‚\n",
              "â”‚ str        â”† i64       â”† i64       â”† i64     â”†   â”† ---       â”† i64       â”† ---       â”† str       â”‚\n",
              "â”‚            â”†           â”†           â”†         â”†   â”† i64       â”†           â”† str       â”†           â”‚\n",
              "â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
              "â”‚ youtube#co â”† 3403605   â”† 23845     â”† 31993   â”† â€¦ â”† null      â”† 0         â”† 2025-04-0 â”† 2025-04-0 â”‚\n",
              "â”‚ mment      â”†           â”†           â”†         â”†   â”†           â”†           â”† 3 13:49:2 â”† 3 13:49:2 â”‚\n",
              "â”‚            â”†           â”†           â”†         â”†   â”†           â”†           â”† 0+00:00   â”† 0+00:00   â”‚\n",
              "â”‚ youtube#co â”† 3957372   â”† 32163     â”† 88966   â”† â€¦ â”† null      â”† 0         â”† 2024-07-1 â”† 2024-07-1 â”‚\n",
              "â”‚ mment      â”†           â”†           â”†         â”†   â”†           â”†           â”† 7 08:51:1 â”† 7 08:51:1 â”‚\n",
              "â”‚            â”†           â”†           â”†         â”†   â”†           â”†           â”† 9+00:00   â”† 9+00:00   â”‚\n",
              "â”‚ youtube#co â”† 1272592   â”† 18073     â”† 69091   â”† â€¦ â”† null      â”† 0         â”† 2023-01-2 â”† 2023-01-2 â”‚\n",
              "â”‚ mment      â”†           â”†           â”†         â”†   â”†           â”†           â”† 9 05:01:0 â”† 9 05:01:0 â”‚\n",
              "â”‚            â”†           â”†           â”†         â”†   â”†           â”†           â”† 6+00:00   â”† 6+00:00   â”‚\n",
              "â”‚ youtube#co â”† 4006296   â”† 47781     â”† 38945   â”† â€¦ â”† null      â”† 0         â”† 2022-07-2 â”† 2022-07-2 â”‚\n",
              "â”‚ mment      â”†           â”†           â”†         â”†   â”†           â”†           â”† 7 22:38:4 â”† 7 22:38:4 â”‚\n",
              "â”‚            â”†           â”†           â”†         â”†   â”†           â”†           â”† 1+00:00   â”† 1+00:00   â”‚\n",
              "â”‚ youtube#co â”† 521568    â”† 13736     â”† 92721   â”† â€¦ â”† null      â”† 0         â”† 2025-05-0 â”† 2025-05-0 â”‚\n",
              "â”‚ mment      â”†           â”†           â”†         â”†   â”†           â”†           â”† 4 20:28:4 â”† 4 20:28:4 â”‚\n",
              "â”‚            â”†           â”†           â”†         â”†   â”†           â”†           â”† 8+00:00   â”† 8+00:00   â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
            ],
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (5, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>kind</th><th>commentId</th><th>channelId</th><th>videoId</th><th>authorId</th><th>textOriginal</th><th>parentCommentId</th><th>likeCount</th><th>publishedAt</th><th>updatedAt</th></tr><tr><td>str</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>str</td><td>i64</td><td>i64</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;youtube#comment&quot;</td><td>3403605</td><td>23845</td><td>31993</td><td>2874725</td><td>&quot;my 5 year old brother said dolâ€¦</td><td>null</td><td>0</td><td>&quot;2025-04-03 13:49:20+00:00&quot;</td><td>&quot;2025-04-03 13:49:20+00:00&quot;</td></tr><tr><td>&quot;youtube#comment&quot;</td><td>3957372</td><td>32163</td><td>88966</td><td>757766</td><td>&quot;You look very beautiful and cuâ€¦</td><td>null</td><td>0</td><td>&quot;2024-07-17 08:51:19+00:00&quot;</td><td>&quot;2024-07-17 08:51:19+00:00&quot;</td></tr><tr><td>&quot;youtube#comment&quot;</td><td>1272592</td><td>18073</td><td>69091</td><td>858228</td><td>&quot;American ğŸ‡ºğŸ‡²&quot;</td><td>null</td><td>0</td><td>&quot;2023-01-29 05:01:06+00:00&quot;</td><td>&quot;2023-01-29 05:01:06+00:00&quot;</td></tr><tr><td>&quot;youtube#comment&quot;</td><td>4006296</td><td>47781</td><td>38945</td><td>2179100</td><td>&quot;MASHA&#x27;ALLAHâ™¥â™¥â™¥&quot;</td><td>null</td><td>0</td><td>&quot;2022-07-27 22:38:41+00:00&quot;</td><td>&quot;2022-07-27 22:38:41+00:00&quot;</td></tr><tr><td>&quot;youtube#comment&quot;</td><td>521568</td><td>13736</td><td>92721</td><td>364652</td><td>&quot;As a scene kid, YOU KILLED ITğŸ˜»â€¦</td><td>null</td><td>0</td><td>&quot;2025-05-04 20:28:48+00:00&quot;</td><td>&quot;2025-05-04 20:28:48+00:00&quot;</td></tr></tbody></table></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "b2d7692b",
        "outputId": "c2da9ee8-ad87-4230-f394-f507e6aa2079"
      },
      "source": [
        "def clean_text(text: str) -> str:\n",
        "    if text is None:\n",
        "        return None\n",
        "\n",
        "    # âœ… Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # âœ… Remove punctuation, numbers, and symbols but KEEP emojis & non-English chars\n",
        "    # \\p{L} = letters, \\p{M} = diacritics, \\p{Zs} = spaces, \\p{Emoji} not directly supported in regex\n",
        "    # So instead: only remove ASCII punctuation/numbers\n",
        "    text = re.sub(r\"[0-9!\\\"#$%&'()*+,\\-./:;<=>?@\\[\\]^_`{|}~]\", \" \", text)\n",
        "\n",
        "    # âœ… Remove extra spaces\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply cleaning with Polars\n",
        "df2 = df2.with_columns(\n",
        "    pl.col(\"textOriginal\").map_elements(clean_text, return_dtype=pl.Utf8).alias(\"cleaned_comment\")\n",
        ")\n",
        "\n",
        "df2.select([\"textOriginal\", \"cleaned_comment\"]).head(10)"
      ],
      "id": "b2d7692b",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "shape: (10, 2)\n",
              "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
              "â”‚ textOriginal                    â”† cleaned_comment                 â”‚\n",
              "â”‚ ---                             â”† ---                             â”‚\n",
              "â”‚ str                             â”† str                             â”‚\n",
              "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
              "â”‚ my 5 year old brother said dolâ€¦ â”† my year old brother said dolphâ€¦ â”‚\n",
              "â”‚ You look very beautiful and cuâ€¦ â”† you look very beautiful and cuâ€¦ â”‚\n",
              "â”‚ American ğŸ‡ºğŸ‡²                     â”† american ğŸ‡ºğŸ‡²                     â”‚\n",
              "â”‚ MASHA'ALLAHâ™¥â™¥â™¥                  â”† masha allahâ™¥â™¥â™¥                  â”‚\n",
              "â”‚ As a scene kid, YOU KILLED      â”† as a scene kid you killed       â”‚\n",
              "â”‚ ITğŸ˜»â€¦                           â”† itğŸ˜»â€¼â€¦                          â”‚\n",
              "â”‚ They all are cute but that blaâ€¦ â”† they all are cute but that blaâ€¦ â”‚\n",
              "â”‚ Scrubbing is a big NO.          â”† scrubbing is a big no           â”‚\n",
              "â”‚ Ooooo bhai ,,,maro mujhe maro   â”† ooooo bhai maro mujhe maro      â”‚\n",
              "â”‚ Nice as Heaven                  â”† nice as heaven                  â”‚\n",
              "â”‚ It looks cute, but why does thâ€¦ â”† it looks cute but why does theâ€¦ â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
            ],
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (10, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>textOriginal</th><th>cleaned_comment</th></tr><tr><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;my 5 year old brother said dolâ€¦</td><td>&quot;my year old brother said dolphâ€¦</td></tr><tr><td>&quot;You look very beautiful and cuâ€¦</td><td>&quot;you look very beautiful and cuâ€¦</td></tr><tr><td>&quot;American ğŸ‡ºğŸ‡²&quot;</td><td>&quot;american ğŸ‡ºğŸ‡²&quot;</td></tr><tr><td>&quot;MASHA&#x27;ALLAHâ™¥â™¥â™¥&quot;</td><td>&quot;masha allahâ™¥â™¥â™¥&quot;</td></tr><tr><td>&quot;As a scene kid, YOU KILLED ITğŸ˜»â€¦</td><td>&quot;as a scene kid you killed itğŸ˜»â€¼â€¦</td></tr><tr><td>&quot;They all are cute but that blaâ€¦</td><td>&quot;they all are cute but that blaâ€¦</td></tr><tr><td>&quot;Scrubbing is a big NO.&quot;</td><td>&quot;scrubbing is a big no&quot;</td></tr><tr><td>&quot;Ooooo bhai ,,,maro mujhe maro&quot;</td><td>&quot;ooooo bhai maro mujhe maro&quot;</td></tr><tr><td>&quot;Nice as Heaven&quot;</td><td>&quot;nice as heaven&quot;</td></tr><tr><td>&quot;It looks cute, but why does thâ€¦</td><td>&quot;it looks cute but why does theâ€¦</td></tr></tbody></table></div>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4f25228",
        "outputId": "601f59c1-082e-46c9-f1a1-a81a9e1f420e"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# -----------------------\n",
        "# Simple Heuristic Spam Detector\n",
        "# -----------------------\n",
        "\n",
        "def is_spam(comment: str) -> bool:\n",
        "    \"\"\"Return True if the comment looks like spam based on simple rules.\"\"\"\n",
        "    if not isinstance(comment, str):\n",
        "        return False\n",
        "\n",
        "    text = comment.lower()\n",
        "\n",
        "    # Rule 1: contains a link\n",
        "    if re.search(r\"(http|www\\.|\\.com|\\.net|\\.org|\\.io|\\.co)\", text):\n",
        "        return True\n",
        "\n",
        "    # Rule 2: spammy keywords\n",
        "    if re.search(r\"(subscribe|check my channel|giveaway|free|click here|follow me|dm me)\", text):\n",
        "        return True\n",
        "\n",
        "    # Rule 3: too many emojis/symbols\n",
        "    if len(re.findall(r\"[^\\w\\s]\", comment)) > 10:\n",
        "        return True\n",
        "\n",
        "    # Rule 4: ultra-short and meaningless\n",
        "    tokens = comment.split()\n",
        "    if len(tokens) <= 1 and len(comment.strip()) <= 4:\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# Example usage\n",
        "# -----------------------\n",
        "\n",
        "# load a dataset\n",
        "df2_labeled = pd.read_csv(\"/content/comments2.csv\")\n",
        "\n",
        "# apply heuristics\n",
        "df2_labeled[\"spam\"] = df2_labeled[\"textOriginal\"].apply(is_spam)\n",
        "\n",
        "# count spam vs not spam\n",
        "counts = df2_labeled[\"spam\"].value_counts()\n",
        "print(\"Spam:\", counts.get(True, 0))\n",
        "print(\"Not spam:\", counts.get(False, 0))\n",
        "\n",
        "# save labeled dataset\n",
        "df2_labeled.to_csv(\"/content/comments2_labeled.csv\", index=False)"
      ],
      "id": "d4f25228",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spam: 139967\n",
            "Not spam: 860032\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b02b623"
      },
      "source": [
        "# Task\n",
        "Create a Python program that takes a file path as input, loads the data, cleans the text using the provided `clean_text` function, applies the provided `is_spam` function to filter spam, and returns the processed DataFrame. Include example usage of the program."
      ],
      "id": "6b02b623"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "373101da"
      },
      "source": [
        "## Define a function\n",
        "\n",
        "### Subtask:\n",
        "Create a Python function that accepts a file path as an argument.\n"
      ],
      "id": "373101da"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "604196cf"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the `process_comments` function that accepts a file path as an argument.\n",
        "\n"
      ],
      "id": "604196cf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1786a3c6"
      },
      "source": [
        "def process_comments(file_path):\n",
        "    \"\"\"\n",
        "    Processes a comments file by cleaning text and identifying spam.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the comments CSV file.\n",
        "\n",
        "    Returns:\n",
        "        polars.DataFrame: The processed DataFrame with cleaned comments and spam labels.\n",
        "    \"\"\"\n",
        "    pass"
      ],
      "id": "1786a3c6",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "526c9a8f"
      },
      "source": [
        "## Load the data\n",
        "\n",
        "### Subtask:\n",
        "Inside the `process_comments` function, load the data from the provided file path into a polars DataFrame.\n"
      ],
      "id": "526c9a8f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b251041b"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the data from the provided file path into a polars DataFrame inside the `process_comments` function.\n",
        "\n"
      ],
      "id": "b251041b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd83c794"
      },
      "source": [
        "def process_comments(file_path):\n",
        "    \"\"\"\n",
        "    Processes a comments file by cleaning text and identifying spam.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the comments CSV file.\n",
        "\n",
        "    Returns:\n",
        "        polars.DataFrame: The processed DataFrame with cleaned comments and spam labels.\n",
        "    \"\"\"\n",
        "    df = pl.read_csv(file_path)\n",
        "    return df"
      ],
      "id": "cd83c794",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2e0737a"
      },
      "source": [
        "## Apply cleaning\n",
        "\n",
        "### Subtask:\n",
        "Apply the `clean_text` function to the relevant text column in the DataFrame loaded within the `process_comments` function.\n"
      ],
      "id": "c2e0737a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b829e89"
      },
      "source": [
        "**Reasoning**:\n",
        "Apply the clean_text function to the 'textOriginal' column and create a new 'cleaned_comment' column.\n",
        "\n"
      ],
      "id": "0b829e89"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "147ca044"
      },
      "source": [
        "def process_comments(file_path):\n",
        "    \"\"\"\n",
        "    Processes a comments file by cleaning text and identifying spam.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the comments CSV file.\n",
        "\n",
        "    Returns:\n",
        "        polars.DataFrame: The processed DataFrame with cleaned comments and spam labels.\n",
        "    \"\"\"\n",
        "    df = pl.read_csv(file_path)\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"textOriginal\").map_elements(clean_text, return_dtype=pl.Utf8).alias(\"cleaned_comment\")\n",
        "    )\n",
        "    return df"
      ],
      "id": "147ca044",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d27bb0d5"
      },
      "source": [
        "## Apply spam detection\n",
        "\n",
        "### Subtask:\n",
        "Apply the `is_spam` function to the cleaned text column to identify spam.\n"
      ],
      "id": "d27bb0d5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bafacb08"
      },
      "source": [
        "**Reasoning**:\n",
        "Apply the `is_spam` function to the `cleaned_comment` column and store the result in a new boolean column named `spam`.\n",
        "\n"
      ],
      "id": "bafacb08"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1a35fc1"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def process_comments(file_path):\n",
        "    \"\"\"\n",
        "    Processes a comments file by cleaning text and identifying spam.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the comments CSV file.\n",
        "\n",
        "    Returns:\n",
        "        polars.DataFrame: The processed DataFrame with cleaned comments and spam labels.\n",
        "    \"\"\"\n",
        "    df = pl.read_csv(file_path)\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"textOriginal\").map_elements(clean_text, return_dtype=pl.Utf8).alias(\"cleaned_comment\")\n",
        "    )\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"cleaned_comment\").map_elements(is_spam, return_dtype=pl.Boolean).alias(\"spam\")\n",
        "    )\n",
        "    return df"
      ],
      "id": "b1a35fc1",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0c76bce"
      },
      "source": [
        "## Return the labeled dataframe\n",
        "\n",
        "### Subtask:\n",
        "The function should return the DataFrame with the added 'cleaned_comment' and 'spam' columns.\n"
      ],
      "id": "d0c76bce"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a23a5ff"
      },
      "source": [
        "**Reasoning**:\n",
        "Review the `process_comments` function to ensure it returns the DataFrame with the added 'cleaned_comment' and 'spam' columns.\n",
        "\n"
      ],
      "id": "4a23a5ff"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a998383"
      },
      "source": [
        "def process_comments(file_path):\n",
        "    \"\"\"\n",
        "    Processes a comments file by cleaning text and identifying spam.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the comments CSV file.\n",
        "\n",
        "    Returns:\n",
        "        polars.DataFrame: The processed DataFrame with cleaned comments and spam labels.\n",
        "    \"\"\"\n",
        "    df = pl.read_csv(file_path)\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"textOriginal\").map_elements(clean_text, return_dtype=pl.Utf8).alias(\"cleaned_comment\")\n",
        "    )\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"cleaned_comment\").map_elements(is_spam, return_dtype=pl.Boolean).alias(\"spam\")\n",
        "    )\n",
        "    return df"
      ],
      "id": "4a998383",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "995ce74a"
      },
      "source": [
        "## Add example usage\n",
        "\n",
        "### Subtask:\n",
        "Include a section outside the function to demonstrate how to use it with a sample file and display the results.\n"
      ],
      "id": "995ce74a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbd401e0"
      },
      "source": [
        "**Reasoning**:\n",
        "Demonstrate the usage of the `process_comments` function by calling it with a sample file and displaying the head of the resulting DataFrame.\n",
        "\n"
      ],
      "id": "cbd401e0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e783ff41",
        "outputId": "420e5c67-2a3b-4a39-f799-f679baca8ea3"
      },
      "source": [
        "# Example usage\n",
        "sample_file_path = '/content/comments2.csv'\n",
        "processed_df = process_comments(sample_file_path)\n",
        "\n",
        "# Calculate and display spam vs non-spam counts and percentages\n",
        "spam_counts = processed_df.group_by(\"spam\").len()\n",
        "total_comments = processed_df.shape[0]\n",
        "\n",
        "print(\"\\nSpam vs Non-Spam Summary:\")\n",
        "for row in spam_counts.iter_rows(named=True):\n",
        "    label = \"Spam\" if row['spam'] else \"Not Spam\"\n",
        "    count = row['len']\n",
        "    percentage = (count / total_comments) * 100 if total_comments > 0 else 0\n",
        "    print(f\"{label}: {count} ({percentage:.2f}%)\")"
      ],
      "id": "e783ff41",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Spam vs Non-Spam Summary:\n",
            "Spam: 127967 (12.80%)\n",
            "Not Spam: 871973 (87.20%)\n",
            "Not Spam: 59 (0.01%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c79c80a1",
        "outputId": "0fe6f9df-0a41-4bcf-a90e-7bce83f9aeac"
      },
      "source": [
        "import polars as pl\n",
        "import re\n",
        "import pandas as pd  # optional\n",
        "\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    if text is None:\n",
        "        return \"\"   # return empty string instead of None\n",
        "\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation, numbers, and symbols but KEEP emojis & non-English chars\n",
        "    text = re.sub(r\"[0-9!\\\"#$%&'()*+,\\-./:;<=>?@\\[\\]^_`{|}~]\", \" \", text)\n",
        "\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def is_spam(comment: str) -> bool:\n",
        "    \"\"\"Return True if the comment looks like spam based on simple rules.\"\"\"\n",
        "    if not isinstance(comment, str):\n",
        "        return False\n",
        "    if not comment.strip():  # empty after cleaning\n",
        "        return False\n",
        "\n",
        "    text = comment.lower()\n",
        "\n",
        "    # Rule 1: contains a link\n",
        "    if re.search(r\"(http|www\\.|\\.com|\\.net|\\.org|\\.io|\\.co)\", text):\n",
        "        return True\n",
        "\n",
        "    # Rule 2: spammy keywords\n",
        "    if re.search(r\"(subscribe|check my channel|giveaway|free|click here|follow me|dm me)\", text):\n",
        "        return True\n",
        "\n",
        "    # Rule 3: too many emojis/symbols\n",
        "    if len(re.findall(r\"[^\\w\\s]\", comment)) > 10:\n",
        "        return True\n",
        "\n",
        "    # Rule 4: ultra-short and meaningless\n",
        "    tokens = comment.split()\n",
        "    if len(tokens) <= 1 and len(comment.strip()) <= 4:\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "def process_comments(file_path):\n",
        "    df = pl.read_csv(file_path)\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"textOriginal\").map_elements(clean_text, return_dtype=pl.Utf8).alias(\"cleaned_comment\")\n",
        "    )\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"cleaned_comment\").map_elements(is_spam, return_dtype=pl.Boolean).alias(\"spam\")\n",
        "    )\n",
        "    return df\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Example usage\n",
        "# -----------------------------\n",
        "sample_file_path = '/content/comments2.csv'\n",
        "processed_df = process_comments(sample_file_path)\n",
        "\n",
        "# Spam vs Non-Spam summary\n",
        "spam_counts = processed_df.group_by(\"spam\").len()\n",
        "total_comments = processed_df.shape[0]\n",
        "\n",
        "print(\"\\nSpam vs Non-Spam Summary:\")\n",
        "for row in spam_counts.iter_rows(named=True):\n",
        "    label = \"Spam\" if row['spam'] else \"Not Spam\"\n",
        "    count = row['len']\n",
        "    percentage = (count / total_comments) * 100 if total_comments > 0 else 0\n",
        "    print(f\"{label}: {count} ({percentage:.2f}%)\")\n",
        "\n",
        "# Overall spam percentage\n",
        "spam_total = spam_counts.filter(pl.col(\"spam\") == True)[\"len\"].sum()\n",
        "spam_percentage = (spam_total / total_comments) * 100 if total_comments > 0 else 0\n",
        "print(f\"\\nOverall Spam Percentage: {spam_percentage:.2f}%\")\n"
      ],
      "id": "c79c80a1",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Spam vs Non-Spam Summary:\n",
            "Not Spam: 59 (0.01%)\n",
            "Not Spam: 877906 (87.79%)\n",
            "Spam: 122034 (12.20%)\n",
            "\n",
            "Overall Spam Percentage: 12.20%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 0) Config\n",
        "# =========================\n",
        "FILE_PATH = \"/content/comments1.csv\"      # <-- change if needed\n",
        "TEXT_COL  = \"textOriginal\"\n",
        "SAVE_LABELED_CSV = True\n",
        "OUT_CSV_PATH = \"/content/comments1_labeled.csv\"\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 1) Cleaning\n",
        "# =========================\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"Lowercase, trim, collapse whitespace. Keep emojis & non-ASCII letters.\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower().strip()\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 2) Spam Heuristics (Enhanced)\n",
        "# =========================\n",
        "GENERIC_PRAISE_PAT = re.compile(\n",
        "    r\"^(love( it)?|i love|like( it)?|i like|nice|nice video|great|great video|amazing|so nice|so good|good|good video)$\"\n",
        ")\n",
        "\n",
        "LINK_PAT      = re.compile(r\"(http|https|www\\.)|(\\.com|\\.net|\\.org|\\.io|\\.co|\\.site|\\.shop)\", re.IGNORECASE)\n",
        "KEYWORDS_PAT  = re.compile(r\"(subscribe|check my channel|giveaway|free|click here|follow me|dm me|promo|collab)\", re.IGNORECASE)\n",
        "SYMBOL_PAT    = re.compile(r\"[^\\w\\s]\", re.UNICODE)\n",
        "\n",
        "def is_spam(comment: str) -> bool:\n",
        "    \"\"\"Deterministic rules for obvious and subtle spam, including generic praise spam.\"\"\"\n",
        "    if not isinstance(comment, str):\n",
        "        return False\n",
        "    if not comment.strip():\n",
        "        return False\n",
        "\n",
        "    text = comment.lower().strip()\n",
        "    tokens = text.split()\n",
        "\n",
        "    # Rule A: Links or classic spammy promo language\n",
        "    if LINK_PAT.search(text):\n",
        "        return True\n",
        "    if KEYWORDS_PAT.search(text):\n",
        "        return True\n",
        "\n",
        "    # Rule B: Overuse of symbols/emojis\n",
        "    # (lower threshold if very short; higher if longer)\n",
        "    sym_count = len(SYMBOL_PAT.findall(comment))\n",
        "    if sym_count > 10:\n",
        "        return True\n",
        "    if len(tokens) < 4 and sym_count > 6:\n",
        "        return True\n",
        "\n",
        "    # Rule C: Ultra-short junk (single token / tiny)\n",
        "    if len(tokens) <= 1 and len(text) <= 4:\n",
        "        return True\n",
        "\n",
        "    # Rule D: Generic praise spam (very short, context-free)\n",
        "    # Exact short phrases like \"love\", \"i love\", \"like\", \"nice video\", \"good\", etc.\n",
        "    if GENERIC_PRAISE_PAT.fullmatch(text):\n",
        "        return True\n",
        "\n",
        "    # Rule E: Low-context positive with too few words (e.g., \"love this\", \"nice one\")\n",
        "    if any(w in text for w in (\"love\", \"like\", \"nice\", \"great\", \"amazing\", \"good\")) and len(tokens) <= 3:\n",
        "        return True\n",
        "\n",
        "    # Rule F: Repeated characters/emoji floods (e.g., \"ğŸ˜ğŸ˜ğŸ˜ğŸ˜\")\n",
        "    if re.search(r\"(.)\\1{6,}\", text):  # 7+ same char in a row\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 3) Sentiment (5 buckets)\n",
        "# =========================\n",
        "def classify_sentiment(comment: str) -> str:\n",
        "    \"\"\"Map to one of: love / like / hate / not like / okay.\"\"\"\n",
        "    if not isinstance(comment, str) or not comment.strip():\n",
        "        return \"okay\"\n",
        "    text = comment.lower()\n",
        "\n",
        "    # Love\n",
        "    if re.search(r\"\\blove\\b|\\badore\\b|\\bamazing\\b|\\bfantastic\\b|\\bperfect\\b|â¤ï¸|ğŸ’–|ğŸ˜\", text):\n",
        "        return \"love\"\n",
        "\n",
        "    # Like\n",
        "    if re.search(r\"\\blike\\b|\\bgood\\b|\\bnice\\b|\\bdecent\\b|\\bwell done\\b|ğŸ‘|ğŸ˜Š\", text):\n",
        "        return \"like\"\n",
        "\n",
        "    # Hate\n",
        "    if re.search(r\"\\bhate\\b|\\bdisgust\\b|\\bworst\\b|\\bawful\\b|\\btrash\\b|ğŸ¤®|ğŸ˜¡\", text):\n",
        "        return \"hate\"\n",
        "\n",
        "    # Not like\n",
        "    if re.search(r\"\\bnot like\\b|\\bbad\\b|\\bboring\\b|\\bdislike\\b|ğŸ‘\", text):\n",
        "        return \"not like\"\n",
        "\n",
        "    # Neutral / other\n",
        "    return \"okay\"\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 4) Load data\n",
        "# =========================\n",
        "df = pd.read_csv(FILE_PATH)\n",
        "\n",
        "if TEXT_COL not in df.columns:\n",
        "    raise ValueError(f\"Column '{TEXT_COL}' not found. Available: {list(df.columns)[:15]} ...\")\n",
        "\n",
        "# Clean text\n",
        "df[\"clean_text\"] = df[TEXT_COL].map(clean_text)\n",
        "\n",
        "# Optional: drop exact duplicate comments to reduce obvious bot duplication skew\n",
        "# (If you prefer to keep duplicates for raw counts, comment the next line.)\n",
        "# df = df.drop_duplicates(subset=[\"clean_text\"]).reset_index(drop=True)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 5) Spam first, then Sentiment on non-spam\n",
        "# =========================\n",
        "df[\"spam\"] = df[\"clean_text\"].map(is_spam)\n",
        "\n",
        "# Sentiment on ALL (for comparison)\n",
        "df[\"sentiment_raw\"] = df[\"clean_text\"].map(classify_sentiment)\n",
        "\n",
        "# Sentiment on NON-SPAM only\n",
        "df[\"sentiment\"] = df.apply(\n",
        "    lambda r: classify_sentiment(r[\"clean_text\"]) if not r[\"spam\"] else None, axis=1\n",
        ")\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 6) Summaries\n",
        "# =========================\n",
        "total = len(df)\n",
        "spam_ct = int(df[\"spam\"].sum())\n",
        "not_spam_ct = total - spam_ct\n",
        "\n",
        "def pct(x, d=2):\n",
        "    return round(100.0 * x / total, d) if total else 0.0\n",
        "\n",
        "print(\"\\n=== Spam vs Not Spam ===\")\n",
        "print({\n",
        "    \"spam\": spam_ct,\n",
        "    \"not_spam\": not_spam_ct,\n",
        "    \"spam_%\": pct(spam_ct),\n",
        "    \"not_spam_%\": pct(not_spam_ct),\n",
        "})\n",
        "\n",
        "print(\"\\n=== Sentiment Distribution (All Comments, Spam INCLUDED) ===\")\n",
        "sent_raw = (df[\"sentiment_raw\"].value_counts(normalize=True) * 100).round(2).to_dict()\n",
        "print(sent_raw)\n",
        "\n",
        "print(\"\\n=== Sentiment Distribution (NON-SPAM ONLY) ===\")\n",
        "sent_clean = (df.loc[~df[\"spam\"], \"sentiment\"].value_counts(normalize=True) * 100).round(2).to_dict()\n",
        "print(sent_clean)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 7) Example comments per bucket (Non-Spam Only)\n",
        "# =========================\n",
        "print(\"\\n=== Example Comments per Sentiment Bucket (Non-Spam) ===\")\n",
        "for bucket in [\"love\", \"like\", \"hate\", \"not like\", \"okay\"]:\n",
        "    ex = df.loc[(~df[\"spam\"]) & (df[\"sentiment\"] == bucket), TEXT_COL].head(5).tolist()\n",
        "    print(f\"\\n--- {bucket.upper()} ---\")\n",
        "    for s in ex:\n",
        "        print(f\"- {s}\")\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 8) (Optional) Save labeled output\n",
        "# =========================\n",
        "if SAVE_LABELED_CSV:\n",
        "    cols_to_save = [TEXT_COL, \"clean_text\", \"spam\", \"sentiment_raw\", \"sentiment\"]\n",
        "    df.to_csv(OUT_CSV_PATH, index=False, columns=[c for c in cols_to_save if c in df.columns])\n",
        "    print(f\"\\nSaved labeled file to: {OUT_CSV_PATH}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdZOuukJWo3F",
        "outputId": "2d3e939e-dc67-4a80-debc-0244128bfd25"
      },
      "id": "xdZOuukJWo3F",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Spam vs Not Spam ===\n",
            "{'spam': 189938, 'not_spam': 810062, 'spam_%': 18.99, 'not_spam_%': 81.01}\n",
            "\n",
            "=== Sentiment Distribution (All Comments, Spam INCLUDED) ===\n",
            "{'okay': 75.02, 'like': 12.24, 'love': 11.28, 'hate': 0.89, 'not like': 0.57}\n",
            "\n",
            "=== Sentiment Distribution (NON-SPAM ONLY) ===\n",
            "{'okay': 77.15, 'like': 11.48, 'love': 10.1, 'hate': 0.7, 'not like': 0.56}\n",
            "\n",
            "=== Example Comments per Sentiment Bucket (Non-Spam) ===\n",
            "\n",
            "--- LOVE ---\n",
            "- Love your videos. Thank you â¤â¤â¤\n",
            "- I love korean and japanese makeup!\n",
            "- \"men love pterodactyls\" *inserts fire alarm that jumpscared me outta my damn seat*\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "EDIT: Im a year late to this but who gives a shit anyways thanks for the 0 likes. Could care less if you liked or not, thats all your decision not mine. So have a great day!\n",
            "- Praise God for His beautiful creation. Letâ€™s all Love the Lord our creator with all our hearts, souls & minds. Matthew 22:37-40. God bless.\n",
            "- Oud Satin Mood ğŸ˜\n",
            "\n",
            "--- LIKE ---\n",
            "- you look like raven from phenomena raven no cap\n",
            "- What hair oil did u use because the front of my hair a really short like yours was and Iâ€™m trying to grow it out but no hair oils are working so do u know which one u used itâ€™s ok if donâ€™t.\n",
            "- Heyyy video is excellent but why she has to do that much effort and why this kind of makeup it may harm her skin ...and i think that not good\n",
            "- I like your all vedios and your makeup\n",
            "- Youâ€™re welcome ğŸ˜Š\n",
            "\n",
            "--- HATE ---\n",
            "- I hate your videos you always say it's about lollipopsğŸ˜‚\n",
            "- I KNOW I HATE IT\n",
            "- I have the worst RBF but I'm 50 years old.  It would take this makeup tip plus 2 little blackbirds hovering on either side of my face holding fishing line attached to my mouth with fish hooks to fix that sht\n",
            "- Why is her head always tilted? Not hate just observation lol\n",
            "- me compared to you...i look trash :(\n",
            "\n",
            "--- NOT LIKE ---\n",
            "- Bhen itna makeup khrb krne k badğŸ˜¢\n",
            "- you guys would anyone wanna watch my grwm's.... im loek inspired by leahhh! i posted one which was kinda bad but im going to record one this weekend and post if any of you girlies would be interested <3 i would really appreciate the support!\n",
            "- NÃ£o ğŸ‘ a minha vida\n",
            "- I feel bad for the woman with the black \n",
            "ContactsğŸ˜\n",
            "ğŸ‘‡ğŸ¿\n",
            "- Itâ€™s cool that the product are vegan but I except a guy I the vidÃ©o we are now in 2020 and manny guys use makeup NOW so  I still donâ€™t see guys in the videos itâ€™s pity ğŸ‘ğŸ¼\n",
            "\n",
            "--- OKAY ---\n",
            "- PLEASE LESBIAN FLAG I BEG YOU \n",
            "\n",
            "You would rock it\n",
            "- Apply mashed potato juice and mixed it with curd\n",
            "- 69 missed calls from marsğŸ‘½\n",
            "- American\n",
            "- Sahi disha me ja ja raha india ka Future..\n",
            "\n",
            "Saved labeled file to: /content/comments1_labeled.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# =========================\n",
        "# Config\n",
        "# =========================\n",
        "FILE_PATH = \"/content/comments1.csv\"   # <-- change if needed\n",
        "TEXT_COL  = \"textOriginal\"\n",
        "SAVE_LABELED_CSV = True\n",
        "OUT_CSV_PATH = \"/content/comments1_labeled.csv\"\n",
        "\n",
        "# Thresholds\n",
        "SHALLOW_MAX_WORDS = 3\n",
        "EMOJI_FLOOD_MIN   = 5\n",
        "SHALLOW_EMOJI_MIN = 3\n",
        "SYMBOLS_OVERALL   = 12\n",
        "\n",
        "POS_GENERIC = (\"love\", \"like\", \"nice\", \"good\", \"great\", \"amazing\", \"wow\", \"cool\")\n",
        "ABUSE_PAT = re.compile(r\"\\b(stupid|idiot|dumb|trash|kill yourself|kys|loser|disgusting|ugly)\\b\", re.IGNORECASE)\n",
        "\n",
        "# =========================\n",
        "# Cleaning\n",
        "# =========================\n",
        "def clean_text(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower().strip()\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text\n",
        "\n",
        "# =========================\n",
        "# Spam detection helpers\n",
        "# =========================\n",
        "LINK_PAT     = re.compile(r\"(http|https|www\\.)|(\\.com|\\.net|\\.org|\\.io|\\.co|\\.site|\\.shop)\", re.IGNORECASE)\n",
        "KEYWORDS_PAT = re.compile(r\"(subscribe|check my channel|giveaway|free|click here|follow me|dm me|promo|collab)\", re.IGNORECASE)\n",
        "SYMBOL_PAT   = re.compile(r\"[^\\w\\s]\", re.UNICODE)\n",
        "\n",
        "def token_count(text: str) -> int:\n",
        "    return len(text.split())\n",
        "\n",
        "def symbol_count(raw: str) -> int:\n",
        "    return len(SYMBOL_PAT.findall(raw))\n",
        "\n",
        "def is_shallow(text: str) -> bool:\n",
        "    return token_count(text) <= SHALLOW_MAX_WORDS\n",
        "\n",
        "def has_pos_generic(text: str) -> bool:\n",
        "    return any(w in text for w in POS_GENERIC)\n",
        "\n",
        "def emoji_flood(raw: str) -> bool:\n",
        "    return symbol_count(raw) >= EMOJI_FLOOD_MIN\n",
        "\n",
        "def spam_with_reason(comment: str):\n",
        "    reasons = set()\n",
        "    if not isinstance(comment, str) or not comment.strip():\n",
        "        return False, reasons\n",
        "\n",
        "    text = comment.lower().strip()\n",
        "    toks = token_count(text)\n",
        "    syms = symbol_count(comment)\n",
        "\n",
        "    if LINK_PAT.search(text): reasons.add(\"link\")\n",
        "    if KEYWORDS_PAT.search(text): reasons.add(\"promo\")\n",
        "    if syms >= SYMBOLS_OVERALL: reasons.add(\"symbols_overall\")\n",
        "    if is_shallow(text) and has_pos_generic(text): reasons.add(\"shallow_generic\")\n",
        "    if emoji_flood(comment): reasons.add(\"emoji_flood\")\n",
        "    if is_shallow(text) and syms >= SHALLOW_EMOJI_MIN: reasons.add(\"shallow_symbols\")\n",
        "    if toks <= 1 and len(text) <= 4: reasons.add(\"ultra_short\")\n",
        "\n",
        "    return (len(reasons) > 0), reasons\n",
        "\n",
        "# =========================\n",
        "# Sentiment (5 buckets)\n",
        "# =========================\n",
        "def classify_sentiment(comment: str) -> str:\n",
        "    if not isinstance(comment, str) or not comment.strip():\n",
        "        return \"okay\"\n",
        "    text = comment.lower()\n",
        "    if re.search(r\"\\blove\\b|\\badore\\b|\\bamazing\\b|\\bfantastic\\b|\\bperfect\\b|â¤ï¸|ğŸ’–|ğŸ˜\", text): return \"love\"\n",
        "    if re.search(r\"\\blike\\b|\\bgood\\b|\\bnice\\b|\\bdecent\\b|\\bwell done\\b|ğŸ‘|ğŸ˜Š\", text): return \"like\"\n",
        "    if re.search(r\"\\bhate\\b|\\bdisgust\\b|\\bworst\\b|\\bawful\\b|\\btrash\\b|ğŸ¤®|ğŸ˜¡\", text): return \"hate\"\n",
        "    if re.search(r\"\\bnot like\\b|\\bbad\\b|\\bboring\\b|\\bdislike\\b|ğŸ‘\", text): return \"not like\"\n",
        "    return \"okay\"\n",
        "\n",
        "NEGATIVE_BUCKETS = {\"hate\", \"not like\"}\n",
        "\n",
        "# =========================\n",
        "# Toxicity\n",
        "# =========================\n",
        "def is_toxic(comment: str, spam_reasons: set, sentiment_label: str) -> bool:\n",
        "    if not isinstance(comment, str): return False\n",
        "    negative = sentiment_label in NEGATIVE_BUCKETS\n",
        "    shallow_or_emoji_spam = any(r in spam_reasons for r in (\"shallow_generic\",\"emoji_flood\",\"shallow_symbols\"))\n",
        "    abusive = ABUSE_PAT.search(comment) is not None\n",
        "    return negative and (shallow_or_emoji_spam or abusive)\n",
        "\n",
        "# =========================\n",
        "# Load & process\n",
        "# =========================\n",
        "df = pd.read_csv(FILE_PATH)\n",
        "df[\"clean_text\"] = df[TEXT_COL].map(clean_text)\n",
        "\n",
        "spam_info = df[\"clean_text\"].apply(spam_with_reason)\n",
        "df[\"spam\"] = spam_info.map(lambda x: x[0])\n",
        "df[\"spam_reasons\"] = spam_info.map(lambda x: \";\".join(sorted(list(x[1]))))\n",
        "\n",
        "df[\"sentiment_raw\"] = df[\"clean_text\"].map(classify_sentiment)\n",
        "df[\"sentiment\"] = df.apply(lambda r: classify_sentiment(r[\"clean_text\"]) if not r[\"spam\"] else None, axis=1)\n",
        "\n",
        "df[\"toxic\"] = df.apply(\n",
        "    lambda r: is_toxic(r[\"clean_text\"], set(r[\"spam_reasons\"].split(\";\")) if r[\"spam_reasons\"] else set(), r[\"sentiment_raw\"]),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# Summaries\n",
        "# =========================\n",
        "total = len(df)\n",
        "spam_ct = int(df[\"spam\"].sum())\n",
        "not_spam_ct = total - spam_ct\n",
        "toxic_ct = int(df[\"toxic\"].sum())\n",
        "\n",
        "def pct(part, whole): return round(100.0 * part / max(whole,1), 2)\n",
        "\n",
        "print(\"\\n=== Overall Summary ===\")\n",
        "print({\n",
        "    \"total\": total,\n",
        "    \"spam\": spam_ct,\n",
        "    \"not_spam\": not_spam_ct,\n",
        "    \"toxic\": toxic_ct,\n",
        "    \"spam_%\": pct(spam_ct, total),\n",
        "    \"not_spam_%\": pct(not_spam_ct, total),\n",
        "    \"toxic_%\": pct(toxic_ct, total)\n",
        "})\n",
        "\n",
        "print(\"\\n=== Sentiment Distribution (NON-SPAM ONLY, %) ===\")\n",
        "sent_clean = (df.loc[~df[\"spam\"], \"sentiment\"].value_counts(normalize=True) * 100).round(2).to_dict()\n",
        "print(sent_clean)\n",
        "\n",
        "# =========================\n",
        "# Save labeled file (optional)\n",
        "# =========================\n",
        "if SAVE_LABELED_CSV:\n",
        "    cols_to_save = [TEXT_COL, \"clean_text\", \"spam\", \"spam_reasons\", \"sentiment_raw\", \"sentiment\", \"toxic\"]\n",
        "    df.to_csv(OUT_CSV_PATH, index=False, columns=[c for c in cols_to_save if c in df.columns])\n",
        "    print(f\"\\nSaved labeled file to: {OUT_CSV_PATH}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHMyPOPybN89",
        "outputId": "46adb2ef-c411-4047-86b8-cfefdeb3f544"
      },
      "id": "NHMyPOPybN89",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Overall Summary ===\n",
            "{'total': 1000000, 'spam': 330046, 'not_spam': 669954, 'toxic': 6621, 'spam_%': 33.0, 'not_spam_%': 67.0, 'toxic_%': 0.66}\n",
            "\n",
            "=== Sentiment Distribution (NON-SPAM ONLY, %) ===\n",
            "{'okay': 79.86, 'like': 10.8, 'love': 8.2, 'hate': 0.59, 'not like': 0.54}\n",
            "\n",
            "Saved labeled file to: /content/comments1_labeled.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "THE MAIN PIPELINE (PLS JUST RUN THIS)"
      ],
      "metadata": {
        "id": "JhAQv4zzmBs6"
      },
      "id": "JhAQv4zzmBs6"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# ============================================================\n",
        "# Config\n",
        "# ============================================================\n",
        "FILE_PATH  = \"/content/comments1.csv\"     # <-- change to your file\n",
        "TEXT_COL   = \"textOriginal\"               # comment text column\n",
        "DATE_COL   = \"publishedAt\"                # optional; set None if not available\n",
        "USER_COL   = \"authorDisplayName\"          # optional; set None if not available\n",
        "\n",
        "# Output options\n",
        "SAVE_LABELED_CSV   = True\n",
        "LABELED_OUT_PATH   = \"/content/comments1_labeled.csv\"\n",
        "SAVE_SUMMARY_JSON  = True\n",
        "SUMMARY_OUT_PATH   = \"/content/comments1_kpis.json\"\n",
        "SAVE_CATEGORY_CSV  = True\n",
        "CATEGORY_CSV_PATH  = \"/content/comments1_category_sentiment.csv\"\n",
        "SAVE_TRENDS_CSV    = True\n",
        "TRENDS_CSV_PATH    = \"/content/comments1_time_trends.csv\"\n",
        "\n",
        "# KPI thresholds\n",
        "SHALLOW_MAX_WORDS = 3\n",
        "EMOJI_FLOOD_MIN   = 5\n",
        "SHALLOW_EMOJI_MIN = 3\n",
        "SYMBOLS_OVERALL   = 12\n",
        "\n",
        "# Category keyword mapping (extend/tune freely)\n",
        "CATEGORY_RULES = {\n",
        "    \"Hair\":       [r\"\\bhair\\b\", r\"\\bshampoo\\b\", r\"\\bconditioner\\b\", r\"\\bhair(oil|mask|spray)\\b\"],\n",
        "    \"Makeup\":     [r\"\\bmake ?up\\b\", r\"\\bmascara\\b\", r\"\\blip(stick|gloss)\\b\", r\"\\beyeliner\\b\", r\"\\bfoundation\\b\", r\"\\bblush\\b\"],\n",
        "    \"Skincare\":   [r\"\\bskin ?care\\b\", r\"\\bserum\\b\", r\"\\bmoisturizer\\b\", r\"\\bretinol\\b\", r\"\\bspf\\b\", r\"\\bsunscreen\\b\", r\"\\bcleanser\\b\"],\n",
        "    \"Packaging\":  [r\"\\bpackage|packaging|box|bottle|cap|pump\\b\"],\n",
        "    \"Fragrance\":  [r\"\\bperfume\\b\", r\"\\bfragrance\\b\", r\"\\beau de\\b\", r\"\\bparfum\\b\", r\"\\bcologne\\b\"],\n",
        "}\n",
        "# Everything else goes to \"Other\"\n",
        "\n",
        "# ============================================================\n",
        "# Utilities & Cleaning\n",
        "# ============================================================\n",
        "def pct(part, whole, d=2):\n",
        "    return round(100.0 * part / max(whole, 1), d)\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower().strip()\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text\n",
        "\n",
        "# ============================================================\n",
        "# Spam Detection (classic + shallow + emoji)\n",
        "# ============================================================\n",
        "LINK_PAT     = re.compile(r\"(http|https|www\\.)|(\\.com|\\.net|\\.org|\\.io|\\.co|\\.site|\\.shop)\", re.IGNORECASE)\n",
        "KEYWORDS_PAT = re.compile(r\"(subscribe|check my channel|giveaway|free|click here|follow me|dm me|promo|collab)\", re.IGNORECASE)\n",
        "SYMBOL_PAT   = re.compile(r\"[^\\w\\s]\", re.UNICODE)\n",
        "ABUSE_PAT    = re.compile(r\"\\b(stupid|idiot|dumb|trash|kill yourself|kys|loser|disgusting|ugly)\\b\", re.IGNORECASE)\n",
        "\n",
        "POS_GENERIC = (\"love\", \"like\", \"nice\", \"good\", \"great\", \"amazing\", \"wow\", \"cool\")\n",
        "NEGATIVE_BUCKETS = {\"Hated it\", \"Disliked it\"}\n",
        "\n",
        "def token_count(text: str) -> int:\n",
        "    return len(text.split())\n",
        "\n",
        "def symbol_count(raw: str) -> int:\n",
        "    return len(SYMBOL_PAT.findall(raw))\n",
        "\n",
        "def is_shallow(text: str) -> bool:\n",
        "    return token_count(text) <= SHALLOW_MAX_WORDS\n",
        "\n",
        "def has_pos_generic(text: str) -> bool:\n",
        "    return any(w in text for w in POS_GENERIC)\n",
        "\n",
        "def emoji_flood(raw: str) -> bool:\n",
        "    return symbol_count(raw) >= EMOJI_FLOOD_MIN\n",
        "\n",
        "def spam_with_reason(comment: str):\n",
        "    \"\"\"\n",
        "    Returns (is_spam: bool, reasons: set[str])\n",
        "    reasons âŠ† {link, promo, symbols_overall, shallow_generic, emoji_flood, shallow_symbols, ultra_short}\n",
        "    \"\"\"\n",
        "    reasons = set()\n",
        "    if not isinstance(comment, str) or not comment.strip():\n",
        "        return False, reasons\n",
        "\n",
        "    text = comment.lower().strip()\n",
        "    toks = token_count(text)\n",
        "    syms = symbol_count(comment)\n",
        "\n",
        "    if LINK_PAT.search(text): reasons.add(\"link\")\n",
        "    if KEYWORDS_PAT.search(text): reasons.add(\"promo\")\n",
        "    if syms >= SYMBOLS_OVERALL: reasons.add(\"symbols_overall\")\n",
        "\n",
        "    # Shallow generic praise\n",
        "    if is_shallow(text) and has_pos_generic(text):\n",
        "        reasons.add(\"shallow_generic\")\n",
        "\n",
        "    # Emoji flood + shallow+symbols combo\n",
        "    if emoji_flood(comment): reasons.add(\"emoji_flood\")\n",
        "    if is_shallow(text) and syms >= SHALLOW_EMOJI_MIN: reasons.add(\"shallow_symbols\")\n",
        "\n",
        "    # Ultra-short junk (e.g., \"ok\", \"yo\", \"ğŸ‘\")\n",
        "    if toks <= 1 and len(text) <= 4:\n",
        "        reasons.add(\"ultra_short\")\n",
        "\n",
        "    return (len(reasons) > 0), reasons\n",
        "\n",
        "# ============================================================\n",
        "# Sentiment (5 buckets)\n",
        "# ============================================================\n",
        "def classify_sentiment(comment: str) -> str:\n",
        "    \"\"\"\n",
        "    5 buckets for reporting:\n",
        "      Loved it / Liked it / Okay / Disliked it / Hated it\n",
        "    \"\"\"\n",
        "    if not isinstance(comment, str) or not comment.strip():\n",
        "        return \"Okay\"\n",
        "    text = comment.lower()\n",
        "    if re.search(r\"\\blove\\b|\\badore\\b|\\bamazing\\b|\\bfantastic\\b|\\bperfect\\b|â¤ï¸|ğŸ’–|ğŸ˜\", text): return \"Loved it\"\n",
        "    if re.search(r\"\\blike\\b|\\bgood\\b|\\bnice\\b|\\bdecent\\b|\\bwell done\\b|ğŸ‘|ğŸ˜Š\", text): return \"Liked it\"\n",
        "    if re.search(r\"\\bhate\\b|\\bdisgust\\b|\\bworst\\b|\\bawful\\b|\\btrash\\b|ğŸ¤®|ğŸ˜¡\", text): return \"Hated it\"\n",
        "    if re.search(r\"\\bnot like\\b|\\bbad\\b|\\bboring\\b|\\bdislike\\b|ğŸ‘\", text): return \"Disliked it\"\n",
        "    return \"Okay\"\n",
        "\n",
        "# ============================================================\n",
        "# Toxicity\n",
        "# ============================================================\n",
        "def is_toxic(comment: str, spam_reasons: set, sentiment_label: str) -> bool:\n",
        "    \"\"\"\n",
        "    Toxic if:\n",
        "      - sentiment is negative (Hated it / Disliked it), AND\n",
        "      - (spam due to shallow/emoji) OR contains obvious abusive lexicon.\n",
        "    \"\"\"\n",
        "    if not isinstance(comment, str): return False\n",
        "    negative = sentiment_label in NEGATIVE_BUCKETS\n",
        "    shallow_or_emoji_spam = any(r in spam_reasons for r in (\"shallow_generic\",\"emoji_flood\",\"shallow_symbols\"))\n",
        "    abusive = ABUSE_PAT.search(comment) is not None\n",
        "    return negative and (shallow_or_emoji_spam or abusive)\n",
        "\n",
        "# ============================================================\n",
        "# Category tagging\n",
        "# ============================================================\n",
        "def assign_category(text: str) -> str:\n",
        "    for cat, patterns in CATEGORY_RULES.items():\n",
        "        for pat in patterns:\n",
        "            if re.search(pat, text):\n",
        "                return cat\n",
        "    return \"Other\"\n",
        "\n",
        "# ============================================================\n",
        "# KPI Computation (collision-safe)\n",
        "# ============================================================\n",
        "def compute_kpis(df: pd.DataFrame):\n",
        "    total = len(df)\n",
        "    spam_ct = int(df[\"spam\"].sum())\n",
        "    non_spam_ct = total - spam_ct\n",
        "    toxic_ct = int(df[\"toxic\"].sum())\n",
        "\n",
        "    # Engagement metrics\n",
        "    avg_len = float(df[\"clean_text\"].str.len().mean() or 0)\n",
        "    unique_users = int(df[USER_COL].nunique()) if USER_COL and USER_COL in df.columns else None\n",
        "    dedup_count = int(df[\"clean_text\"].nunique())\n",
        "\n",
        "    # Comment Quality Index (meaningful / shallow+emoji)\n",
        "    shallow_or_emoji_flags = df[\"spam_reasons\"].str.contains(\n",
        "        \"shallow_generic|emoji_flood|shallow_symbols\", regex=True, na=False\n",
        "    )\n",
        "    shallow_emoji_ct = int(shallow_or_emoji_flags.sum())\n",
        "    cqi_ratio = round((non_spam_ct / max(shallow_emoji_ct, 1)), 3)\n",
        "\n",
        "    # Sentiment on non-spam\n",
        "    non_spam = df.loc[~df[\"spam\"]].copy()\n",
        "    sent_dist_pct = (non_spam[\"sentiment\"].value_counts(normalize=True) * 100).round(2).to_dict()\n",
        "\n",
        "    # Category-wise sentiment (non-spam)\n",
        "    non_spam_cat = non_spam.copy()\n",
        "    if \"category\" not in non_spam_cat.columns:\n",
        "        non_spam_cat[\"category\"] = \"Other\"\n",
        "    cat_sent = non_spam_cat.groupby([\"category\", \"sentiment\"]).size().reset_index(name=\"count\")\n",
        "    if len(cat_sent):\n",
        "        cat_sent_pivot = cat_sent.pivot_table(\n",
        "            index=\"category\", columns=\"sentiment\", values=\"count\", aggfunc=\"sum\", fill_value=0\n",
        "        )\n",
        "        cat_sent_pivot[\"total\"] = cat_sent_pivot.sum(axis=1)\n",
        "        for col in cat_sent_pivot.columns:\n",
        "            if col != \"total\":\n",
        "                cat_sent_pivot[col] = (cat_sent_pivot[col] / cat_sent_pivot[\"total\"] * 100).round(2)\n",
        "    else:\n",
        "        cat_sent_pivot = pd.DataFrame(columns=[\"category\", \"total\"]).set_index(\"category\")\n",
        "\n",
        "    # Time trends (daily sentiment %, weekly toxicity %) if DATE_COL exists\n",
        "    trends_daily = None\n",
        "    trends_weekly = None\n",
        "    if DATE_COL and DATE_COL in df.columns:\n",
        "        dft = df.copy()\n",
        "        dft[\"date\"] = pd.to_datetime(dft[DATE_COL], errors=\"coerce\")\n",
        "        dft = dft.dropna(subset=[\"date\"])\n",
        "\n",
        "        # Daily sentiment % (non-spam) â€” use temp column to avoid collisions\n",
        "        daily = dft.loc[~dft[\"spam\"]].copy()\n",
        "        DAY_COL = \"_day_tmp\"\n",
        "        if DAY_COL in daily.columns:\n",
        "            daily.drop(columns=[DAY_COL], inplace=True, errors=\"ignore\")\n",
        "        daily.loc[:, DAY_COL] = daily[\"date\"].dt.date\n",
        "\n",
        "        if len(daily):\n",
        "            trends_daily = (\n",
        "                daily.groupby([DAY_COL, \"sentiment\"])\n",
        "                .size()\n",
        "                .groupby(level=0)\n",
        "                .apply(lambda s: (s / s.sum() * 100).round(2))\n",
        "                .unstack(fill_value=0)\n",
        "                .reset_index()\n",
        "                .rename(columns={DAY_COL: \"day\"})\n",
        "            )\n",
        "\n",
        "        # Weekly toxicity % â€” use temp column to avoid collisions\n",
        "        dft_local = dft.copy()\n",
        "        WEEK_COL = \"_week_tmp\"\n",
        "        if WEEK_COL in dft_local.columns:\n",
        "            dft_local.drop(columns=[WEEK_COL], inplace=True, errors=\"ignore\")\n",
        "        dft_local.loc[:, WEEK_COL] = dft_local[\"date\"].dt.to_period(\"W-SUN\").apply(lambda p: p.start_time.date())\n",
        "\n",
        "        if len(dft_local):\n",
        "            trends_weekly = (\n",
        "                dft_local.groupby(WEEK_COL)[\"toxic\"]\n",
        "                .mean()\n",
        "                .mul(100)\n",
        "                .round(2)\n",
        "                .reset_index()\n",
        "                .rename(columns={WEEK_COL: \"week\", \"toxic\": \"toxic_%\"})\n",
        "            )\n",
        "\n",
        "    summary = {\n",
        "        \"kpis\": {\n",
        "            \"percent_spam\": pct(spam_ct, total),\n",
        "            \"percent_non_spam\": pct(non_spam_ct, total),\n",
        "            \"percent_toxic\": pct(toxic_ct, total),\n",
        "        },\n",
        "        \"sentiment_distribution_non_spam_%\": sent_dist_pct,\n",
        "        \"comment_quality_index_ratio_meaningful_over_shallow_emoji\": cqi_ratio,\n",
        "        \"engagement\": {\n",
        "            \"avg_comment_length_chars\": round(avg_len, 2),\n",
        "            \"unique_users\": unique_users,\n",
        "            \"unique_texts_after_dedup\": dedup_count,\n",
        "            \"duplicates_removed\": total - dedup_count,\n",
        "        },\n",
        "    }\n",
        "    return summary, cat_sent_pivot, trends_daily, trends_weekly\n",
        "\n",
        "# ============================================================\n",
        "# Main\n",
        "# ============================================================\n",
        "df = pd.read_csv(FILE_PATH)\n",
        "if TEXT_COL not in df.columns:\n",
        "    raise ValueError(f\"Column '{TEXT_COL}' not found. Available: {list(df.columns)[:15]} ...\")\n",
        "\n",
        "# Clean\n",
        "df[\"clean_text\"] = df[TEXT_COL].map(clean_text)\n",
        "\n",
        "# Spam + reasons\n",
        "spam_info = df[\"clean_text\"].apply(spam_with_reason)\n",
        "df[\"spam\"] = spam_info.map(lambda x: x[0])\n",
        "df[\"spam_reasons\"] = spam_info.map(lambda x: \";\".join(sorted(list(x[1]))))\n",
        "\n",
        "# Sentiment\n",
        "df[\"sentiment_raw\"] = df[\"clean_text\"].map(classify_sentiment)\n",
        "df[\"sentiment\"] = df.apply(lambda r: classify_sentiment(r[\"clean_text\"]) if not r[\"spam\"] else None, axis=1)\n",
        "\n",
        "# Toxic\n",
        "df[\"toxic\"] = df.apply(\n",
        "    lambda r: is_toxic(\n",
        "        r[\"clean_text\"],\n",
        "        set(r[\"spam_reasons\"].split(\";\")) if r[\"spam_reasons\"] else set(),\n",
        "        r[\"sentiment_raw\"],\n",
        "    ),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Category tagging\n",
        "df[\"category\"] = df[\"clean_text\"].map(assign_category)\n",
        "\n",
        "# KPIs\n",
        "summary, cat_sent_pivot, trends_daily, trends_weekly = compute_kpis(df)\n",
        "\n",
        "# Print KPI summary (numbers only)\n",
        "print(\"\\n=== KPI Summary ===\")\n",
        "print(json.dumps(summary, ensure_ascii=False, indent=2))\n",
        "\n",
        "# Save labeled file\n",
        "if SAVE_LABELED_CSV:\n",
        "    cols_to_save = [TEXT_COL,\"clean_text\",\"spam\",\"spam_reasons\",\"sentiment\",\"sentiment_raw\",\"toxic\",\"category\"]\n",
        "    df.to_csv(LABELED_OUT_PATH, index=False, columns=[c for c in cols_to_save if c in df.columns])\n",
        "    print(f\"Saved labeled file to: {LABELED_OUT_PATH}\")\n",
        "\n",
        "# Save KPI summary JSON\n",
        "if SAVE_SUMMARY_JSON:\n",
        "    with open(SUMMARY_OUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(summary, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"Saved KPI summary JSON to: {SUMMARY_OUT_PATH}\")\n",
        "\n",
        "# Save category-wise sentiment CSV\n",
        "if SAVE_CATEGORY_CSV:\n",
        "    cat_sent_pivot.to_csv(CATEGORY_CSV_PATH)\n",
        "    print(f\"Saved category-wise sentiment CSV to: {CATEGORY_CSV_PATH}\")\n",
        "\n",
        "# Save time trends CSV (if date is available)\n",
        "if SAVE_TRENDS_CSV and trends_daily is not None:\n",
        "    trends_daily.to_csv(TRENDS_CSV_PATH, index=False)\n",
        "    print(f\"Saved time trends (daily sentiment %) CSV to: {TRENDS_CSV_PATH}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "rm5zC0PRlsVr",
        "outputId": "5e8aa0ba-c2d0-40bb-9d0b-cee6f2744ee4"
      },
      "id": "rm5zC0PRlsVr",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "cannot insert _day_tmp, already exists",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3200182969.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;31m# KPIs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_sent_pivot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrends_daily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrends_weekly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_kpis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;31m# Print KPI summary (numbers only)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3200182969.py\u001b[0m in \u001b[0;36mcompute_kpis\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    212\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0munstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m                 \u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mDAY_COL\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"day\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mreset_index\u001b[0;34m(self, level, drop, inplace, col_level, col_fill, allow_duplicates, names)\u001b[0m\n\u001b[1;32m   6470\u001b[0m                     )\n\u001b[1;32m   6471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6472\u001b[0;31m                 new_obj.insert(\n\u001b[0m\u001b[1;32m   6473\u001b[0m                     \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6474\u001b[0m                     \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(self, loc, column, value, allow_duplicates)\u001b[0m\n\u001b[1;32m   5156\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_duplicates\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5157\u001b[0m             \u001b[0;31m# Should this be a different kind of error??\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5158\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"cannot insert {column}, already exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5159\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5160\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loc must be int\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot insert _day_tmp, already exists"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}