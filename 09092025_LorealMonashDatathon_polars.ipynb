{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "491e8112-55df-484e-925d-407f77181709",
      "metadata": {
        "id": "491e8112-55df-484e-925d-407f77181709"
      },
      "source": [
        "## COMBINING DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "56e9cc1d-4014-4e40-ad77-8b7a8080142a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56e9cc1d-4014-4e40-ad77-8b7a8080142a",
        "outputId": "abf241c2-bb81-4fa0-9283-e5718819b1c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: polars in /usr/local/lib/python3.12/dist-packages (1.25.2)\n"
          ]
        }
      ],
      "source": [
        "! pip install polars\n",
        "import polars as pl"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OoG-twd-PbKx"
      },
      "id": "OoG-twd-PbKx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "swLQurqyP77q"
      },
      "id": "swLQurqyP77q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "id": "a53d16fd",
        "outputId": "73122ee8-52d6-4c1f-d8c0-3ed485fe131e"
      },
      "source": [
        "df2 = pl.read_csv('/content/comments2.csv')\n",
        "display(df2.head())"
      ],
      "id": "a53d16fd",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "shape: (5, 10)\n",
              "┌────────────┬───────────┬───────────┬─────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
              "│ kind       ┆ commentId ┆ channelId ┆ videoId ┆ … ┆ parentCom ┆ likeCount ┆ published ┆ updatedAt │\n",
              "│ ---        ┆ ---       ┆ ---       ┆ ---     ┆   ┆ mentId    ┆ ---       ┆ At        ┆ ---       │\n",
              "│ str        ┆ i64       ┆ i64       ┆ i64     ┆   ┆ ---       ┆ i64       ┆ ---       ┆ str       │\n",
              "│            ┆           ┆           ┆         ┆   ┆ i64       ┆           ┆ str       ┆           │\n",
              "╞════════════╪═══════════╪═══════════╪═════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
              "│ youtube#co ┆ 3403605   ┆ 23845     ┆ 31993   ┆ … ┆ null      ┆ 0         ┆ 2025-04-0 ┆ 2025-04-0 │\n",
              "│ mment      ┆           ┆           ┆         ┆   ┆           ┆           ┆ 3 13:49:2 ┆ 3 13:49:2 │\n",
              "│            ┆           ┆           ┆         ┆   ┆           ┆           ┆ 0+00:00   ┆ 0+00:00   │\n",
              "│ youtube#co ┆ 3957372   ┆ 32163     ┆ 88966   ┆ … ┆ null      ┆ 0         ┆ 2024-07-1 ┆ 2024-07-1 │\n",
              "│ mment      ┆           ┆           ┆         ┆   ┆           ┆           ┆ 7 08:51:1 ┆ 7 08:51:1 │\n",
              "│            ┆           ┆           ┆         ┆   ┆           ┆           ┆ 9+00:00   ┆ 9+00:00   │\n",
              "│ youtube#co ┆ 1272592   ┆ 18073     ┆ 69091   ┆ … ┆ null      ┆ 0         ┆ 2023-01-2 ┆ 2023-01-2 │\n",
              "│ mment      ┆           ┆           ┆         ┆   ┆           ┆           ┆ 9 05:01:0 ┆ 9 05:01:0 │\n",
              "│            ┆           ┆           ┆         ┆   ┆           ┆           ┆ 6+00:00   ┆ 6+00:00   │\n",
              "│ youtube#co ┆ 4006296   ┆ 47781     ┆ 38945   ┆ … ┆ null      ┆ 0         ┆ 2022-07-2 ┆ 2022-07-2 │\n",
              "│ mment      ┆           ┆           ┆         ┆   ┆           ┆           ┆ 7 22:38:4 ┆ 7 22:38:4 │\n",
              "│            ┆           ┆           ┆         ┆   ┆           ┆           ┆ 1+00:00   ┆ 1+00:00   │\n",
              "│ youtube#co ┆ 521568    ┆ 13736     ┆ 92721   ┆ … ┆ null      ┆ 0         ┆ 2025-05-0 ┆ 2025-05-0 │\n",
              "│ mment      ┆           ┆           ┆         ┆   ┆           ┆           ┆ 4 20:28:4 ┆ 4 20:28:4 │\n",
              "│            ┆           ┆           ┆         ┆   ┆           ┆           ┆ 8+00:00   ┆ 8+00:00   │\n",
              "└────────────┴───────────┴───────────┴─────────┴───┴───────────┴───────────┴───────────┴───────────┘"
            ],
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (5, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>kind</th><th>commentId</th><th>channelId</th><th>videoId</th><th>authorId</th><th>textOriginal</th><th>parentCommentId</th><th>likeCount</th><th>publishedAt</th><th>updatedAt</th></tr><tr><td>str</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>str</td><td>i64</td><td>i64</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;youtube#comment&quot;</td><td>3403605</td><td>23845</td><td>31993</td><td>2874725</td><td>&quot;my 5 year old brother said dol…</td><td>null</td><td>0</td><td>&quot;2025-04-03 13:49:20+00:00&quot;</td><td>&quot;2025-04-03 13:49:20+00:00&quot;</td></tr><tr><td>&quot;youtube#comment&quot;</td><td>3957372</td><td>32163</td><td>88966</td><td>757766</td><td>&quot;You look very beautiful and cu…</td><td>null</td><td>0</td><td>&quot;2024-07-17 08:51:19+00:00&quot;</td><td>&quot;2024-07-17 08:51:19+00:00&quot;</td></tr><tr><td>&quot;youtube#comment&quot;</td><td>1272592</td><td>18073</td><td>69091</td><td>858228</td><td>&quot;American 🇺🇲&quot;</td><td>null</td><td>0</td><td>&quot;2023-01-29 05:01:06+00:00&quot;</td><td>&quot;2023-01-29 05:01:06+00:00&quot;</td></tr><tr><td>&quot;youtube#comment&quot;</td><td>4006296</td><td>47781</td><td>38945</td><td>2179100</td><td>&quot;MASHA&#x27;ALLAH♥♥♥&quot;</td><td>null</td><td>0</td><td>&quot;2022-07-27 22:38:41+00:00&quot;</td><td>&quot;2022-07-27 22:38:41+00:00&quot;</td></tr><tr><td>&quot;youtube#comment&quot;</td><td>521568</td><td>13736</td><td>92721</td><td>364652</td><td>&quot;As a scene kid, YOU KILLED IT😻…</td><td>null</td><td>0</td><td>&quot;2025-05-04 20:28:48+00:00&quot;</td><td>&quot;2025-05-04 20:28:48+00:00&quot;</td></tr></tbody></table></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "b2d7692b",
        "outputId": "c2da9ee8-ad87-4230-f394-f507e6aa2079"
      },
      "source": [
        "def clean_text(text: str) -> str:\n",
        "    if text is None:\n",
        "        return None\n",
        "\n",
        "    # ✅ Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # ✅ Remove punctuation, numbers, and symbols but KEEP emojis & non-English chars\n",
        "    # \\p{L} = letters, \\p{M} = diacritics, \\p{Zs} = spaces, \\p{Emoji} not directly supported in regex\n",
        "    # So instead: only remove ASCII punctuation/numbers\n",
        "    text = re.sub(r\"[0-9!\\\"#$%&'()*+,\\-./:;<=>?@\\[\\]^_`{|}~]\", \" \", text)\n",
        "\n",
        "    # ✅ Remove extra spaces\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply cleaning with Polars\n",
        "df2 = df2.with_columns(\n",
        "    pl.col(\"textOriginal\").map_elements(clean_text, return_dtype=pl.Utf8).alias(\"cleaned_comment\")\n",
        ")\n",
        "\n",
        "df2.select([\"textOriginal\", \"cleaned_comment\"]).head(10)"
      ],
      "id": "b2d7692b",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "shape: (10, 2)\n",
              "┌─────────────────────────────────┬─────────────────────────────────┐\n",
              "│ textOriginal                    ┆ cleaned_comment                 │\n",
              "│ ---                             ┆ ---                             │\n",
              "│ str                             ┆ str                             │\n",
              "╞═════════════════════════════════╪═════════════════════════════════╡\n",
              "│ my 5 year old brother said dol… ┆ my year old brother said dolph… │\n",
              "│ You look very beautiful and cu… ┆ you look very beautiful and cu… │\n",
              "│ American 🇺🇲                     ┆ american 🇺🇲                     │\n",
              "│ MASHA'ALLAH♥♥♥                  ┆ masha allah♥♥♥                  │\n",
              "│ As a scene kid, YOU KILLED      ┆ as a scene kid you killed       │\n",
              "│ IT😻…                           ┆ it😻‼…                          │\n",
              "│ They all are cute but that bla… ┆ they all are cute but that bla… │\n",
              "│ Scrubbing is a big NO.          ┆ scrubbing is a big no           │\n",
              "│ Ooooo bhai ,,,maro mujhe maro   ┆ ooooo bhai maro mujhe maro      │\n",
              "│ Nice as Heaven                  ┆ nice as heaven                  │\n",
              "│ It looks cute, but why does th… ┆ it looks cute but why does the… │\n",
              "└─────────────────────────────────┴─────────────────────────────────┘"
            ],
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (10, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>textOriginal</th><th>cleaned_comment</th></tr><tr><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;my 5 year old brother said dol…</td><td>&quot;my year old brother said dolph…</td></tr><tr><td>&quot;You look very beautiful and cu…</td><td>&quot;you look very beautiful and cu…</td></tr><tr><td>&quot;American 🇺🇲&quot;</td><td>&quot;american 🇺🇲&quot;</td></tr><tr><td>&quot;MASHA&#x27;ALLAH♥♥♥&quot;</td><td>&quot;masha allah♥♥♥&quot;</td></tr><tr><td>&quot;As a scene kid, YOU KILLED IT😻…</td><td>&quot;as a scene kid you killed it😻‼…</td></tr><tr><td>&quot;They all are cute but that bla…</td><td>&quot;they all are cute but that bla…</td></tr><tr><td>&quot;Scrubbing is a big NO.&quot;</td><td>&quot;scrubbing is a big no&quot;</td></tr><tr><td>&quot;Ooooo bhai ,,,maro mujhe maro&quot;</td><td>&quot;ooooo bhai maro mujhe maro&quot;</td></tr><tr><td>&quot;Nice as Heaven&quot;</td><td>&quot;nice as heaven&quot;</td></tr><tr><td>&quot;It looks cute, but why does th…</td><td>&quot;it looks cute but why does the…</td></tr></tbody></table></div>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4f25228",
        "outputId": "601f59c1-082e-46c9-f1a1-a81a9e1f420e"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# -----------------------\n",
        "# Simple Heuristic Spam Detector\n",
        "# -----------------------\n",
        "\n",
        "def is_spam(comment: str) -> bool:\n",
        "    \"\"\"Return True if the comment looks like spam based on simple rules.\"\"\"\n",
        "    if not isinstance(comment, str):\n",
        "        return False\n",
        "\n",
        "    text = comment.lower()\n",
        "\n",
        "    # Rule 1: contains a link\n",
        "    if re.search(r\"(http|www\\.|\\.com|\\.net|\\.org|\\.io|\\.co)\", text):\n",
        "        return True\n",
        "\n",
        "    # Rule 2: spammy keywords\n",
        "    if re.search(r\"(subscribe|check my channel|giveaway|free|click here|follow me|dm me)\", text):\n",
        "        return True\n",
        "\n",
        "    # Rule 3: too many emojis/symbols\n",
        "    if len(re.findall(r\"[^\\w\\s]\", comment)) > 10:\n",
        "        return True\n",
        "\n",
        "    # Rule 4: ultra-short and meaningless\n",
        "    tokens = comment.split()\n",
        "    if len(tokens) <= 1 and len(comment.strip()) <= 4:\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# Example usage\n",
        "# -----------------------\n",
        "\n",
        "# load a dataset\n",
        "df2_labeled = pd.read_csv(\"/content/comments2.csv\")\n",
        "\n",
        "# apply heuristics\n",
        "df2_labeled[\"spam\"] = df2_labeled[\"textOriginal\"].apply(is_spam)\n",
        "\n",
        "# count spam vs not spam\n",
        "counts = df2_labeled[\"spam\"].value_counts()\n",
        "print(\"Spam:\", counts.get(True, 0))\n",
        "print(\"Not spam:\", counts.get(False, 0))\n",
        "\n",
        "# save labeled dataset\n",
        "df2_labeled.to_csv(\"/content/comments2_labeled.csv\", index=False)"
      ],
      "id": "d4f25228",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spam: 139967\n",
            "Not spam: 860032\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b02b623"
      },
      "source": [
        "# Task\n",
        "Create a Python program that takes a file path as input, loads the data, cleans the text using the provided `clean_text` function, applies the provided `is_spam` function to filter spam, and returns the processed DataFrame. Include example usage of the program."
      ],
      "id": "6b02b623"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "373101da"
      },
      "source": [
        "## Define a function\n",
        "\n",
        "### Subtask:\n",
        "Create a Python function that accepts a file path as an argument.\n"
      ],
      "id": "373101da"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "604196cf"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the `process_comments` function that accepts a file path as an argument.\n",
        "\n"
      ],
      "id": "604196cf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1786a3c6"
      },
      "source": [
        "def process_comments(file_path):\n",
        "    \"\"\"\n",
        "    Processes a comments file by cleaning text and identifying spam.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the comments CSV file.\n",
        "\n",
        "    Returns:\n",
        "        polars.DataFrame: The processed DataFrame with cleaned comments and spam labels.\n",
        "    \"\"\"\n",
        "    pass"
      ],
      "id": "1786a3c6",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "526c9a8f"
      },
      "source": [
        "## Load the data\n",
        "\n",
        "### Subtask:\n",
        "Inside the `process_comments` function, load the data from the provided file path into a polars DataFrame.\n"
      ],
      "id": "526c9a8f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b251041b"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the data from the provided file path into a polars DataFrame inside the `process_comments` function.\n",
        "\n"
      ],
      "id": "b251041b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd83c794"
      },
      "source": [
        "def process_comments(file_path):\n",
        "    \"\"\"\n",
        "    Processes a comments file by cleaning text and identifying spam.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the comments CSV file.\n",
        "\n",
        "    Returns:\n",
        "        polars.DataFrame: The processed DataFrame with cleaned comments and spam labels.\n",
        "    \"\"\"\n",
        "    df = pl.read_csv(file_path)\n",
        "    return df"
      ],
      "id": "cd83c794",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2e0737a"
      },
      "source": [
        "## Apply cleaning\n",
        "\n",
        "### Subtask:\n",
        "Apply the `clean_text` function to the relevant text column in the DataFrame loaded within the `process_comments` function.\n"
      ],
      "id": "c2e0737a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b829e89"
      },
      "source": [
        "**Reasoning**:\n",
        "Apply the clean_text function to the 'textOriginal' column and create a new 'cleaned_comment' column.\n",
        "\n"
      ],
      "id": "0b829e89"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "147ca044"
      },
      "source": [
        "def process_comments(file_path):\n",
        "    \"\"\"\n",
        "    Processes a comments file by cleaning text and identifying spam.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the comments CSV file.\n",
        "\n",
        "    Returns:\n",
        "        polars.DataFrame: The processed DataFrame with cleaned comments and spam labels.\n",
        "    \"\"\"\n",
        "    df = pl.read_csv(file_path)\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"textOriginal\").map_elements(clean_text, return_dtype=pl.Utf8).alias(\"cleaned_comment\")\n",
        "    )\n",
        "    return df"
      ],
      "id": "147ca044",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d27bb0d5"
      },
      "source": [
        "## Apply spam detection\n",
        "\n",
        "### Subtask:\n",
        "Apply the `is_spam` function to the cleaned text column to identify spam.\n"
      ],
      "id": "d27bb0d5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bafacb08"
      },
      "source": [
        "**Reasoning**:\n",
        "Apply the `is_spam` function to the `cleaned_comment` column and store the result in a new boolean column named `spam`.\n",
        "\n"
      ],
      "id": "bafacb08"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1a35fc1"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def process_comments(file_path):\n",
        "    \"\"\"\n",
        "    Processes a comments file by cleaning text and identifying spam.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the comments CSV file.\n",
        "\n",
        "    Returns:\n",
        "        polars.DataFrame: The processed DataFrame with cleaned comments and spam labels.\n",
        "    \"\"\"\n",
        "    df = pl.read_csv(file_path)\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"textOriginal\").map_elements(clean_text, return_dtype=pl.Utf8).alias(\"cleaned_comment\")\n",
        "    )\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"cleaned_comment\").map_elements(is_spam, return_dtype=pl.Boolean).alias(\"spam\")\n",
        "    )\n",
        "    return df"
      ],
      "id": "b1a35fc1",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0c76bce"
      },
      "source": [
        "## Return the labeled dataframe\n",
        "\n",
        "### Subtask:\n",
        "The function should return the DataFrame with the added 'cleaned_comment' and 'spam' columns.\n"
      ],
      "id": "d0c76bce"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a23a5ff"
      },
      "source": [
        "**Reasoning**:\n",
        "Review the `process_comments` function to ensure it returns the DataFrame with the added 'cleaned_comment' and 'spam' columns.\n",
        "\n"
      ],
      "id": "4a23a5ff"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a998383"
      },
      "source": [
        "def process_comments(file_path):\n",
        "    \"\"\"\n",
        "    Processes a comments file by cleaning text and identifying spam.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the comments CSV file.\n",
        "\n",
        "    Returns:\n",
        "        polars.DataFrame: The processed DataFrame with cleaned comments and spam labels.\n",
        "    \"\"\"\n",
        "    df = pl.read_csv(file_path)\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"textOriginal\").map_elements(clean_text, return_dtype=pl.Utf8).alias(\"cleaned_comment\")\n",
        "    )\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"cleaned_comment\").map_elements(is_spam, return_dtype=pl.Boolean).alias(\"spam\")\n",
        "    )\n",
        "    return df"
      ],
      "id": "4a998383",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "995ce74a"
      },
      "source": [
        "## Add example usage\n",
        "\n",
        "### Subtask:\n",
        "Include a section outside the function to demonstrate how to use it with a sample file and display the results.\n"
      ],
      "id": "995ce74a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbd401e0"
      },
      "source": [
        "**Reasoning**:\n",
        "Demonstrate the usage of the `process_comments` function by calling it with a sample file and displaying the head of the resulting DataFrame.\n",
        "\n"
      ],
      "id": "cbd401e0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e783ff41",
        "outputId": "420e5c67-2a3b-4a39-f799-f679baca8ea3"
      },
      "source": [
        "# Example usage\n",
        "sample_file_path = '/content/comments2.csv'\n",
        "processed_df = process_comments(sample_file_path)\n",
        "\n",
        "# Calculate and display spam vs non-spam counts and percentages\n",
        "spam_counts = processed_df.group_by(\"spam\").len()\n",
        "total_comments = processed_df.shape[0]\n",
        "\n",
        "print(\"\\nSpam vs Non-Spam Summary:\")\n",
        "for row in spam_counts.iter_rows(named=True):\n",
        "    label = \"Spam\" if row['spam'] else \"Not Spam\"\n",
        "    count = row['len']\n",
        "    percentage = (count / total_comments) * 100 if total_comments > 0 else 0\n",
        "    print(f\"{label}: {count} ({percentage:.2f}%)\")"
      ],
      "id": "e783ff41",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Spam vs Non-Spam Summary:\n",
            "Spam: 127967 (12.80%)\n",
            "Not Spam: 871973 (87.20%)\n",
            "Not Spam: 59 (0.01%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c79c80a1",
        "outputId": "0fe6f9df-0a41-4bcf-a90e-7bce83f9aeac"
      },
      "source": [
        "import polars as pl\n",
        "import re\n",
        "import pandas as pd  # optional\n",
        "\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    if text is None:\n",
        "        return \"\"   # return empty string instead of None\n",
        "\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation, numbers, and symbols but KEEP emojis & non-English chars\n",
        "    text = re.sub(r\"[0-9!\\\"#$%&'()*+,\\-./:;<=>?@\\[\\]^_`{|}~]\", \" \", text)\n",
        "\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def is_spam(comment: str) -> bool:\n",
        "    \"\"\"Return True if the comment looks like spam based on simple rules.\"\"\"\n",
        "    if not isinstance(comment, str):\n",
        "        return False\n",
        "    if not comment.strip():  # empty after cleaning\n",
        "        return False\n",
        "\n",
        "    text = comment.lower()\n",
        "\n",
        "    # Rule 1: contains a link\n",
        "    if re.search(r\"(http|www\\.|\\.com|\\.net|\\.org|\\.io|\\.co)\", text):\n",
        "        return True\n",
        "\n",
        "    # Rule 2: spammy keywords\n",
        "    if re.search(r\"(subscribe|check my channel|giveaway|free|click here|follow me|dm me)\", text):\n",
        "        return True\n",
        "\n",
        "    # Rule 3: too many emojis/symbols\n",
        "    if len(re.findall(r\"[^\\w\\s]\", comment)) > 10:\n",
        "        return True\n",
        "\n",
        "    # Rule 4: ultra-short and meaningless\n",
        "    tokens = comment.split()\n",
        "    if len(tokens) <= 1 and len(comment.strip()) <= 4:\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "def process_comments(file_path):\n",
        "    df = pl.read_csv(file_path)\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"textOriginal\").map_elements(clean_text, return_dtype=pl.Utf8).alias(\"cleaned_comment\")\n",
        "    )\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"cleaned_comment\").map_elements(is_spam, return_dtype=pl.Boolean).alias(\"spam\")\n",
        "    )\n",
        "    return df\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Example usage\n",
        "# -----------------------------\n",
        "sample_file_path = '/content/comments2.csv'\n",
        "processed_df = process_comments(sample_file_path)\n",
        "\n",
        "# Spam vs Non-Spam summary\n",
        "spam_counts = processed_df.group_by(\"spam\").len()\n",
        "total_comments = processed_df.shape[0]\n",
        "\n",
        "print(\"\\nSpam vs Non-Spam Summary:\")\n",
        "for row in spam_counts.iter_rows(named=True):\n",
        "    label = \"Spam\" if row['spam'] else \"Not Spam\"\n",
        "    count = row['len']\n",
        "    percentage = (count / total_comments) * 100 if total_comments > 0 else 0\n",
        "    print(f\"{label}: {count} ({percentage:.2f}%)\")\n",
        "\n",
        "# Overall spam percentage\n",
        "spam_total = spam_counts.filter(pl.col(\"spam\") == True)[\"len\"].sum()\n",
        "spam_percentage = (spam_total / total_comments) * 100 if total_comments > 0 else 0\n",
        "print(f\"\\nOverall Spam Percentage: {spam_percentage:.2f}%\")\n"
      ],
      "id": "c79c80a1",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Spam vs Non-Spam Summary:\n",
            "Not Spam: 59 (0.01%)\n",
            "Not Spam: 877906 (87.79%)\n",
            "Spam: 122034 (12.20%)\n",
            "\n",
            "Overall Spam Percentage: 12.20%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 0) Config\n",
        "# =========================\n",
        "FILE_PATH = \"/content/comments1.csv\"      # <-- change if needed\n",
        "TEXT_COL  = \"textOriginal\"\n",
        "SAVE_LABELED_CSV = True\n",
        "OUT_CSV_PATH = \"/content/comments1_labeled.csv\"\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 1) Cleaning\n",
        "# =========================\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"Lowercase, trim, collapse whitespace. Keep emojis & non-ASCII letters.\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower().strip()\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 2) Spam Heuristics (Enhanced)\n",
        "# =========================\n",
        "GENERIC_PRAISE_PAT = re.compile(\n",
        "    r\"^(love( it)?|i love|like( it)?|i like|nice|nice video|great|great video|amazing|so nice|so good|good|good video)$\"\n",
        ")\n",
        "\n",
        "LINK_PAT      = re.compile(r\"(http|https|www\\.)|(\\.com|\\.net|\\.org|\\.io|\\.co|\\.site|\\.shop)\", re.IGNORECASE)\n",
        "KEYWORDS_PAT  = re.compile(r\"(subscribe|check my channel|giveaway|free|click here|follow me|dm me|promo|collab)\", re.IGNORECASE)\n",
        "SYMBOL_PAT    = re.compile(r\"[^\\w\\s]\", re.UNICODE)\n",
        "\n",
        "def is_spam(comment: str) -> bool:\n",
        "    \"\"\"Deterministic rules for obvious and subtle spam, including generic praise spam.\"\"\"\n",
        "    if not isinstance(comment, str):\n",
        "        return False\n",
        "    if not comment.strip():\n",
        "        return False\n",
        "\n",
        "    text = comment.lower().strip()\n",
        "    tokens = text.split()\n",
        "\n",
        "    # Rule A: Links or classic spammy promo language\n",
        "    if LINK_PAT.search(text):\n",
        "        return True\n",
        "    if KEYWORDS_PAT.search(text):\n",
        "        return True\n",
        "\n",
        "    # Rule B: Overuse of symbols/emojis\n",
        "    # (lower threshold if very short; higher if longer)\n",
        "    sym_count = len(SYMBOL_PAT.findall(comment))\n",
        "    if sym_count > 10:\n",
        "        return True\n",
        "    if len(tokens) < 4 and sym_count > 6:\n",
        "        return True\n",
        "\n",
        "    # Rule C: Ultra-short junk (single token / tiny)\n",
        "    if len(tokens) <= 1 and len(text) <= 4:\n",
        "        return True\n",
        "\n",
        "    # Rule D: Generic praise spam (very short, context-free)\n",
        "    # Exact short phrases like \"love\", \"i love\", \"like\", \"nice video\", \"good\", etc.\n",
        "    if GENERIC_PRAISE_PAT.fullmatch(text):\n",
        "        return True\n",
        "\n",
        "    # Rule E: Low-context positive with too few words (e.g., \"love this\", \"nice one\")\n",
        "    if any(w in text for w in (\"love\", \"like\", \"nice\", \"great\", \"amazing\", \"good\")) and len(tokens) <= 3:\n",
        "        return True\n",
        "\n",
        "    # Rule F: Repeated characters/emoji floods (e.g., \"😍😍😍😍\")\n",
        "    if re.search(r\"(.)\\1{6,}\", text):  # 7+ same char in a row\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 3) Sentiment (5 buckets)\n",
        "# =========================\n",
        "def classify_sentiment(comment: str) -> str:\n",
        "    \"\"\"Map to one of: love / like / hate / not like / okay.\"\"\"\n",
        "    if not isinstance(comment, str) or not comment.strip():\n",
        "        return \"okay\"\n",
        "    text = comment.lower()\n",
        "\n",
        "    # Love\n",
        "    if re.search(r\"\\blove\\b|\\badore\\b|\\bamazing\\b|\\bfantastic\\b|\\bperfect\\b|❤️|💖|😍\", text):\n",
        "        return \"love\"\n",
        "\n",
        "    # Like\n",
        "    if re.search(r\"\\blike\\b|\\bgood\\b|\\bnice\\b|\\bdecent\\b|\\bwell done\\b|👍|😊\", text):\n",
        "        return \"like\"\n",
        "\n",
        "    # Hate\n",
        "    if re.search(r\"\\bhate\\b|\\bdisgust\\b|\\bworst\\b|\\bawful\\b|\\btrash\\b|🤮|😡\", text):\n",
        "        return \"hate\"\n",
        "\n",
        "    # Not like\n",
        "    if re.search(r\"\\bnot like\\b|\\bbad\\b|\\bboring\\b|\\bdislike\\b|👎\", text):\n",
        "        return \"not like\"\n",
        "\n",
        "    # Neutral / other\n",
        "    return \"okay\"\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 4) Load data\n",
        "# =========================\n",
        "df = pd.read_csv(FILE_PATH)\n",
        "\n",
        "if TEXT_COL not in df.columns:\n",
        "    raise ValueError(f\"Column '{TEXT_COL}' not found. Available: {list(df.columns)[:15]} ...\")\n",
        "\n",
        "# Clean text\n",
        "df[\"clean_text\"] = df[TEXT_COL].map(clean_text)\n",
        "\n",
        "# Optional: drop exact duplicate comments to reduce obvious bot duplication skew\n",
        "# (If you prefer to keep duplicates for raw counts, comment the next line.)\n",
        "# df = df.drop_duplicates(subset=[\"clean_text\"]).reset_index(drop=True)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 5) Spam first, then Sentiment on non-spam\n",
        "# =========================\n",
        "df[\"spam\"] = df[\"clean_text\"].map(is_spam)\n",
        "\n",
        "# Sentiment on ALL (for comparison)\n",
        "df[\"sentiment_raw\"] = df[\"clean_text\"].map(classify_sentiment)\n",
        "\n",
        "# Sentiment on NON-SPAM only\n",
        "df[\"sentiment\"] = df.apply(\n",
        "    lambda r: classify_sentiment(r[\"clean_text\"]) if not r[\"spam\"] else None, axis=1\n",
        ")\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 6) Summaries\n",
        "# =========================\n",
        "total = len(df)\n",
        "spam_ct = int(df[\"spam\"].sum())\n",
        "not_spam_ct = total - spam_ct\n",
        "\n",
        "def pct(x, d=2):\n",
        "    return round(100.0 * x / total, d) if total else 0.0\n",
        "\n",
        "print(\"\\n=== Spam vs Not Spam ===\")\n",
        "print({\n",
        "    \"spam\": spam_ct,\n",
        "    \"not_spam\": not_spam_ct,\n",
        "    \"spam_%\": pct(spam_ct),\n",
        "    \"not_spam_%\": pct(not_spam_ct),\n",
        "})\n",
        "\n",
        "print(\"\\n=== Sentiment Distribution (All Comments, Spam INCLUDED) ===\")\n",
        "sent_raw = (df[\"sentiment_raw\"].value_counts(normalize=True) * 100).round(2).to_dict()\n",
        "print(sent_raw)\n",
        "\n",
        "print(\"\\n=== Sentiment Distribution (NON-SPAM ONLY) ===\")\n",
        "sent_clean = (df.loc[~df[\"spam\"], \"sentiment\"].value_counts(normalize=True) * 100).round(2).to_dict()\n",
        "print(sent_clean)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 7) Example comments per bucket (Non-Spam Only)\n",
        "# =========================\n",
        "print(\"\\n=== Example Comments per Sentiment Bucket (Non-Spam) ===\")\n",
        "for bucket in [\"love\", \"like\", \"hate\", \"not like\", \"okay\"]:\n",
        "    ex = df.loc[(~df[\"spam\"]) & (df[\"sentiment\"] == bucket), TEXT_COL].head(5).tolist()\n",
        "    print(f\"\\n--- {bucket.upper()} ---\")\n",
        "    for s in ex:\n",
        "        print(f\"- {s}\")\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 8) (Optional) Save labeled output\n",
        "# =========================\n",
        "if SAVE_LABELED_CSV:\n",
        "    cols_to_save = [TEXT_COL, \"clean_text\", \"spam\", \"sentiment_raw\", \"sentiment\"]\n",
        "    df.to_csv(OUT_CSV_PATH, index=False, columns=[c for c in cols_to_save if c in df.columns])\n",
        "    print(f\"\\nSaved labeled file to: {OUT_CSV_PATH}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdZOuukJWo3F",
        "outputId": "2d3e939e-dc67-4a80-debc-0244128bfd25"
      },
      "id": "xdZOuukJWo3F",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Spam vs Not Spam ===\n",
            "{'spam': 189938, 'not_spam': 810062, 'spam_%': 18.99, 'not_spam_%': 81.01}\n",
            "\n",
            "=== Sentiment Distribution (All Comments, Spam INCLUDED) ===\n",
            "{'okay': 75.02, 'like': 12.24, 'love': 11.28, 'hate': 0.89, 'not like': 0.57}\n",
            "\n",
            "=== Sentiment Distribution (NON-SPAM ONLY) ===\n",
            "{'okay': 77.15, 'like': 11.48, 'love': 10.1, 'hate': 0.7, 'not like': 0.56}\n",
            "\n",
            "=== Example Comments per Sentiment Bucket (Non-Spam) ===\n",
            "\n",
            "--- LOVE ---\n",
            "- Love your videos. Thank you ❤❤❤\n",
            "- I love korean and japanese makeup!\n",
            "- \"men love pterodactyls\" *inserts fire alarm that jumpscared me outta my damn seat*\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "EDIT: Im a year late to this but who gives a shit anyways thanks for the 0 likes. Could care less if you liked or not, thats all your decision not mine. So have a great day!\n",
            "- Praise God for His beautiful creation. Let’s all Love the Lord our creator with all our hearts, souls & minds. Matthew 22:37-40. God bless.\n",
            "- Oud Satin Mood 😍\n",
            "\n",
            "--- LIKE ---\n",
            "- you look like raven from phenomena raven no cap\n",
            "- What hair oil did u use because the front of my hair a really short like yours was and I’m trying to grow it out but no hair oils are working so do u know which one u used it’s ok if don’t.\n",
            "- Heyyy video is excellent but why she has to do that much effort and why this kind of makeup it may harm her skin ...and i think that not good\n",
            "- I like your all vedios and your makeup\n",
            "- You’re welcome 😊\n",
            "\n",
            "--- HATE ---\n",
            "- I hate your videos you always say it's about lollipops😂\n",
            "- I KNOW I HATE IT\n",
            "- I have the worst RBF but I'm 50 years old.  It would take this makeup tip plus 2 little blackbirds hovering on either side of my face holding fishing line attached to my mouth with fish hooks to fix that sht\n",
            "- Why is her head always tilted? Not hate just observation lol\n",
            "- me compared to you...i look trash :(\n",
            "\n",
            "--- NOT LIKE ---\n",
            "- Bhen itna makeup khrb krne k bad😢\n",
            "- you guys would anyone wanna watch my grwm's.... im loek inspired by leahhh! i posted one which was kinda bad but im going to record one this weekend and post if any of you girlies would be interested <3 i would really appreciate the support!\n",
            "- Não 👎 a minha vida\n",
            "- I feel bad for the woman with the black \n",
            "Contacts😞\n",
            "👇🏿\n",
            "- It’s cool that the product are vegan but I except a guy I the vidéo we are now in 2020 and manny guys use makeup NOW so  I still don’t see guys in the videos it’s pity 👎🏼\n",
            "\n",
            "--- OKAY ---\n",
            "- PLEASE LESBIAN FLAG I BEG YOU \n",
            "\n",
            "You would rock it\n",
            "- Apply mashed potato juice and mixed it with curd\n",
            "- 69 missed calls from mars👽\n",
            "- American\n",
            "- Sahi disha me ja ja raha india ka Future..\n",
            "\n",
            "Saved labeled file to: /content/comments1_labeled.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# =========================\n",
        "# Config\n",
        "# =========================\n",
        "FILE_PATH = \"/content/comments1.csv\"   # <-- change if needed\n",
        "TEXT_COL  = \"textOriginal\"\n",
        "SAVE_LABELED_CSV = True\n",
        "OUT_CSV_PATH = \"/content/comments1_labeled.csv\"\n",
        "\n",
        "# Thresholds\n",
        "SHALLOW_MAX_WORDS = 3\n",
        "EMOJI_FLOOD_MIN   = 5\n",
        "SHALLOW_EMOJI_MIN = 3\n",
        "SYMBOLS_OVERALL   = 12\n",
        "\n",
        "POS_GENERIC = (\"love\", \"like\", \"nice\", \"good\", \"great\", \"amazing\", \"wow\", \"cool\")\n",
        "ABUSE_PAT = re.compile(r\"\\b(stupid|idiot|dumb|trash|kill yourself|kys|loser|disgusting|ugly)\\b\", re.IGNORECASE)\n",
        "\n",
        "# =========================\n",
        "# Cleaning\n",
        "# =========================\n",
        "def clean_text(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower().strip()\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text\n",
        "\n",
        "# =========================\n",
        "# Spam detection helpers\n",
        "# =========================\n",
        "LINK_PAT     = re.compile(r\"(http|https|www\\.)|(\\.com|\\.net|\\.org|\\.io|\\.co|\\.site|\\.shop)\", re.IGNORECASE)\n",
        "KEYWORDS_PAT = re.compile(r\"(subscribe|check my channel|giveaway|free|click here|follow me|dm me|promo|collab)\", re.IGNORECASE)\n",
        "SYMBOL_PAT   = re.compile(r\"[^\\w\\s]\", re.UNICODE)\n",
        "\n",
        "def token_count(text: str) -> int:\n",
        "    return len(text.split())\n",
        "\n",
        "def symbol_count(raw: str) -> int:\n",
        "    return len(SYMBOL_PAT.findall(raw))\n",
        "\n",
        "def is_shallow(text: str) -> bool:\n",
        "    return token_count(text) <= SHALLOW_MAX_WORDS\n",
        "\n",
        "def has_pos_generic(text: str) -> bool:\n",
        "    return any(w in text for w in POS_GENERIC)\n",
        "\n",
        "def emoji_flood(raw: str) -> bool:\n",
        "    return symbol_count(raw) >= EMOJI_FLOOD_MIN\n",
        "\n",
        "def spam_with_reason(comment: str):\n",
        "    reasons = set()\n",
        "    if not isinstance(comment, str) or not comment.strip():\n",
        "        return False, reasons\n",
        "\n",
        "    text = comment.lower().strip()\n",
        "    toks = token_count(text)\n",
        "    syms = symbol_count(comment)\n",
        "\n",
        "    if LINK_PAT.search(text): reasons.add(\"link\")\n",
        "    if KEYWORDS_PAT.search(text): reasons.add(\"promo\")\n",
        "    if syms >= SYMBOLS_OVERALL: reasons.add(\"symbols_overall\")\n",
        "    if is_shallow(text) and has_pos_generic(text): reasons.add(\"shallow_generic\")\n",
        "    if emoji_flood(comment): reasons.add(\"emoji_flood\")\n",
        "    if is_shallow(text) and syms >= SHALLOW_EMOJI_MIN: reasons.add(\"shallow_symbols\")\n",
        "    if toks <= 1 and len(text) <= 4: reasons.add(\"ultra_short\")\n",
        "\n",
        "    return (len(reasons) > 0), reasons\n",
        "\n",
        "# =========================\n",
        "# Sentiment (5 buckets)\n",
        "# =========================\n",
        "def classify_sentiment(comment: str) -> str:\n",
        "    if not isinstance(comment, str) or not comment.strip():\n",
        "        return \"okay\"\n",
        "    text = comment.lower()\n",
        "    if re.search(r\"\\blove\\b|\\badore\\b|\\bamazing\\b|\\bfantastic\\b|\\bperfect\\b|❤️|💖|😍\", text): return \"love\"\n",
        "    if re.search(r\"\\blike\\b|\\bgood\\b|\\bnice\\b|\\bdecent\\b|\\bwell done\\b|👍|😊\", text): return \"like\"\n",
        "    if re.search(r\"\\bhate\\b|\\bdisgust\\b|\\bworst\\b|\\bawful\\b|\\btrash\\b|🤮|😡\", text): return \"hate\"\n",
        "    if re.search(r\"\\bnot like\\b|\\bbad\\b|\\bboring\\b|\\bdislike\\b|👎\", text): return \"not like\"\n",
        "    return \"okay\"\n",
        "\n",
        "NEGATIVE_BUCKETS = {\"hate\", \"not like\"}\n",
        "\n",
        "# =========================\n",
        "# Toxicity\n",
        "# =========================\n",
        "def is_toxic(comment: str, spam_reasons: set, sentiment_label: str) -> bool:\n",
        "    if not isinstance(comment, str): return False\n",
        "    negative = sentiment_label in NEGATIVE_BUCKETS\n",
        "    shallow_or_emoji_spam = any(r in spam_reasons for r in (\"shallow_generic\",\"emoji_flood\",\"shallow_symbols\"))\n",
        "    abusive = ABUSE_PAT.search(comment) is not None\n",
        "    return negative and (shallow_or_emoji_spam or abusive)\n",
        "\n",
        "# =========================\n",
        "# Load & process\n",
        "# =========================\n",
        "df = pd.read_csv(FILE_PATH)\n",
        "df[\"clean_text\"] = df[TEXT_COL].map(clean_text)\n",
        "\n",
        "spam_info = df[\"clean_text\"].apply(spam_with_reason)\n",
        "df[\"spam\"] = spam_info.map(lambda x: x[0])\n",
        "df[\"spam_reasons\"] = spam_info.map(lambda x: \";\".join(sorted(list(x[1]))))\n",
        "\n",
        "df[\"sentiment_raw\"] = df[\"clean_text\"].map(classify_sentiment)\n",
        "df[\"sentiment\"] = df.apply(lambda r: classify_sentiment(r[\"clean_text\"]) if not r[\"spam\"] else None, axis=1)\n",
        "\n",
        "df[\"toxic\"] = df.apply(\n",
        "    lambda r: is_toxic(r[\"clean_text\"], set(r[\"spam_reasons\"].split(\";\")) if r[\"spam_reasons\"] else set(), r[\"sentiment_raw\"]),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# Summaries\n",
        "# =========================\n",
        "total = len(df)\n",
        "spam_ct = int(df[\"spam\"].sum())\n",
        "not_spam_ct = total - spam_ct\n",
        "toxic_ct = int(df[\"toxic\"].sum())\n",
        "\n",
        "def pct(part, whole): return round(100.0 * part / max(whole,1), 2)\n",
        "\n",
        "print(\"\\n=== Overall Summary ===\")\n",
        "print({\n",
        "    \"total\": total,\n",
        "    \"spam\": spam_ct,\n",
        "    \"not_spam\": not_spam_ct,\n",
        "    \"toxic\": toxic_ct,\n",
        "    \"spam_%\": pct(spam_ct, total),\n",
        "    \"not_spam_%\": pct(not_spam_ct, total),\n",
        "    \"toxic_%\": pct(toxic_ct, total)\n",
        "})\n",
        "\n",
        "print(\"\\n=== Sentiment Distribution (NON-SPAM ONLY, %) ===\")\n",
        "sent_clean = (df.loc[~df[\"spam\"], \"sentiment\"].value_counts(normalize=True) * 100).round(2).to_dict()\n",
        "print(sent_clean)\n",
        "\n",
        "# =========================\n",
        "# Save labeled file (optional)\n",
        "# =========================\n",
        "if SAVE_LABELED_CSV:\n",
        "    cols_to_save = [TEXT_COL, \"clean_text\", \"spam\", \"spam_reasons\", \"sentiment_raw\", \"sentiment\", \"toxic\"]\n",
        "    df.to_csv(OUT_CSV_PATH, index=False, columns=[c for c in cols_to_save if c in df.columns])\n",
        "    print(f\"\\nSaved labeled file to: {OUT_CSV_PATH}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHMyPOPybN89",
        "outputId": "46adb2ef-c411-4047-86b8-cfefdeb3f544"
      },
      "id": "NHMyPOPybN89",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Overall Summary ===\n",
            "{'total': 1000000, 'spam': 330046, 'not_spam': 669954, 'toxic': 6621, 'spam_%': 33.0, 'not_spam_%': 67.0, 'toxic_%': 0.66}\n",
            "\n",
            "=== Sentiment Distribution (NON-SPAM ONLY, %) ===\n",
            "{'okay': 79.86, 'like': 10.8, 'love': 8.2, 'hate': 0.59, 'not like': 0.54}\n",
            "\n",
            "Saved labeled file to: /content/comments1_labeled.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "THE MAIN PIPELINE (PLS JUST RUN THIS)"
      ],
      "metadata": {
        "id": "JhAQv4zzmBs6"
      },
      "id": "JhAQv4zzmBs6"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# ============================================================\n",
        "# Config\n",
        "# ============================================================\n",
        "FILE_PATH  = \"/content/comments1.csv\"     # <-- change to your file\n",
        "TEXT_COL   = \"textOriginal\"               # comment text column\n",
        "DATE_COL   = \"publishedAt\"                # optional; set None if not available\n",
        "USER_COL   = \"authorDisplayName\"          # optional; set None if not available\n",
        "\n",
        "# Output options\n",
        "SAVE_LABELED_CSV   = True\n",
        "LABELED_OUT_PATH   = \"/content/comments1_labeled.csv\"\n",
        "SAVE_SUMMARY_JSON  = True\n",
        "SUMMARY_OUT_PATH   = \"/content/comments1_kpis.json\"\n",
        "SAVE_CATEGORY_CSV  = True\n",
        "CATEGORY_CSV_PATH  = \"/content/comments1_category_sentiment.csv\"\n",
        "SAVE_TRENDS_CSV    = True\n",
        "TRENDS_CSV_PATH    = \"/content/comments1_time_trends.csv\"\n",
        "\n",
        "# KPI thresholds\n",
        "SHALLOW_MAX_WORDS = 3\n",
        "EMOJI_FLOOD_MIN   = 5\n",
        "SHALLOW_EMOJI_MIN = 3\n",
        "SYMBOLS_OVERALL   = 12\n",
        "\n",
        "# Category keyword mapping (extend/tune freely)\n",
        "CATEGORY_RULES = {\n",
        "    \"Hair\":       [r\"\\bhair\\b\", r\"\\bshampoo\\b\", r\"\\bconditioner\\b\", r\"\\bhair(oil|mask|spray)\\b\"],\n",
        "    \"Makeup\":     [r\"\\bmake ?up\\b\", r\"\\bmascara\\b\", r\"\\blip(stick|gloss)\\b\", r\"\\beyeliner\\b\", r\"\\bfoundation\\b\", r\"\\bblush\\b\"],\n",
        "    \"Skincare\":   [r\"\\bskin ?care\\b\", r\"\\bserum\\b\", r\"\\bmoisturizer\\b\", r\"\\bretinol\\b\", r\"\\bspf\\b\", r\"\\bsunscreen\\b\", r\"\\bcleanser\\b\"],\n",
        "    \"Packaging\":  [r\"\\bpackage|packaging|box|bottle|cap|pump\\b\"],\n",
        "    \"Fragrance\":  [r\"\\bperfume\\b\", r\"\\bfragrance\\b\", r\"\\beau de\\b\", r\"\\bparfum\\b\", r\"\\bcologne\\b\"],\n",
        "}\n",
        "# Everything else goes to \"Other\"\n",
        "\n",
        "# ============================================================\n",
        "# Utilities & Cleaning\n",
        "# ============================================================\n",
        "def pct(part, whole, d=2):\n",
        "    return round(100.0 * part / max(whole, 1), d)\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower().strip()\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text\n",
        "\n",
        "# ============================================================\n",
        "# Spam Detection (classic + shallow + emoji)\n",
        "# ============================================================\n",
        "LINK_PAT     = re.compile(r\"(http|https|www\\.)|(\\.com|\\.net|\\.org|\\.io|\\.co|\\.site|\\.shop)\", re.IGNORECASE)\n",
        "KEYWORDS_PAT = re.compile(r\"(subscribe|check my channel|giveaway|free|click here|follow me|dm me|promo|collab)\", re.IGNORECASE)\n",
        "SYMBOL_PAT   = re.compile(r\"[^\\w\\s]\", re.UNICODE)\n",
        "ABUSE_PAT    = re.compile(r\"\\b(stupid|idiot|dumb|trash|kill yourself|kys|loser|disgusting|ugly)\\b\", re.IGNORECASE)\n",
        "\n",
        "POS_GENERIC = (\"love\", \"like\", \"nice\", \"good\", \"great\", \"amazing\", \"wow\", \"cool\")\n",
        "NEGATIVE_BUCKETS = {\"Hated it\", \"Disliked it\"}\n",
        "\n",
        "def token_count(text: str) -> int:\n",
        "    return len(text.split())\n",
        "\n",
        "def symbol_count(raw: str) -> int:\n",
        "    return len(SYMBOL_PAT.findall(raw))\n",
        "\n",
        "def is_shallow(text: str) -> bool:\n",
        "    return token_count(text) <= SHALLOW_MAX_WORDS\n",
        "\n",
        "def has_pos_generic(text: str) -> bool:\n",
        "    return any(w in text for w in POS_GENERIC)\n",
        "\n",
        "def emoji_flood(raw: str) -> bool:\n",
        "    return symbol_count(raw) >= EMOJI_FLOOD_MIN\n",
        "\n",
        "def spam_with_reason(comment: str):\n",
        "    \"\"\"\n",
        "    Returns (is_spam: bool, reasons: set[str])\n",
        "    reasons ⊆ {link, promo, symbols_overall, shallow_generic, emoji_flood, shallow_symbols, ultra_short}\n",
        "    \"\"\"\n",
        "    reasons = set()\n",
        "    if not isinstance(comment, str) or not comment.strip():\n",
        "        return False, reasons\n",
        "\n",
        "    text = comment.lower().strip()\n",
        "    toks = token_count(text)\n",
        "    syms = symbol_count(comment)\n",
        "\n",
        "    if LINK_PAT.search(text): reasons.add(\"link\")\n",
        "    if KEYWORDS_PAT.search(text): reasons.add(\"promo\")\n",
        "    if syms >= SYMBOLS_OVERALL: reasons.add(\"symbols_overall\")\n",
        "\n",
        "    # Shallow generic praise\n",
        "    if is_shallow(text) and has_pos_generic(text):\n",
        "        reasons.add(\"shallow_generic\")\n",
        "\n",
        "    # Emoji flood + shallow+symbols combo\n",
        "    if emoji_flood(comment): reasons.add(\"emoji_flood\")\n",
        "    if is_shallow(text) and syms >= SHALLOW_EMOJI_MIN: reasons.add(\"shallow_symbols\")\n",
        "\n",
        "    # Ultra-short junk (e.g., \"ok\", \"yo\", \"👍\")\n",
        "    if toks <= 1 and len(text) <= 4:\n",
        "        reasons.add(\"ultra_short\")\n",
        "\n",
        "    return (len(reasons) > 0), reasons\n",
        "\n",
        "# ============================================================\n",
        "# Sentiment (5 buckets)\n",
        "# ============================================================\n",
        "def classify_sentiment(comment: str) -> str:\n",
        "    \"\"\"\n",
        "    5 buckets for reporting:\n",
        "      Loved it / Liked it / Okay / Disliked it / Hated it\n",
        "    \"\"\"\n",
        "    if not isinstance(comment, str) or not comment.strip():\n",
        "        return \"Okay\"\n",
        "    text = comment.lower()\n",
        "    if re.search(r\"\\blove\\b|\\badore\\b|\\bamazing\\b|\\bfantastic\\b|\\bperfect\\b|❤️|💖|😍\", text): return \"Loved it\"\n",
        "    if re.search(r\"\\blike\\b|\\bgood\\b|\\bnice\\b|\\bdecent\\b|\\bwell done\\b|👍|😊\", text): return \"Liked it\"\n",
        "    if re.search(r\"\\bhate\\b|\\bdisgust\\b|\\bworst\\b|\\bawful\\b|\\btrash\\b|🤮|😡\", text): return \"Hated it\"\n",
        "    if re.search(r\"\\bnot like\\b|\\bbad\\b|\\bboring\\b|\\bdislike\\b|👎\", text): return \"Disliked it\"\n",
        "    return \"Okay\"\n",
        "\n",
        "# ============================================================\n",
        "# Toxicity\n",
        "# ============================================================\n",
        "def is_toxic(comment: str, spam_reasons: set, sentiment_label: str) -> bool:\n",
        "    \"\"\"\n",
        "    Toxic if:\n",
        "      - sentiment is negative (Hated it / Disliked it), AND\n",
        "      - (spam due to shallow/emoji) OR contains obvious abusive lexicon.\n",
        "    \"\"\"\n",
        "    if not isinstance(comment, str): return False\n",
        "    negative = sentiment_label in NEGATIVE_BUCKETS\n",
        "    shallow_or_emoji_spam = any(r in spam_reasons for r in (\"shallow_generic\",\"emoji_flood\",\"shallow_symbols\"))\n",
        "    abusive = ABUSE_PAT.search(comment) is not None\n",
        "    return negative and (shallow_or_emoji_spam or abusive)\n",
        "\n",
        "# ============================================================\n",
        "# Category tagging\n",
        "# ============================================================\n",
        "def assign_category(text: str) -> str:\n",
        "    for cat, patterns in CATEGORY_RULES.items():\n",
        "        for pat in patterns:\n",
        "            if re.search(pat, text):\n",
        "                return cat\n",
        "    return \"Other\"\n",
        "\n",
        "# ============================================================\n",
        "# KPI Computation (collision-safe)\n",
        "# ============================================================\n",
        "def compute_kpis(df: pd.DataFrame):\n",
        "    total = len(df)\n",
        "    spam_ct = int(df[\"spam\"].sum())\n",
        "    non_spam_ct = total - spam_ct\n",
        "    toxic_ct = int(df[\"toxic\"].sum())\n",
        "\n",
        "    # Engagement metrics\n",
        "    avg_len = float(df[\"clean_text\"].str.len().mean() or 0)\n",
        "    unique_users = int(df[USER_COL].nunique()) if USER_COL and USER_COL in df.columns else None\n",
        "    dedup_count = int(df[\"clean_text\"].nunique())\n",
        "\n",
        "    # Comment Quality Index (meaningful / shallow+emoji)\n",
        "    shallow_or_emoji_flags = df[\"spam_reasons\"].str.contains(\n",
        "        \"shallow_generic|emoji_flood|shallow_symbols\", regex=True, na=False\n",
        "    )\n",
        "    shallow_emoji_ct = int(shallow_or_emoji_flags.sum())\n",
        "    cqi_ratio = round((non_spam_ct / max(shallow_emoji_ct, 1)), 3)\n",
        "\n",
        "    # Sentiment on non-spam\n",
        "    non_spam = df.loc[~df[\"spam\"]].copy()\n",
        "    sent_dist_pct = (non_spam[\"sentiment\"].value_counts(normalize=True) * 100).round(2).to_dict()\n",
        "\n",
        "    # Category-wise sentiment (non-spam)\n",
        "    non_spam_cat = non_spam.copy()\n",
        "    if \"category\" not in non_spam_cat.columns:\n",
        "        non_spam_cat[\"category\"] = \"Other\"\n",
        "    cat_sent = non_spam_cat.groupby([\"category\", \"sentiment\"]).size().reset_index(name=\"count\")\n",
        "    if len(cat_sent):\n",
        "        cat_sent_pivot = cat_sent.pivot_table(\n",
        "            index=\"category\", columns=\"sentiment\", values=\"count\", aggfunc=\"sum\", fill_value=0\n",
        "        )\n",
        "        cat_sent_pivot[\"total\"] = cat_sent_pivot.sum(axis=1)\n",
        "        for col in cat_sent_pivot.columns:\n",
        "            if col != \"total\":\n",
        "                cat_sent_pivot[col] = (cat_sent_pivot[col] / cat_sent_pivot[\"total\"] * 100).round(2)\n",
        "    else:\n",
        "        cat_sent_pivot = pd.DataFrame(columns=[\"category\", \"total\"]).set_index(\"category\")\n",
        "\n",
        "    # Time trends (daily sentiment %, weekly toxicity %) if DATE_COL exists\n",
        "    trends_daily = None\n",
        "    trends_weekly = None\n",
        "    if DATE_COL and DATE_COL in df.columns:\n",
        "        dft = df.copy()\n",
        "        dft[\"date\"] = pd.to_datetime(dft[DATE_COL], errors=\"coerce\")\n",
        "        dft = dft.dropna(subset=[\"date\"])\n",
        "\n",
        "        # Daily sentiment % (non-spam) — use temp column to avoid collisions\n",
        "        daily = dft.loc[~dft[\"spam\"]].copy()\n",
        "        DAY_COL = \"_day_tmp\"\n",
        "        if DAY_COL in daily.columns:\n",
        "            daily.drop(columns=[DAY_COL], inplace=True, errors=\"ignore\")\n",
        "        daily.loc[:, DAY_COL] = daily[\"date\"].dt.date\n",
        "\n",
        "        if len(daily):\n",
        "            trends_daily = (\n",
        "                daily.groupby([DAY_COL, \"sentiment\"])\n",
        "                .size()\n",
        "                .groupby(level=0)\n",
        "                .apply(lambda s: (s / s.sum() * 100).round(2))\n",
        "                .unstack(fill_value=0)\n",
        "                .reset_index()\n",
        "                .rename(columns={DAY_COL: \"day\"})\n",
        "            )\n",
        "\n",
        "        # Weekly toxicity % — use temp column to avoid collisions\n",
        "        dft_local = dft.copy()\n",
        "        WEEK_COL = \"_week_tmp\"\n",
        "        if WEEK_COL in dft_local.columns:\n",
        "            dft_local.drop(columns=[WEEK_COL], inplace=True, errors=\"ignore\")\n",
        "        dft_local.loc[:, WEEK_COL] = dft_local[\"date\"].dt.to_period(\"W-SUN\").apply(lambda p: p.start_time.date())\n",
        "\n",
        "        if len(dft_local):\n",
        "            trends_weekly = (\n",
        "                dft_local.groupby(WEEK_COL)[\"toxic\"]\n",
        "                .mean()\n",
        "                .mul(100)\n",
        "                .round(2)\n",
        "                .reset_index()\n",
        "                .rename(columns={WEEK_COL: \"week\", \"toxic\": \"toxic_%\"})\n",
        "            )\n",
        "\n",
        "    summary = {\n",
        "        \"kpis\": {\n",
        "            \"percent_spam\": pct(spam_ct, total),\n",
        "            \"percent_non_spam\": pct(non_spam_ct, total),\n",
        "            \"percent_toxic\": pct(toxic_ct, total),\n",
        "        },\n",
        "        \"sentiment_distribution_non_spam_%\": sent_dist_pct,\n",
        "        \"comment_quality_index_ratio_meaningful_over_shallow_emoji\": cqi_ratio,\n",
        "        \"engagement\": {\n",
        "            \"avg_comment_length_chars\": round(avg_len, 2),\n",
        "            \"unique_users\": unique_users,\n",
        "            \"unique_texts_after_dedup\": dedup_count,\n",
        "            \"duplicates_removed\": total - dedup_count,\n",
        "        },\n",
        "    }\n",
        "    return summary, cat_sent_pivot, trends_daily, trends_weekly\n",
        "\n",
        "# ============================================================\n",
        "# Main\n",
        "# ============================================================\n",
        "df = pd.read_csv(FILE_PATH)\n",
        "if TEXT_COL not in df.columns:\n",
        "    raise ValueError(f\"Column '{TEXT_COL}' not found. Available: {list(df.columns)[:15]} ...\")\n",
        "\n",
        "# Clean\n",
        "df[\"clean_text\"] = df[TEXT_COL].map(clean_text)\n",
        "\n",
        "# Spam + reasons\n",
        "spam_info = df[\"clean_text\"].apply(spam_with_reason)\n",
        "df[\"spam\"] = spam_info.map(lambda x: x[0])\n",
        "df[\"spam_reasons\"] = spam_info.map(lambda x: \";\".join(sorted(list(x[1]))))\n",
        "\n",
        "# Sentiment\n",
        "df[\"sentiment_raw\"] = df[\"clean_text\"].map(classify_sentiment)\n",
        "df[\"sentiment\"] = df.apply(lambda r: classify_sentiment(r[\"clean_text\"]) if not r[\"spam\"] else None, axis=1)\n",
        "\n",
        "# Toxic\n",
        "df[\"toxic\"] = df.apply(\n",
        "    lambda r: is_toxic(\n",
        "        r[\"clean_text\"],\n",
        "        set(r[\"spam_reasons\"].split(\";\")) if r[\"spam_reasons\"] else set(),\n",
        "        r[\"sentiment_raw\"],\n",
        "    ),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Category tagging\n",
        "df[\"category\"] = df[\"clean_text\"].map(assign_category)\n",
        "\n",
        "# KPIs\n",
        "summary, cat_sent_pivot, trends_daily, trends_weekly = compute_kpis(df)\n",
        "\n",
        "# Print KPI summary (numbers only)\n",
        "print(\"\\n=== KPI Summary ===\")\n",
        "print(json.dumps(summary, ensure_ascii=False, indent=2))\n",
        "\n",
        "# Save labeled file\n",
        "if SAVE_LABELED_CSV:\n",
        "    cols_to_save = [TEXT_COL,\"clean_text\",\"spam\",\"spam_reasons\",\"sentiment\",\"sentiment_raw\",\"toxic\",\"category\"]\n",
        "    df.to_csv(LABELED_OUT_PATH, index=False, columns=[c for c in cols_to_save if c in df.columns])\n",
        "    print(f\"Saved labeled file to: {LABELED_OUT_PATH}\")\n",
        "\n",
        "# Save KPI summary JSON\n",
        "if SAVE_SUMMARY_JSON:\n",
        "    with open(SUMMARY_OUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(summary, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"Saved KPI summary JSON to: {SUMMARY_OUT_PATH}\")\n",
        "\n",
        "# Save category-wise sentiment CSV\n",
        "if SAVE_CATEGORY_CSV:\n",
        "    cat_sent_pivot.to_csv(CATEGORY_CSV_PATH)\n",
        "    print(f\"Saved category-wise sentiment CSV to: {CATEGORY_CSV_PATH}\")\n",
        "\n",
        "# Save time trends CSV (if date is available)\n",
        "if SAVE_TRENDS_CSV and trends_daily is not None:\n",
        "    trends_daily.to_csv(TRENDS_CSV_PATH, index=False)\n",
        "    print(f\"Saved time trends (daily sentiment %) CSV to: {TRENDS_CSV_PATH}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "rm5zC0PRlsVr",
        "outputId": "5e8aa0ba-c2d0-40bb-9d0b-cee6f2744ee4"
      },
      "id": "rm5zC0PRlsVr",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "cannot insert _day_tmp, already exists",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3200182969.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;31m# KPIs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_sent_pivot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrends_daily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrends_weekly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_kpis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;31m# Print KPI summary (numbers only)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3200182969.py\u001b[0m in \u001b[0;36mcompute_kpis\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    212\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0munstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m                 \u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mDAY_COL\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"day\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mreset_index\u001b[0;34m(self, level, drop, inplace, col_level, col_fill, allow_duplicates, names)\u001b[0m\n\u001b[1;32m   6470\u001b[0m                     )\n\u001b[1;32m   6471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6472\u001b[0;31m                 new_obj.insert(\n\u001b[0m\u001b[1;32m   6473\u001b[0m                     \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6474\u001b[0m                     \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(self, loc, column, value, allow_duplicates)\u001b[0m\n\u001b[1;32m   5156\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_duplicates\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5157\u001b[0m             \u001b[0;31m# Should this be a different kind of error??\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5158\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"cannot insert {column}, already exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5159\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5160\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loc must be int\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot insert _day_tmp, already exists"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}