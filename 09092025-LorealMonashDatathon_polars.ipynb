{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "491e8112-55df-484e-925d-407f77181709",
   "metadata": {},
   "source": [
    "## COMBINING DATASET "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e9cc1d-4014-4e40-ad77-8b7a8080142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0f7bead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Combined dataset saved as BIG_BOY.csv\n",
      "Shape of BIG BOY: (4725012, 10)\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "\n",
    "# List of CSV files\n",
    "csv_files = [\n",
    "    \"comments1.csv\",\n",
    "    \"comments2.csv\",\n",
    "    \"comments3.csv\",\n",
    "    \"comments4.csv\",\n",
    "    \"comments5.csv\"\n",
    "]\n",
    "\n",
    "# Read and combine into one dataframe\n",
    "df_list = [pl.read_csv(file) for file in csv_files]\n",
    "big_boy = pl.concat(df_list, how=\"vertical\")\n",
    "\n",
    "# Save the combined dataset\n",
    "big_boy.write_csv(\"BIG_BOY.csv\")\n",
    "\n",
    "print(\"✅ Combined dataset saved as BIG_BOY.csv\")\n",
    "print(\"Shape of BIG BOY:\", big_boy.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b46699fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Random sample of 100,000 created\n",
      "Shape of sample: (100000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Randomly choose 20k rows\n",
    "sample_100k = big_boy.sample(n=100000)  # random_state makes it reproducible\n",
    "\n",
    "print(\"✅ Random sample of 100,000 created\")\n",
    "print(\"Shape of sample:\", sample_100k.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa5f5bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.head of shape: (100_000, 10)\n",
       "┌─────────────────┬───────────┬───────────┬─────────┬───┬────────────────┬───────────┬────────────────┬────────────────┐\n",
       "│ kind            ┆ commentId ┆ channelId ┆ videoId ┆ … ┆ parentCommentI ┆ likeCount ┆ publishedAt    ┆ updatedAt      │\n",
       "│ ---             ┆ ---       ┆ ---       ┆ ---     ┆   ┆ d              ┆ ---       ┆ ---            ┆ ---            │\n",
       "│ str             ┆ i64       ┆ i64       ┆ i64     ┆   ┆ ---            ┆ i64       ┆ str            ┆ str            │\n",
       "│                 ┆           ┆           ┆         ┆   ┆ i64            ┆           ┆                ┆                │\n",
       "╞═════════════════╪═══════════╪═══════════╪═════════╪═══╪════════════════╪═══════════╪════════════════╪════════════════╡\n",
       "│ youtube#comment ┆ 4190194   ┆ 27537     ┆ 28697   ┆ … ┆ null           ┆ 0         ┆ 2023-12-08     ┆ 2023-12-08     │\n",
       "│                 ┆           ┆           ┆         ┆   ┆                ┆           ┆ 02:11:52+00:00 ┆ 02:11:52+00:00 │\n",
       "│ youtube#comment ┆ 2850953   ┆ 30497     ┆ 50919   ┆ … ┆ null           ┆ 0         ┆ 2025-04-05     ┆ 2025-04-05     │\n",
       "│                 ┆           ┆           ┆         ┆   ┆                ┆           ┆ 17:24:16+00:00 ┆ 17:24:16+00:00 │\n",
       "│ youtube#comment ┆ 2276279   ┆ 8177      ┆ 38551   ┆ … ┆ null           ┆ 0         ┆ 2023-05-06     ┆ 2023-05-06     │\n",
       "│                 ┆           ┆           ┆         ┆   ┆                ┆           ┆ 09:17:12+00:00 ┆ 09:17:12+00:00 │\n",
       "│ youtube#comment ┆ 233310    ┆ 45150     ┆ 78579   ┆ … ┆ 1141187        ┆ 0         ┆ 2023-06-08     ┆ 2023-06-08     │\n",
       "│                 ┆           ┆           ┆         ┆   ┆                ┆           ┆ 07:16:51+00:00 ┆ 07:16:51+00:00 │\n",
       "│ youtube#comment ┆ 2506607   ┆ 45869     ┆ 16909   ┆ … ┆ null           ┆ 0         ┆ 2022-11-08     ┆ 2022-11-08     │\n",
       "│                 ┆           ┆           ┆         ┆   ┆                ┆           ┆ 15:50:29+00:00 ┆ 15:50:29+00:00 │\n",
       "│ …               ┆ …         ┆ …         ┆ …       ┆ … ┆ …              ┆ …         ┆ …              ┆ …              │\n",
       "│ youtube#comment ┆ 1835435   ┆ 28652     ┆ 11147   ┆ … ┆ null           ┆ 0         ┆ 2023-07-11     ┆ 2023-07-11     │\n",
       "│                 ┆           ┆           ┆         ┆   ┆                ┆           ┆ 11:56:45+00:00 ┆ 11:56:45+00:00 │\n",
       "│ youtube#comment ┆ 114750    ┆ 1100      ┆ 8565    ┆ … ┆ 844353         ┆ 0         ┆ 2022-02-02     ┆ 2022-02-02     │\n",
       "│                 ┆           ┆           ┆         ┆   ┆                ┆           ┆ 07:42:46+00:00 ┆ 07:42:46+00:00 │\n",
       "│ youtube#comment ┆ 2039319   ┆ 16357     ┆ 32130   ┆ … ┆ null           ┆ 0         ┆ 2024-04-28     ┆ 2024-04-28     │\n",
       "│                 ┆           ┆           ┆         ┆   ┆                ┆           ┆ 12:26:29+00:00 ┆ 12:26:29+00:00 │\n",
       "│ youtube#comment ┆ 4621234   ┆ 40931     ┆ 22406   ┆ … ┆ null           ┆ 0         ┆ 2025-04-25     ┆ 2025-04-25     │\n",
       "│                 ┆           ┆           ┆         ┆   ┆                ┆           ┆ 16:43:23+00:00 ┆ 16:43:23+00:00 │\n",
       "│ youtube#comment ┆ 4455470   ┆ 1732      ┆ 68783   ┆ … ┆ null           ┆ 0         ┆ 2023-07-13     ┆ 2023-07-13     │\n",
       "│                 ┆           ┆           ┆         ┆   ┆                ┆           ┆ 01:39:36+00:00 ┆ 01:39:36+00:00 │\n",
       "└─────────────────┴───────────┴───────────┴─────────┴───┴────────────────┴───────────┴────────────────┴────────────────┘>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_100k.head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90a682d-d9a5-40b7-94cf-68be19d27b8e",
   "metadata": {},
   "source": [
    "pipline of whats about to go down\n",
    "\n",
    "- combine dataset to bigboy ✅\n",
    "- randomly select 100k rows for a sample1 and preprocess (remove emoji, anything not in english, any non english words) ✅\n",
    "- run NMF on the sample1 to get categories\n",
    "- go back to bigboy and create categories column and do keyword-matching to sort the data\n",
    "- make a sample of 100k using stratified sampling\n",
    "- pass it onto dalia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2797709-5d2c-469c-89a6-7d8d8cd5bb41",
   "metadata": {},
   "source": [
    "## TEXT CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e00e068",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf18b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741a4749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "from langdetect import detect, DetectorFactory\n",
    "\n",
    "# Fix seed for langdetect reproducibility\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# Use your existing sample_100k Polars DataFrame\n",
    "df = sample_100k\n",
    "\n",
    "# Function: remove emojis\n",
    "def remove_emojis(text: str) -> str:\n",
    "    return emoji.replace_emoji(text, replace='')\n",
    "\n",
    "# Function: clean text\n",
    "def clean_text(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = remove_emojis(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(rf\"[{re.escape(string.punctuation)}]\", \" \", text)  # remove punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # remove extra spaces\n",
    "    return text\n",
    "\n",
    "# Function: keep only English comments\n",
    "def is_english(text: str) -> bool:\n",
    "    try:\n",
    "        return detect(text) == \"en\"\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Apply cleaning with map_elements\n",
    "df = df.with_columns(\n",
    "    df[\"textOriginal\"].map_elements(clean_text, return_dtype=pl.Utf8).alias(\"clean_text\")\n",
    ")\n",
    "\n",
    "# Filter English only\n",
    "df = df.filter(\n",
    "    df[\"clean_text\"].map_elements(is_english, return_dtype=pl.Boolean)\n",
    ")\n",
    "\n",
    "print(df.select([\"textOriginal\", \"clean_text\"]).head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206277b1-fb8c-484f-bd0f-24a0c5709c6d",
   "metadata": {},
   "source": [
    "## NMF MODEL TO GET TOPICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a879afa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Assuming df already has the 'clean_text' column from preprocessing\n",
    "texts = df[\"clean_text\"].drop_nulls().tolist()\n",
    "\n",
    "# Step 1: TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.95,       # ignore very common words\n",
    "    min_df=2,          # ignore very rare words\n",
    "    stop_words=\"english\"  # remove English stopwords\n",
    ")\n",
    "tfidf = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Step 2: NMF Model\n",
    "n_topics = 10\n",
    "nmf_model = NMF(n_components=n_topics, random_state=42)\n",
    "nmf_model.fit(tfidf)\n",
    "\n",
    "# Step 3: Display topics\n",
    "def display_topics(model, feature_names, n_top_words=10):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        print(f\"Topic {topic_idx+1}: {', '.join(top_features)}\")\n",
    "\n",
    "# Print topics\n",
    "tfidf_feature_names = vectorizer.get_feature_names_out()\n",
    "display_topics(nmf_model, tfidf_feature_names, n_top_words=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a2211c-ea01-436e-9eea-73a70efbbedc",
   "metadata": {},
   "source": [
    "## CREATING CATEGORIES COLUMN IN BIGBOY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7303fea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#that we got from NMF model\n",
    "categories = {\n",
    "    \"appearance_compliments\": [\"beautiful\", \"wow\", \"looking\", \"night\", \"absolutely\", \"bald\", \"naturally\"],\n",
    "    \"gratitude_milestones\": [\"thank\", \"share\", \"dear\", \"aww\", \"100million\", \"hit\", \"watching\", \"lets\"],\n",
    "    \"hair\": [\"hair\", \"curly\", \"long\", \"straight\", \"cut\", \"black\", \"style\", \"short\", \"wavy\"],\n",
    "    \"makeup_beauty\": [\"makeup\", \"wear\", \"indian\", \"best\", \"gorgeous\", \"nice\", \"wearing\"],\n",
    "    \"love_india_music\": [\"love\", \"india\", \"videos\", \"omg\", \"natural\", \"song\", \"ginger\"],\n",
    "    \"looks_wigs\": [\"pretty\", \"omg\", \"ur\", \"wig\", \"looks\", \"really\", \"wow\"],\n",
    "    \"general_compliments\": [\"look\", \"good\", \"night\", \"gorgeous\", \"amazing\", \"better\", \"great\", \"day\"],\n",
    "    \"thanks_tips\": [\"thanks\", \"share\", \"dear\", \"sharing\", \"tips\", \"watching\"],\n",
    "    \"opinions_casual\": [\"like\", \"don\", \"just\", \"make\", \"way\", \"people\", \"know\"],\n",
    "    \"cuteness_voice\": [\"cute\", \"wow\", \"voice\", \"boy\", \"girl\", \"soo\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87389a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import re\n",
    "\n",
    "# Example: loading big dataset (replace with your real file)\n",
    "df = big_boy  # contains column 'textOriginal'\n",
    "\n",
    "# Function: match comment to a category\n",
    "def categorize_comment(text, categories):\n",
    "    text = str(text).lower()\n",
    "    for cat, keywords in categories.items():\n",
    "        # simple keyword match\n",
    "        if any(re.search(rf\"\\b{kw}\\b\", text) for kw in keywords):\n",
    "            return cat\n",
    "    return \"other\"  # if no keyword matches\n",
    "\n",
    "# Apply to dataset\n",
    "df[\"category\"] = df[\"textOriginal\"].apply(lambda x: categorize_comment(x, categories))\n",
    "\n",
    "# Check distribution\n",
    "print(df[\"category\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938d70a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install flashtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaaa649",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flashtext import KeywordProcessor\n",
    "\n",
    "# Build processor\n",
    "keyword_processor = KeywordProcessor()\n",
    "for cat, keywords in categories.items():\n",
    "    for kw in keywords:\n",
    "        keyword_processor.add_keyword(kw, cat)\n",
    "\n",
    "# Match function\n",
    "def fast_categorize(text):\n",
    "    matches = keyword_processor.extract_keywords(str(text).lower())\n",
    "    return matches[0] if matches else \"other\"\n",
    "\n",
    "# Apply fast\n",
    "df[\"category\"] = df[\"textOriginal\"].apply(fast_categorize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfa398f-9efa-4ca7-bff3-7eab4d6e1200",
   "metadata": {},
   "source": [
    "## STRATIFIED RANDOM SAMPLING TO GET SAMPLE OF 100K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f834b54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Load your big dataset (already has \"category\" column)\n",
    "\n",
    "# Desired sample size\n",
    "sample_size = 100_000  \n",
    "\n",
    "# Compute sampling fraction per category\n",
    "fractions = sample_size / len(big_boy)\n",
    "\n",
    "# Stratified sample\n",
    "sample4model = big_boy.groupby(\"category\", group_keys=False).apply(\n",
    "    lambda x: x.sample(frac=fractions, random_state=42)\n",
    ")\n",
    "\n",
    "# If rounding causes slight mismatch, fix final size\n",
    "sample4model  = sample4model .sample(n=sample_size, random_state=42)\n",
    "\n",
    "# Save or inspect\n",
    "sample4model .to_csv(\"sample_100k_stratified.csv\", index=False)\n",
    "print(sample4model [\"category\"].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74557d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pl.read_csv(\"sample_100k_stratified.csv\")\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c59500-aa18-4318-8949-cb5e4058be9a",
   "metadata": {},
   "source": [
    "## GETTING VALIDATION SET OF 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e807bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the big dataset (with 'category')\n",
    "val_size = 200\n",
    "fractions = val_size / len(df)\n",
    "\n",
    "# Stratified sample\n",
    "df_val = df.groupby(\"category\", group_keys=False).apply(\n",
    "    lambda x: x.sample(frac=fractions, random_state=123)\n",
    ")\n",
    "\n",
    "# Fix exact size (200 rows)\n",
    "df_val = df_val.sample(n=val_size, random_state=123).reset_index(drop=True)\n",
    "\n",
    "# Save to CSV for manual labeling\n",
    "df_val.to_csv(\"validation_sample_200.csv\", index=False)\n",
    "\n",
    "print(df_val[\"category\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f891e91f-ce72-4d37-97c8-b48093a80857",
   "metadata": {},
   "source": [
    "## WOOHOOO READY FOR MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0d7f13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31efeccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0377f07e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d99be7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
