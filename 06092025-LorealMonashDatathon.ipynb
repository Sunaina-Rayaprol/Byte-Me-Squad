{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "491e8112-55df-484e-925d-407f77181709",
   "metadata": {},
   "source": [
    "## COMBINING DATASET "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06bee039-5f10-4da9-94d5-ddf4c88ff0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Combined dataset saved as BIG_BOY.csv\n",
      "Shape of BIG BOY: (4725012, 10)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of CSV files\n",
    "csv_files = [\n",
    "    \"comments1.csv\",\n",
    "    \"comments2.csv\",\n",
    "    \"comments3.csv\",\n",
    "    \"comments4.csv\",\n",
    "    \"comments5.csv\"\n",
    "]\n",
    "\n",
    "# Read and combine into one dataframe\n",
    "df_list = [pd.read_csv(file) for file in csv_files]\n",
    "big_boy = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Save the combined dataset\n",
    "big_boy.to_csv(\"BIG_BOY.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Combined dataset saved as BIG_BOY.csv\")\n",
    "print(\"Shape of BIG BOY:\", big_boy.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b56be954-1855-4cf3-b1aa-e0825aa4554e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Random sample of 100,000 created\n",
      "Shape of sample: (100000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Randomly choose 20k rows\n",
    "sample_100k = big_boy.sample(n=100000, random_state=42)  # random_state makes it reproducible\n",
    "\n",
    "print(\"‚úÖ Random sample of 100,000 created\")\n",
    "print(\"Shape of sample:\", sample_100k.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd7a928a-1f56-49e4-be3c-35ab311d62e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                     kind  commentId  channelId  videoId  authorId  \\\n",
       "3956772  youtube#comment    1999899      33069    36229   2377260   \n",
       "3284632  youtube#comment    1235288      37371     5214   2495217   \n",
       "1185839  youtube#comment    1120879      18073    76480   2689034   \n",
       "238094   youtube#comment    3806774      46757    78060   3531698   \n",
       "1107951  youtube#comment    1874941      33445    91263    968927   \n",
       "...                  ...        ...        ...      ...       ...   \n",
       "811650   youtube#comment    2788176      41246    87323   2446840   \n",
       "32900    youtube#comment     668979        805    18615   3561486   \n",
       "2355010  youtube#comment    4642725      33445    91263   3547007   \n",
       "4197186  youtube#comment     365684      11679      743   1946253   \n",
       "1421695  youtube#comment    2350664      16876    18101   3254686   \n",
       "\n",
       "                                              textOriginal  parentCommentId  \\\n",
       "3956772  üíôüíôüíôüíôüíôüíôüíôüíôüíôüíôüíôüíôüíôüíôüíôüíôüíôüíôüíôüíôüíôüíôüíôüíôüíôüíôüíôüíôüíôüíôüíôüíôüíôüíôüíôüíôüíôüíôüíôüíôüíôüíôüíôüíôüíôüíô...              NaN   \n",
       "3284632  Reasons why heavily attractive people (youüíó) d...              NaN   \n",
       "1185839                                                 üáµüá∞              NaN   \n",
       "238094              \"hmm what's next\" \\n* hair falls out *              NaN   \n",
       "1107951                          You are so pretty gurl ‚ò∫Ô∏è              NaN   \n",
       "...                                                    ...              ...   \n",
       "811650                                      I kile you üò¢üòÆüòÆ              NaN   \n",
       "32900                                                G√ºzel              NaN   \n",
       "2355010                                         AN AHOGE!!              NaN   \n",
       "4197186  DIEGO‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§...        2102498.0   \n",
       "1421695                                   Boy lag Rahi haüòÇ              NaN   \n",
       "\n",
       "         likeCount                publishedAt                  updatedAt  \n",
       "3956772          0  2025-03-15 12:09:40+00:00  2025-03-15 12:09:40+00:00  \n",
       "3284632          2  2023-12-26 04:22:37+00:00  2023-12-26 04:23:09+00:00  \n",
       "1185839          0  2022-10-02 12:58:13+00:00  2022-10-02 12:58:13+00:00  \n",
       "238094           1  2022-07-23 06:39:15+00:00  2022-07-23 06:39:15+00:00  \n",
       "1107951          0  2024-07-01 10:41:25+00:00  2024-07-01 10:41:25+00:00  \n",
       "...            ...                        ...                        ...  \n",
       "811650           0  2023-09-18 12:08:11+00:00  2023-09-18 12:08:11+00:00  \n",
       "32900            0  2023-02-28 17:36:58+00:00  2023-02-28 17:36:58+00:00  \n",
       "2355010          0  2024-07-26 07:29:03+00:00  2024-07-26 07:29:03+00:00  \n",
       "4197186          0  2024-06-04 22:05:34+00:00  2024-06-04 22:05:34+00:00  \n",
       "1421695          0  2024-04-08 08:20:06+00:00  2024-04-08 08:20:06+00:00  \n",
       "\n",
       "[100000 rows x 10 columns]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_100k.head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90a682d-d9a5-40b7-94cf-68be19d27b8e",
   "metadata": {},
   "source": [
    "pipline of whats about to go down\n",
    "\n",
    "- combine dataset to bigboy ‚úÖ\n",
    "- randomly select 100k rows for a sample1 and preprocess (remove emoji, anything not in english, any non english words) ‚úÖ\n",
    "- run NMF on the sample1 to get categories\n",
    "- go back to bigboy and create categories column and do keyword-matching to sort the data\n",
    "- make a sample of 100k using stratified sampling\n",
    "- pass it onto dalia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2797709-5d2c-469c-89a6-7d8d8cd5bb41",
   "metadata": {},
   "source": [
    "## TEXT CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5720662-5605-4004-9cb2-402a4c8ce436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
      "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
      "   ---------------------------------------- 0.0/590.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 590.6/590.6 kB 14.0 MB/s eta 0:00:00\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-2.14.1\n"
     ]
    }
   ],
   "source": [
    "! pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d1ca766-aa29-4371-bdad-76cb763101a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     ------------------------------------- 981.5/981.5 kB 20.7 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six in c:\\users\\rayap\\anaconda\\lib\\site-packages (from langdetect) (1.17.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993250 sha256=1e87ffef03f2abd86d6d25fa29a775d1916b8b6fb270d1e730ddd77d17ac35ae\n",
      "  Stored in directory: c:\\users\\rayap\\appdata\\local\\pip\\cache\\wheels\\eb\\87\\25\\2dddf1c94e1786054e25022ec5530bfed52bad86d882999c48\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: Building 'langdetect' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'langdetect'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n"
     ]
    }
   ],
   "source": [
    "! pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de108324-8f0d-4ed8-ae33-ca712515e02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        textOriginal  \\\n",
      "0  Reasons why heavily attractive people (youüíó) d...   \n",
      "1             \"hmm what's next\" \\n* hair falls out *   \n",
      "2                          You are so pretty gurl ‚ò∫Ô∏è   \n",
      "3                                    So beautiful ‚ù§‚ù§   \n",
      "4  It doesn't seem like Micheal what's to admit t...   \n",
      "\n",
      "                                          clean_text  \n",
      "0  reasons why heavily attractive people you don‚Äô...  \n",
      "1                     hmm what s next hair falls out  \n",
      "2                             you are so pretty gurl  \n",
      "3                                       so beautiful  \n",
      "4  it doesn t seem like micheal what s to admit t...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "from langdetect import detect, DetectorFactory\n",
    "\n",
    "# Fix seed for langdetect reproducibility\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# Load CSV\n",
    "df = sample_100k\n",
    "\n",
    "# Function: remove emojis\n",
    "def remove_emojis(text):\n",
    "    return emoji.replace_emoji(text, replace='')\n",
    "\n",
    "# Function: clean text\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = remove_emojis(text)\n",
    "    text = text.lower()\n",
    "    # Remove punctuation & symbols\n",
    "    text = re.sub(rf\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# Function: keep only English comments\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == \"en\"\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Apply cleaning\n",
    "df[\"clean_text\"] = df[\"textOriginal\"].apply(clean_text)\n",
    "\n",
    "# Filter English only\n",
    "df = df[df[\"clean_text\"].apply(is_english)]\n",
    "\n",
    "# Reset index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print(df[[\"textOriginal\", \"clean_text\"]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206277b1-fb8c-484f-bd0f-24a0c5709c6d",
   "metadata": {},
   "source": [
    "## NMF MODEL TO GET TOPICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d064f52a-b591-4296-8b91-8914b66a1ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: beautiful, way, wow, looking, night, absolutely, bald, naturally, matter, india\n",
      "Topic 2: thank, share, dear, video, aww, watching, lets, lovvvvvve, 100million, hit\n",
      "Topic 3: hair, curly, long, straight, cut, black, natural, style, short, wavy\n",
      "Topic 4: makeup, wear, better, need, indian, best, gorgeous, nice, india, wearing\n",
      "Topic 5: love, india, videos, omg, natural, video, ginger, song, ur, best\n",
      "Topic 6: pretty, omg, ur, really, way, girl, wow, looks, wig, looking\n",
      "Topic 7: look, good, night, gorgeous, amazing, better, great, bald, does, day\n",
      "Topic 8: thanks, video, share, dear, sharing, watching, lot, wow, great, tips\n",
      "Topic 9: like, don, just, looks, girl, make, way, people, know, good\n",
      "Topic 10: cute, wow, looking, omg, looks, really, voice, boy, girl, soo\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Assuming df already has the 'clean_text' column from preprocessing\n",
    "texts = df[\"clean_text\"].dropna().tolist()\n",
    "\n",
    "# Step 1: TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.95,       # ignore very common words\n",
    "    min_df=2,          # ignore very rare words\n",
    "    stop_words=\"english\"  # remove English stopwords\n",
    ")\n",
    "tfidf = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Step 2: NMF Model\n",
    "n_topics = 10\n",
    "nmf_model = NMF(n_components=n_topics, random_state=42)\n",
    "nmf_model.fit(tfidf)\n",
    "\n",
    "# Step 3: Display topics\n",
    "def display_topics(model, feature_names, n_top_words=10):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        print(f\"Topic {topic_idx+1}: {', '.join(top_features)}\")\n",
    "\n",
    "# Print topics\n",
    "tfidf_feature_names = vectorizer.get_feature_names_out()\n",
    "display_topics(nmf_model, tfidf_feature_names, n_top_words=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a2211c-ea01-436e-9eea-73a70efbbedc",
   "metadata": {},
   "source": [
    "## CREATING CATEGORIES COLUMN IN BIGBOY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5380f6ea-5c0c-4bdb-b553-5269f814b907",
   "metadata": {},
   "outputs": [],
   "source": [
    "#that we got from NMF model\n",
    "categories = {\n",
    "    \"appearance_compliments\": [\"beautiful\", \"wow\", \"looking\", \"night\", \"absolutely\", \"bald\", \"naturally\"],\n",
    "    \"gratitude_milestones\": [\"thank\", \"share\", \"dear\", \"aww\", \"100million\", \"hit\", \"watching\", \"lets\"],\n",
    "    \"hair\": [\"hair\", \"curly\", \"long\", \"straight\", \"cut\", \"black\", \"style\", \"short\", \"wavy\"],\n",
    "    \"makeup_beauty\": [\"makeup\", \"wear\", \"indian\", \"best\", \"gorgeous\", \"nice\", \"wearing\"],\n",
    "    \"love_india_music\": [\"love\", \"india\", \"videos\", \"omg\", \"natural\", \"song\", \"ginger\"],\n",
    "    \"looks_wigs\": [\"pretty\", \"omg\", \"ur\", \"wig\", \"looks\", \"really\", \"wow\"],\n",
    "    \"general_compliments\": [\"look\", \"good\", \"night\", \"gorgeous\", \"amazing\", \"better\", \"great\", \"day\"],\n",
    "    \"thanks_tips\": [\"thanks\", \"share\", \"dear\", \"sharing\", \"tips\", \"watching\"],\n",
    "    \"opinions_casual\": [\"like\", \"don\", \"just\", \"make\", \"way\", \"people\", \"know\"],\n",
    "    \"cuteness_voice\": [\"cute\", \"wow\", \"voice\", \"boy\", \"girl\", \"soo\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c013aaf3-90a1-43b1-90a0-50dd927b40ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category\n",
      "other                     2505390\n",
      "appearance_compliments     459707\n",
      "makeup_beauty              361002\n",
      "hair                       270684\n",
      "love_india_music           260189\n",
      "opinions_casual            256926\n",
      "general_compliments        188768\n",
      "looks_wigs                 180037\n",
      "gratitude_milestones       115690\n",
      "cuteness_voice             101559\n",
      "thanks_tips                 25060\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Example: loading big dataset (replace with your real file)\n",
    "df = big_boy  # contains column 'textOriginal'\n",
    "\n",
    "# Function: match comment to a category\n",
    "def categorize_comment(text, categories):\n",
    "    text = str(text).lower()\n",
    "    for cat, keywords in categories.items():\n",
    "        # simple keyword match\n",
    "        if any(re.search(rf\"\\b{kw}\\b\", text) for kw in keywords):\n",
    "            return cat\n",
    "    return \"other\"  # if no keyword matches\n",
    "\n",
    "# Apply to dataset\n",
    "df[\"category\"] = df[\"textOriginal\"].apply(lambda x: categorize_comment(x, categories))\n",
    "\n",
    "# Check distribution\n",
    "print(df[\"category\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15294737-09e7-4fbc-83a6-f24e2804f9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flashtext\n",
      "  Downloading flashtext-2.7.tar.gz (14 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: flashtext\n",
      "  Building wheel for flashtext (setup.py): started\n",
      "  Building wheel for flashtext (setup.py): finished with status 'done'\n",
      "  Created wheel for flashtext: filename=flashtext-2.7-py2.py3-none-any.whl size=9324 sha256=cd7db3d3fe87d734b3345b624b01d28542592a8dfb9b6b66b057c39846286b85\n",
      "  Stored in directory: c:\\users\\rayap\\appdata\\local\\pip\\cache\\wheels\\be\\12\\c9\\228313ff5cb722777830302f1d4136de4129932e6a0354c056\n",
      "Successfully built flashtext\n",
      "Installing collected packages: flashtext\n",
      "Successfully installed flashtext-2.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: Building 'flashtext' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'flashtext'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n"
     ]
    }
   ],
   "source": [
    "! pip install flashtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "764d60dc-8b5c-4c8b-889c-fa99689642a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flashtext import KeywordProcessor\n",
    "\n",
    "# Build processor\n",
    "keyword_processor = KeywordProcessor()\n",
    "for cat, keywords in categories.items():\n",
    "    for kw in keywords:\n",
    "        keyword_processor.add_keyword(kw, cat)\n",
    "\n",
    "# Match function\n",
    "def fast_categorize(text):\n",
    "    matches = keyword_processor.extract_keywords(str(text).lower())\n",
    "    return matches[0] if matches else \"other\"\n",
    "\n",
    "# Apply fast\n",
    "df[\"category\"] = df[\"textOriginal\"].apply(fast_categorize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfa398f-9efa-4ca7-bff3-7eab4d6e1200",
   "metadata": {},
   "source": [
    "## STRATIFIED RANDOM SAMPLING TO GET SAMPLE OF 100K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b7ef4a6-f8d9-4682-a737-146247b687bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rayap\\AppData\\Local\\Temp\\ipykernel_33872\\1433876936.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sample4model = big_boy.groupby(\"category\", group_keys=False).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category\n",
      "other                     0.53010\n",
      "opinions_casual           0.08861\n",
      "general_compliments       0.06315\n",
      "makeup_beauty             0.05317\n",
      "love_india_music          0.05273\n",
      "looks_wigs                0.05133\n",
      "cuteness_voice            0.04781\n",
      "appearance_compliments    0.04627\n",
      "hair                      0.04026\n",
      "gratitude_milestones      0.01535\n",
      "thanks_tips               0.01122\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your big dataset (already has \"category\" column)\n",
    "\n",
    "# Desired sample size\n",
    "sample_size = 100_000  \n",
    "\n",
    "# Compute sampling fraction per category\n",
    "fractions = sample_size / len(big_boy)\n",
    "\n",
    "# Stratified sample\n",
    "sample4model = big_boy.groupby(\"category\", group_keys=False).apply(\n",
    "    lambda x: x.sample(frac=fractions, random_state=42)\n",
    ")\n",
    "\n",
    "# If rounding causes slight mismatch, fix final size\n",
    "sample4model  = sample4model .sample(n=sample_size, random_state=42)\n",
    "\n",
    "# Save or inspect\n",
    "sample4model .to_csv(\"sample_100k_stratified.csv\", index=False)\n",
    "print(sample4model [\"category\"].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bad8a63a-9de0-46d7-a3d8-f92b8dd504e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 11)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = pd.read_csv(\"sample_100k_stratified.csv\")\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c59500-aa18-4318-8949-cb5e4058be9a",
   "metadata": {},
   "source": [
    "## CLEAN DA SAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da096fab-1d0c-48a5-894e-7283d288db8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f891e91f-ce72-4d37-97c8-b48093a80857",
   "metadata": {},
   "source": [
    "## WOOHOOO READY FOR MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc892e4-dd9e-4dd8-9030-fefc225ffdae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508a10d0-085c-48ca-b907-d0810841bb3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f3740a-29e9-45ca-84e4-34e087b4df14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6268c56e-58c2-4e57-936b-1890feaa9585",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
